{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "We now try building af ML pipeline. The data for this exercise is the same as in L01, meaning that the OECD data from the 'intro.ipynb' have been save into a Python 'pickle' file. \n",
    "\n",
    "The pickle library is a nifty data preservation method in Python, and from L01 the tuple `(X, y)` have been stored to the pickle file `tmal_l01_data.pkl', try reloading it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(29, 1),  y.shape=(29,)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    import pickle\n",
    "    filename = \"./itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "\n",
    "print(f\"X.shape={X.shape},  y.shape={y.shape}\")\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # did a y.ravel() before saving to picke file\n",
    "assert X.shape[0] == 29\n",
    "\n",
    "# re-create plot data (not stored in the Pickel file)\n",
    "m = np.linspace(0, 60000, 1000)\n",
    "M = np.empty([m.shape[0], 1])\n",
    "M[:, 0] = m\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Revisiting the problem with the MLP\n",
    "\n",
    "Using the MLP for the QECD data in Qd) from `intro.ipynb` produced a negative $R^2$, meaning that it was unable to fit the data, and the MPL model was actually _worse_ than the naive $\\hat y$ (mean value of y).\n",
    "\n",
    "Let's just revisit this fact. When running the next cell you should now see an OK $~R^2_{lin.reg}~$ score and a negative $~R^2_{mlp}~$ score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\n",
      "\n",
      "lin.reg.score(X, y)=0.73\n",
      "MLP    .score(X, y)=-8.85\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup the MLP and lin. regression again..\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def PlotModels(model1, model2, X, y, name_model1=\"lin.reg\", name_model2=\"MLP\"):\n",
    "\n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(model1, model2, X, y):\n",
    "        y_pred_model1 = model1.predict(X)\n",
    "        y_pred_model2 = model2.predict(X)\n",
    "\n",
    "        # call r2\n",
    "        score_model1 = r2_score(y, y_pred_model1)\n",
    "        score_model2 = r2_score(y, y_pred_model2)\n",
    "\n",
    "        return y_pred_model1, y_pred_model2, score_model1, score_model2\n",
    "    \n",
    "    def Fill(s, n):\n",
    "        while(len(s)<n):\n",
    "            s += \" \"\n",
    "        return s\n",
    "\n",
    "    y_pred_model1, y_pred_model2, score_model1, score_model2 = CalcPredAndScore(\n",
    "        model1, model2, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.plot(X, y_pred_model2, \"kx-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([name_model1, name_model2, \"X OECD data\"])\n",
    "\n",
    "    l = max(len(name_model1), len(name_model2))\n",
    "    \n",
    "    print(f\"{Fill(name_model1,l)}.score(X, y)={score_model1:0.2f}\")\n",
    "    print(f\"{Fill(name_model2,l)}.score(X, y)={score_model2:0.2f}\")\n",
    "\n",
    "\n",
    "# lets make a linear and MLP regressor and redo the plots\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)\n",
    "linreg = LinearRegression()\n",
    "\n",
    "mlp.fit(X, y)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "print(\"The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\\n\")\n",
    "PlotModels(linreg, mlp, X, y)\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Create a Min/max scaler for the MLP\n",
    "\n",
    "Now, the neurons in neural networks normally expect input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range then the neuron will saturate to its min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. Af fix to the problem in Qd), from `intro.ipynb`, is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to manually scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?\n",
    "\n",
    "(If you already made exercise \"Qe) Neural Network with pre-scaling\" in L01, then reuse Your work here!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 18.28253901\n",
      "Iteration 2, loss = 18.26565519\n",
      "Iteration 3, loss = 18.24867758\n",
      "Iteration 4, loss = 18.23196407\n",
      "Iteration 5, loss = 18.21519929\n",
      "Iteration 6, loss = 18.19834801\n",
      "Iteration 7, loss = 18.18146241\n",
      "Iteration 8, loss = 18.16446101\n",
      "Iteration 9, loss = 18.14744640\n",
      "Iteration 10, loss = 18.13041941\n",
      "Iteration 11, loss = 18.11338060\n",
      "Iteration 12, loss = 18.09633038\n",
      "Iteration 13, loss = 18.07926690\n",
      "Iteration 14, loss = 18.06206856\n",
      "Iteration 15, loss = 18.04480362\n",
      "Iteration 16, loss = 18.02742877\n",
      "Iteration 17, loss = 18.01003203\n",
      "Iteration 18, loss = 17.99261494\n",
      "Iteration 19, loss = 17.97517874\n",
      "Iteration 20, loss = 17.95760769\n",
      "Iteration 21, loss = 17.93998657\n",
      "Iteration 22, loss = 17.92234063\n",
      "Iteration 23, loss = 17.90467142\n",
      "Iteration 24, loss = 17.88698018\n",
      "Iteration 25, loss = 17.86926789\n",
      "Iteration 26, loss = 17.85153536\n",
      "Iteration 27, loss = 17.83378321\n",
      "Iteration 28, loss = 17.81590961\n",
      "Iteration 29, loss = 17.79800074\n",
      "Iteration 30, loss = 17.78006634\n",
      "Iteration 31, loss = 17.76210748\n",
      "Iteration 32, loss = 17.74412504\n",
      "Iteration 33, loss = 17.72611972\n",
      "Iteration 34, loss = 17.70832432\n",
      "Iteration 35, loss = 17.69054229\n",
      "Iteration 36, loss = 17.67286828\n",
      "Iteration 37, loss = 17.65531073\n",
      "Iteration 38, loss = 17.63776985\n",
      "Iteration 39, loss = 17.62026952\n",
      "Iteration 40, loss = 17.60284774\n",
      "Iteration 41, loss = 17.58541745\n",
      "Iteration 42, loss = 17.56797643\n",
      "Iteration 43, loss = 17.55052136\n",
      "Iteration 44, loss = 17.53293408\n",
      "Iteration 45, loss = 17.51532246\n",
      "Iteration 46, loss = 17.49768583\n",
      "Iteration 47, loss = 17.48002358\n",
      "Iteration 48, loss = 17.46224477\n",
      "Iteration 49, loss = 17.44439259\n",
      "Iteration 50, loss = 17.42650350\n",
      "Iteration 51, loss = 17.40857809\n",
      "Iteration 52, loss = 17.39061684\n",
      "Iteration 53, loss = 17.37268473\n",
      "Iteration 54, loss = 17.35477302\n",
      "Iteration 55, loss = 17.33683277\n",
      "Iteration 56, loss = 17.31886338\n",
      "Iteration 57, loss = 17.30086427\n",
      "Iteration 58, loss = 17.28283491\n",
      "Iteration 59, loss = 17.26477478\n",
      "Iteration 60, loss = 17.24668342\n",
      "Iteration 61, loss = 17.22856037\n",
      "Iteration 62, loss = 17.21040523\n",
      "Iteration 63, loss = 17.19221761\n",
      "Iteration 64, loss = 17.17402045\n",
      "Iteration 65, loss = 17.15586701\n",
      "Iteration 66, loss = 17.13758971\n",
      "Iteration 67, loss = 17.11925938\n",
      "Iteration 68, loss = 17.10089047\n",
      "Iteration 69, loss = 17.08248312\n",
      "Iteration 70, loss = 17.06403740\n",
      "Iteration 71, loss = 17.04555339\n",
      "Iteration 72, loss = 17.02696083\n",
      "Iteration 73, loss = 17.00829345\n",
      "Iteration 74, loss = 16.98957986\n",
      "Iteration 75, loss = 16.97082091\n",
      "Iteration 76, loss = 16.95203425\n",
      "Iteration 77, loss = 16.93317301\n",
      "Iteration 78, loss = 16.91426576\n",
      "Iteration 79, loss = 16.89526689\n",
      "Iteration 80, loss = 16.87614889\n",
      "Iteration 81, loss = 16.85697744\n",
      "Iteration 82, loss = 16.83775406\n",
      "Iteration 83, loss = 16.81848008\n",
      "Iteration 84, loss = 16.79914364\n",
      "Iteration 85, loss = 16.77963709\n",
      "Iteration 86, loss = 16.76007260\n",
      "Iteration 87, loss = 16.74045216\n",
      "Iteration 88, loss = 16.72077750\n",
      "Iteration 89, loss = 16.70105017\n",
      "Iteration 90, loss = 16.68127155\n",
      "Iteration 91, loss = 16.66144285\n",
      "Iteration 92, loss = 16.64156515\n",
      "Iteration 93, loss = 16.62163940\n",
      "Iteration 94, loss = 16.60166646\n",
      "Iteration 95, loss = 16.58164707\n",
      "Iteration 96, loss = 16.56158191\n",
      "Iteration 97, loss = 16.54147158\n",
      "Iteration 98, loss = 16.52131660\n",
      "Iteration 99, loss = 16.50111744\n",
      "Iteration 100, loss = 16.48075782\n",
      "Iteration 101, loss = 16.46026182\n",
      "Iteration 102, loss = 16.43968460\n",
      "Iteration 103, loss = 16.41904737\n",
      "Iteration 104, loss = 16.39835229\n",
      "Iteration 105, loss = 16.37760125\n",
      "Iteration 106, loss = 16.35679598\n",
      "Iteration 107, loss = 16.33593799\n",
      "Iteration 108, loss = 16.31502864\n",
      "Iteration 109, loss = 16.29406915\n",
      "Iteration 110, loss = 16.27306060\n",
      "Iteration 111, loss = 16.25200397\n",
      "Iteration 112, loss = 16.23092119\n",
      "Iteration 113, loss = 16.20983434\n",
      "Iteration 114, loss = 16.18865433\n",
      "Iteration 115, loss = 16.16735740\n",
      "Iteration 116, loss = 16.14594304\n",
      "Iteration 117, loss = 16.12440691\n",
      "Iteration 118, loss = 16.10281078\n",
      "Iteration 119, loss = 16.08115676\n",
      "Iteration 120, loss = 16.05944673\n",
      "Iteration 121, loss = 16.03768239\n",
      "Iteration 122, loss = 16.01586522\n",
      "Iteration 123, loss = 15.99405048\n",
      "Iteration 124, loss = 15.97219625\n",
      "Iteration 125, loss = 15.95029658\n",
      "Iteration 126, loss = 15.92835196\n",
      "Iteration 127, loss = 15.90636280\n",
      "Iteration 128, loss = 15.88432950\n",
      "Iteration 129, loss = 15.86225241\n",
      "Iteration 130, loss = 15.84013184\n",
      "Iteration 131, loss = 15.81796808\n",
      "Iteration 132, loss = 15.79576138\n",
      "Iteration 133, loss = 15.77351199\n",
      "Iteration 134, loss = 15.75122011\n",
      "Iteration 135, loss = 15.72888597\n",
      "Iteration 136, loss = 15.70650973\n",
      "Iteration 137, loss = 15.68409158\n",
      "Iteration 138, loss = 15.66163167\n",
      "Iteration 139, loss = 15.63913018\n",
      "Iteration 140, loss = 15.61658725\n",
      "Iteration 141, loss = 15.59400301\n",
      "Iteration 142, loss = 15.57137762\n",
      "Iteration 143, loss = 15.54871120\n",
      "Iteration 144, loss = 15.52600389\n",
      "Iteration 145, loss = 15.50325581\n",
      "Iteration 146, loss = 15.48046709\n",
      "Iteration 147, loss = 15.45763787\n",
      "Iteration 148, loss = 15.43476825\n",
      "Iteration 149, loss = 15.41185837\n",
      "Iteration 150, loss = 15.38890836\n",
      "Iteration 151, loss = 15.36591832\n",
      "Iteration 152, loss = 15.34288840\n",
      "Iteration 153, loss = 15.31981871\n",
      "Iteration 154, loss = 15.29670939\n",
      "Iteration 155, loss = 15.27356055\n",
      "Iteration 156, loss = 15.25040620\n",
      "Iteration 157, loss = 15.22722861\n",
      "Iteration 158, loss = 15.20401493\n",
      "Iteration 159, loss = 15.18076489\n",
      "Iteration 160, loss = 15.15747829\n",
      "Iteration 161, loss = 15.13415494\n",
      "Iteration 162, loss = 15.11079468\n",
      "Iteration 163, loss = 15.08739741\n",
      "Iteration 164, loss = 15.06396303\n",
      "Iteration 165, loss = 15.04049147\n",
      "Iteration 166, loss = 15.01698268\n",
      "Iteration 167, loss = 14.99343666\n",
      "Iteration 168, loss = 14.96985338\n",
      "Iteration 169, loss = 14.94623286\n",
      "Iteration 170, loss = 14.92257512\n",
      "Iteration 171, loss = 14.89888021\n",
      "Iteration 172, loss = 14.87514818\n",
      "Iteration 173, loss = 14.85137910\n",
      "Iteration 174, loss = 14.82757303\n",
      "Iteration 175, loss = 14.80373008\n",
      "Iteration 176, loss = 14.77985032\n",
      "Iteration 177, loss = 14.75593387\n",
      "Iteration 178, loss = 14.73198083\n",
      "Iteration 179, loss = 14.70799133\n",
      "Iteration 180, loss = 14.68396548\n",
      "Iteration 181, loss = 14.65990342\n",
      "Iteration 182, loss = 14.63580528\n",
      "Iteration 183, loss = 14.61167119\n",
      "Iteration 184, loss = 14.58750132\n",
      "Iteration 185, loss = 14.56329579\n",
      "Iteration 186, loss = 14.53905478\n",
      "Iteration 187, loss = 14.51477843\n",
      "Iteration 188, loss = 14.49046690\n",
      "Iteration 189, loss = 14.46612036\n",
      "Iteration 190, loss = 14.44173898\n",
      "Iteration 191, loss = 14.41732292\n",
      "Iteration 192, loss = 14.39287235\n",
      "Iteration 193, loss = 14.36838746\n",
      "Iteration 194, loss = 14.34386841\n",
      "Iteration 195, loss = 14.31931539\n",
      "Iteration 196, loss = 14.29472857\n",
      "Iteration 197, loss = 14.27010814\n",
      "Iteration 198, loss = 14.24545429\n",
      "Iteration 199, loss = 14.22076720\n",
      "Iteration 200, loss = 14.19604706\n",
      "Iteration 201, loss = 14.17129406\n",
      "Iteration 202, loss = 14.14650839\n",
      "Iteration 203, loss = 14.12169024\n",
      "Iteration 204, loss = 14.09683981\n",
      "Iteration 205, loss = 14.07195730\n",
      "Iteration 206, loss = 14.04704290\n",
      "Iteration 207, loss = 14.02209681\n",
      "Iteration 208, loss = 13.99711924\n",
      "Iteration 209, loss = 13.97211037\n",
      "Iteration 210, loss = 13.94707042\n",
      "Iteration 211, loss = 13.92199959\n",
      "Iteration 212, loss = 13.89689808\n",
      "Iteration 213, loss = 13.87176610\n",
      "Iteration 214, loss = 13.84660385\n",
      "Iteration 215, loss = 13.82141155\n",
      "Iteration 216, loss = 13.79618939\n",
      "Iteration 217, loss = 13.77093760\n",
      "Iteration 218, loss = 13.74565638\n",
      "Iteration 219, loss = 13.72034594\n",
      "Iteration 220, loss = 13.69500649\n",
      "Iteration 221, loss = 13.66963826\n",
      "Iteration 222, loss = 13.64424145\n",
      "Iteration 223, loss = 13.61881628\n",
      "Iteration 224, loss = 13.59336296\n",
      "Iteration 225, loss = 13.56788171\n",
      "Iteration 226, loss = 13.54237275\n",
      "Iteration 227, loss = 13.51684719\n",
      "Iteration 228, loss = 13.49130651\n",
      "Iteration 229, loss = 13.46574019\n",
      "Iteration 230, loss = 13.44014826\n",
      "Iteration 231, loss = 13.41453077\n",
      "Iteration 232, loss = 13.38888779\n",
      "Iteration 233, loss = 13.36321940\n",
      "Iteration 234, loss = 13.33752570\n",
      "Iteration 235, loss = 13.31180681\n",
      "Iteration 236, loss = 13.28606284\n",
      "Iteration 237, loss = 13.26029394\n",
      "Iteration 238, loss = 13.23450024\n",
      "Iteration 239, loss = 13.20868191\n",
      "Iteration 240, loss = 13.18283911\n",
      "Iteration 241, loss = 13.15697200\n",
      "Iteration 242, loss = 13.13108076\n",
      "Iteration 243, loss = 13.10516558\n",
      "Iteration 244, loss = 13.07922665\n",
      "Iteration 245, loss = 13.05326415\n",
      "Iteration 246, loss = 13.02727830\n",
      "Iteration 247, loss = 13.00126928\n",
      "Iteration 248, loss = 12.97523731\n",
      "Iteration 249, loss = 12.94918260\n",
      "Iteration 250, loss = 12.92310536\n",
      "Iteration 251, loss = 12.89700581\n",
      "Iteration 252, loss = 12.87088416\n",
      "Iteration 253, loss = 12.84474065\n",
      "Iteration 254, loss = 12.81857549\n",
      "Iteration 255, loss = 12.79238890\n",
      "Iteration 256, loss = 12.76618113\n",
      "Iteration 257, loss = 12.73995240\n",
      "Iteration 258, loss = 12.71370294\n",
      "Iteration 259, loss = 12.68743298\n",
      "Iteration 260, loss = 12.66114277\n",
      "Iteration 261, loss = 12.63483253\n",
      "Iteration 262, loss = 12.60850250\n",
      "Iteration 263, loss = 12.58215293\n",
      "Iteration 264, loss = 12.55578405\n",
      "Iteration 265, loss = 12.52939611\n",
      "Iteration 266, loss = 12.50298934\n",
      "Iteration 267, loss = 12.47656399\n",
      "Iteration 268, loss = 12.45012031\n",
      "Iteration 269, loss = 12.42365853\n",
      "Iteration 270, loss = 12.39717891\n",
      "Iteration 271, loss = 12.37068169\n",
      "Iteration 272, loss = 12.34416712\n",
      "Iteration 273, loss = 12.31763544\n",
      "Iteration 274, loss = 12.29108690\n",
      "Iteration 275, loss = 12.26452176\n",
      "Iteration 276, loss = 12.23794026\n",
      "Iteration 277, loss = 12.21134265\n",
      "Iteration 278, loss = 12.18472918\n",
      "Iteration 279, loss = 12.15810011\n",
      "Iteration 280, loss = 12.13145569\n",
      "Iteration 281, loss = 12.10479616\n",
      "Iteration 282, loss = 12.07812178\n",
      "Iteration 283, loss = 12.05143281\n",
      "Iteration 284, loss = 12.02472949\n",
      "Iteration 285, loss = 11.99801208\n",
      "Iteration 286, loss = 11.97128084\n",
      "Iteration 287, loss = 11.94453602\n",
      "Iteration 288, loss = 11.91777788\n",
      "Iteration 289, loss = 11.89100667\n",
      "Iteration 290, loss = 11.86422264\n",
      "Iteration 291, loss = 11.83742606\n",
      "Iteration 292, loss = 11.81061718\n",
      "Iteration 293, loss = 11.78379625\n",
      "Iteration 294, loss = 11.75696354\n",
      "Iteration 295, loss = 11.73011930\n",
      "Iteration 296, loss = 11.70326378\n",
      "Iteration 297, loss = 11.67639726\n",
      "Iteration 298, loss = 11.64951998\n",
      "Iteration 299, loss = 11.62263221\n",
      "Iteration 300, loss = 11.59573420\n",
      "Iteration 301, loss = 11.56882621\n",
      "Iteration 302, loss = 11.54190851\n",
      "Iteration 303, loss = 11.51498134\n",
      "Iteration 304, loss = 11.48804498\n",
      "Iteration 305, loss = 11.46109968\n",
      "Iteration 306, loss = 11.43414571\n",
      "Iteration 307, loss = 11.40718331\n",
      "Iteration 308, loss = 11.38021276\n",
      "Iteration 309, loss = 11.35323432\n",
      "Iteration 310, loss = 11.32624824\n",
      "Iteration 311, loss = 11.29925479\n",
      "Iteration 312, loss = 11.27225423\n",
      "Iteration 313, loss = 11.24524682\n",
      "Iteration 314, loss = 11.21823282\n",
      "Iteration 315, loss = 11.19121793\n",
      "Iteration 316, loss = 11.16420346\n",
      "Iteration 317, loss = 11.13718389\n",
      "Iteration 318, loss = 11.11015938\n",
      "Iteration 319, loss = 11.08313011\n",
      "Iteration 320, loss = 11.05609624\n",
      "Iteration 321, loss = 11.02905798\n",
      "Iteration 322, loss = 11.00201552\n",
      "Iteration 323, loss = 10.97496906\n",
      "Iteration 324, loss = 10.94791882\n",
      "Iteration 325, loss = 10.92086501\n",
      "Iteration 326, loss = 10.89380786\n",
      "Iteration 327, loss = 10.86674758\n",
      "Iteration 328, loss = 10.83968441\n",
      "Iteration 329, loss = 10.81261857\n",
      "Iteration 330, loss = 10.78555032\n",
      "Iteration 331, loss = 10.75847989\n",
      "Iteration 332, loss = 10.73140751\n",
      "Iteration 333, loss = 10.70433344\n",
      "Iteration 334, loss = 10.67725792\n",
      "Iteration 335, loss = 10.65018119\n",
      "Iteration 336, loss = 10.62310352\n",
      "Iteration 337, loss = 10.59602515\n",
      "Iteration 338, loss = 10.56894634\n",
      "Iteration 339, loss = 10.54186733\n",
      "Iteration 340, loss = 10.51478840\n",
      "Iteration 341, loss = 10.48770978\n",
      "Iteration 342, loss = 10.46063175\n",
      "Iteration 343, loss = 10.43355456\n",
      "Iteration 344, loss = 10.40647847\n",
      "Iteration 345, loss = 10.37940374\n",
      "Iteration 346, loss = 10.35233063\n",
      "Iteration 347, loss = 10.32525940\n",
      "Iteration 348, loss = 10.29819031\n",
      "Iteration 349, loss = 10.27112363\n",
      "Iteration 350, loss = 10.24405962\n",
      "Iteration 351, loss = 10.21699853\n",
      "Iteration 352, loss = 10.18994064\n",
      "Iteration 353, loss = 10.16288620\n",
      "Iteration 354, loss = 10.13583549\n",
      "Iteration 355, loss = 10.10878875\n",
      "Iteration 356, loss = 10.08174626\n",
      "Iteration 357, loss = 10.05470829\n",
      "Iteration 358, loss = 10.02767508\n",
      "Iteration 359, loss = 10.00064691\n",
      "Iteration 360, loss = 9.97362404\n",
      "Iteration 361, loss = 9.94660673\n",
      "Iteration 362, loss = 9.91959525\n",
      "Iteration 363, loss = 9.89258987\n",
      "Iteration 364, loss = 9.86559083\n",
      "Iteration 365, loss = 9.83859841\n",
      "Iteration 366, loss = 9.81161288\n",
      "Iteration 367, loss = 9.78463448\n",
      "Iteration 368, loss = 9.75766350\n",
      "Iteration 369, loss = 9.73070018\n",
      "Iteration 370, loss = 9.70374480\n",
      "Iteration 371, loss = 9.67679761\n",
      "Iteration 372, loss = 9.64985888\n",
      "Iteration 373, loss = 9.62292887\n",
      "Iteration 374, loss = 9.59600784\n",
      "Iteration 375, loss = 9.56909605\n",
      "Iteration 376, loss = 9.54219376\n",
      "Iteration 377, loss = 9.51530125\n",
      "Iteration 378, loss = 9.48841876\n",
      "Iteration 379, loss = 9.46154656\n",
      "Iteration 380, loss = 9.43468491\n",
      "Iteration 381, loss = 9.40783406\n",
      "Iteration 382, loss = 9.38099429\n",
      "Iteration 383, loss = 9.35416585\n",
      "Iteration 384, loss = 9.32734899\n",
      "Iteration 385, loss = 9.30054399\n",
      "Iteration 386, loss = 9.27375109\n",
      "Iteration 387, loss = 9.24697055\n",
      "Iteration 388, loss = 9.22020264\n",
      "Iteration 389, loss = 9.19344762\n",
      "Iteration 390, loss = 9.16670573\n",
      "Iteration 391, loss = 9.13997724\n",
      "Iteration 392, loss = 9.11326241\n",
      "Iteration 393, loss = 9.08656149\n",
      "Iteration 394, loss = 9.05987474\n",
      "Iteration 395, loss = 9.03320241\n",
      "Iteration 396, loss = 9.00654476\n",
      "Iteration 397, loss = 8.97990205\n",
      "Iteration 398, loss = 8.95327453\n",
      "Iteration 399, loss = 8.92666245\n",
      "Iteration 400, loss = 8.90006607\n",
      "Iteration 401, loss = 8.87348565\n",
      "Iteration 402, loss = 8.84692143\n",
      "Iteration 403, loss = 8.82037367\n",
      "Iteration 404, loss = 8.79384263\n",
      "Iteration 405, loss = 8.76732855\n",
      "Iteration 406, loss = 8.74083169\n",
      "Iteration 407, loss = 8.71435229\n",
      "Iteration 408, loss = 8.68789062\n",
      "Iteration 409, loss = 8.66144691\n",
      "Iteration 410, loss = 8.63502143\n",
      "Iteration 411, loss = 8.60861441\n",
      "Iteration 412, loss = 8.58222612\n",
      "Iteration 413, loss = 8.55585679\n",
      "Iteration 414, loss = 8.52950668\n",
      "Iteration 415, loss = 8.50317603\n",
      "Iteration 416, loss = 8.47686509\n",
      "Iteration 417, loss = 8.45057411\n",
      "Iteration 418, loss = 8.42430334\n",
      "Iteration 419, loss = 8.39805301\n",
      "Iteration 420, loss = 8.37182339\n",
      "Iteration 421, loss = 8.34561470\n",
      "Iteration 422, loss = 8.31942720\n",
      "Iteration 423, loss = 8.29326112\n",
      "Iteration 424, loss = 8.26711672\n",
      "Iteration 425, loss = 8.24099424\n",
      "Iteration 426, loss = 8.21489391\n",
      "Iteration 427, loss = 8.18881598\n",
      "Iteration 428, loss = 8.16276069\n",
      "Iteration 429, loss = 8.13672828\n",
      "Iteration 430, loss = 8.11071900\n",
      "Iteration 431, loss = 8.08473307\n",
      "Iteration 432, loss = 8.05877074\n",
      "Iteration 433, loss = 8.03283226\n",
      "Iteration 434, loss = 8.00691784\n",
      "Iteration 435, loss = 7.98102774\n",
      "Iteration 436, loss = 7.95516219\n",
      "Iteration 437, loss = 7.92932143\n",
      "Iteration 438, loss = 7.90350569\n",
      "Iteration 439, loss = 7.87771520\n",
      "Iteration 440, loss = 7.85195021\n",
      "Iteration 441, loss = 7.82621094\n",
      "Iteration 442, loss = 7.80049762\n",
      "Iteration 443, loss = 7.77481050\n",
      "Iteration 444, loss = 7.74914979\n",
      "Iteration 445, loss = 7.72351574\n",
      "Iteration 446, loss = 7.69790857\n",
      "Iteration 447, loss = 7.67232852\n",
      "Iteration 448, loss = 7.64677580\n",
      "Iteration 449, loss = 7.62125066\n",
      "Iteration 450, loss = 7.59575331\n",
      "Iteration 451, loss = 7.57028399\n",
      "Iteration 452, loss = 7.54484292\n",
      "Iteration 453, loss = 7.51943033\n",
      "Iteration 454, loss = 7.49404644\n",
      "Iteration 455, loss = 7.46869148\n",
      "Iteration 456, loss = 7.44336567\n",
      "Iteration 457, loss = 7.41806923\n",
      "Iteration 458, loss = 7.39280239\n",
      "Iteration 459, loss = 7.36756537\n",
      "Iteration 460, loss = 7.34235839\n",
      "Iteration 461, loss = 7.31718166\n",
      "Iteration 462, loss = 7.29203762\n",
      "Iteration 463, loss = 7.26692476\n",
      "Iteration 464, loss = 7.24184296\n",
      "Iteration 465, loss = 7.21679240\n",
      "Iteration 466, loss = 7.19177328\n",
      "Iteration 467, loss = 7.16678580\n",
      "Iteration 468, loss = 7.14183016\n",
      "Iteration 469, loss = 7.11690655\n",
      "Iteration 470, loss = 7.09201518\n",
      "Iteration 471, loss = 7.06715624\n",
      "Iteration 472, loss = 7.04232994\n",
      "Iteration 473, loss = 7.01753648\n",
      "Iteration 474, loss = 6.99277606\n",
      "Iteration 475, loss = 6.96804887\n",
      "Iteration 476, loss = 6.94335514\n",
      "Iteration 477, loss = 6.91869504\n",
      "Iteration 478, loss = 6.89406880\n",
      "Iteration 479, loss = 6.86947660\n",
      "Iteration 480, loss = 6.84491865\n",
      "Iteration 481, loss = 6.82039515\n",
      "Iteration 482, loss = 6.79590630\n",
      "Iteration 483, loss = 6.77145231\n",
      "Iteration 484, loss = 6.74703337\n",
      "Iteration 485, loss = 6.72264968\n",
      "Iteration 486, loss = 6.69830144\n",
      "Iteration 487, loss = 6.67398884\n",
      "Iteration 488, loss = 6.64971210\n",
      "Iteration 489, loss = 6.62547140\n",
      "Iteration 490, loss = 6.60126693\n",
      "Iteration 491, loss = 6.57709890\n",
      "Iteration 492, loss = 6.55296751\n",
      "Iteration 493, loss = 6.52887293\n",
      "Iteration 494, loss = 6.50481538\n",
      "Iteration 495, loss = 6.48079503\n",
      "Iteration 496, loss = 6.45681209\n",
      "Iteration 497, loss = 6.43286674\n",
      "Iteration 498, loss = 6.40895917\n",
      "Iteration 499, loss = 6.38508958\n",
      "Iteration 500, loss = 6.36125815\n",
      "Iteration 501, loss = 6.33746507\n",
      "Iteration 502, loss = 6.31371053\n",
      "Iteration 503, loss = 6.28999471\n",
      "Iteration 504, loss = 6.26631780\n",
      "Iteration 505, loss = 6.24267998\n",
      "Iteration 506, loss = 6.21908144\n",
      "Iteration 507, loss = 6.19552236\n",
      "Iteration 508, loss = 6.17200291\n",
      "Iteration 509, loss = 6.14852329\n",
      "Iteration 510, loss = 6.12508367\n",
      "Iteration 511, loss = 6.10168423\n",
      "Iteration 512, loss = 6.07832514\n",
      "Iteration 513, loss = 6.05500659\n",
      "Iteration 514, loss = 6.03172875\n",
      "Iteration 515, loss = 6.00849179\n",
      "Iteration 516, loss = 5.98529590\n",
      "Iteration 517, loss = 5.96214124\n",
      "Iteration 518, loss = 5.93902798\n",
      "Iteration 519, loss = 5.91595630\n",
      "Iteration 520, loss = 5.89292636\n",
      "Iteration 521, loss = 5.86993834\n",
      "Iteration 522, loss = 5.84699240\n",
      "Iteration 523, loss = 5.82408872\n",
      "Iteration 524, loss = 5.80122745\n",
      "Iteration 525, loss = 5.77840876\n",
      "Iteration 526, loss = 5.75563282\n",
      "Iteration 527, loss = 5.73289979\n",
      "Iteration 528, loss = 5.71020983\n",
      "Iteration 529, loss = 5.68756311\n",
      "Iteration 530, loss = 5.66495977\n",
      "Iteration 531, loss = 5.64239999\n",
      "Iteration 532, loss = 5.61988392\n",
      "Iteration 533, loss = 5.59741172\n",
      "Iteration 534, loss = 5.57498355\n",
      "Iteration 535, loss = 5.55259955\n",
      "Iteration 536, loss = 5.53025988\n",
      "Iteration 537, loss = 5.50796470\n",
      "Iteration 538, loss = 5.48571415\n",
      "Iteration 539, loss = 5.46350840\n",
      "Iteration 540, loss = 5.44134758\n",
      "Iteration 541, loss = 5.41923185\n",
      "Iteration 542, loss = 5.39716135\n",
      "Iteration 543, loss = 5.37513624\n",
      "Iteration 544, loss = 5.35315665\n",
      "Iteration 545, loss = 5.33122273\n",
      "Iteration 546, loss = 5.30933463\n",
      "Iteration 547, loss = 5.28749248\n",
      "Iteration 548, loss = 5.26569643\n",
      "Iteration 549, loss = 5.24394662\n",
      "Iteration 550, loss = 5.22224318\n",
      "Iteration 551, loss = 5.20058626\n",
      "Iteration 552, loss = 5.17897599\n",
      "Iteration 553, loss = 5.15741251\n",
      "Iteration 554, loss = 5.13589595\n",
      "Iteration 555, loss = 5.11442644\n",
      "Iteration 556, loss = 5.09300413\n",
      "Iteration 557, loss = 5.07162913\n",
      "Iteration 558, loss = 5.05030158\n",
      "Iteration 559, loss = 5.02902162\n",
      "Iteration 560, loss = 5.00778936\n",
      "Iteration 561, loss = 4.98660493\n",
      "Iteration 562, loss = 4.96546846\n",
      "Iteration 563, loss = 4.94438008\n",
      "Iteration 564, loss = 4.92333990\n",
      "Iteration 565, loss = 4.90234806\n",
      "Iteration 566, loss = 4.88140466\n",
      "Iteration 567, loss = 4.86050984\n",
      "Iteration 568, loss = 4.83966371\n",
      "Iteration 569, loss = 4.81886639\n",
      "Iteration 570, loss = 4.79811799\n",
      "Iteration 571, loss = 4.77741863\n",
      "Iteration 572, loss = 4.75676843\n",
      "Iteration 573, loss = 4.73616750\n",
      "Iteration 574, loss = 4.71561596\n",
      "Iteration 575, loss = 4.69511390\n",
      "Iteration 576, loss = 4.67466144\n",
      "Iteration 577, loss = 4.65425870\n",
      "Iteration 578, loss = 4.63390578\n",
      "Iteration 579, loss = 4.61360278\n",
      "Iteration 580, loss = 4.59334981\n",
      "Iteration 581, loss = 4.57314697\n",
      "Iteration 582, loss = 4.55299438\n",
      "Iteration 583, loss = 4.53289212\n",
      "Iteration 584, loss = 4.51284031\n",
      "Iteration 585, loss = 4.49283903\n",
      "Iteration 586, loss = 4.47288839\n",
      "Iteration 587, loss = 4.45298849\n",
      "Iteration 588, loss = 4.43313942\n",
      "Iteration 589, loss = 4.41334128\n",
      "Iteration 590, loss = 4.39359416\n",
      "Iteration 591, loss = 4.37389815\n",
      "Iteration 592, loss = 4.35425334\n",
      "Iteration 593, loss = 4.33465983\n",
      "Iteration 594, loss = 4.31511770\n",
      "Iteration 595, loss = 4.29562705\n",
      "Iteration 596, loss = 4.27618795\n",
      "Iteration 597, loss = 4.25680049\n",
      "Iteration 598, loss = 4.23746476\n",
      "Iteration 599, loss = 4.21818085\n",
      "Iteration 600, loss = 4.19894882\n",
      "Iteration 601, loss = 4.17976878\n",
      "Iteration 602, loss = 4.16064078\n",
      "Iteration 603, loss = 4.14156491\n",
      "Iteration 604, loss = 4.12254126\n",
      "Iteration 605, loss = 4.10356989\n",
      "Iteration 606, loss = 4.08465088\n",
      "Iteration 607, loss = 4.06578430\n",
      "Iteration 608, loss = 4.04697023\n",
      "Iteration 609, loss = 4.02820874\n",
      "Iteration 610, loss = 4.00949989\n",
      "Iteration 611, loss = 3.99084377\n",
      "Iteration 612, loss = 3.97224042\n",
      "Iteration 613, loss = 3.95368993\n",
      "Iteration 614, loss = 3.93519235\n",
      "Iteration 615, loss = 3.91674775\n",
      "Iteration 616, loss = 3.89835620\n",
      "Iteration 617, loss = 3.88001775\n",
      "Iteration 618, loss = 3.86173247\n",
      "Iteration 619, loss = 3.84350041\n",
      "Iteration 620, loss = 3.82532164\n",
      "Iteration 621, loss = 3.80719621\n",
      "Iteration 622, loss = 3.78912417\n",
      "Iteration 623, loss = 3.77110558\n",
      "Iteration 624, loss = 3.75314050\n",
      "Iteration 625, loss = 3.73522898\n",
      "Iteration 626, loss = 3.71737106\n",
      "Iteration 627, loss = 3.69956680\n",
      "Iteration 628, loss = 3.68181625\n",
      "Iteration 629, loss = 3.66411945\n",
      "Iteration 630, loss = 3.64647646\n",
      "Iteration 631, loss = 3.62888730\n",
      "Iteration 632, loss = 3.61135204\n",
      "Iteration 633, loss = 3.59387071\n",
      "Iteration 634, loss = 3.57644335\n",
      "Iteration 635, loss = 3.55907001\n",
      "Iteration 636, loss = 3.54175072\n",
      "Iteration 637, loss = 3.52448552\n",
      "Iteration 638, loss = 3.50727445\n",
      "Iteration 639, loss = 3.49011754\n",
      "Iteration 640, loss = 3.47301483\n",
      "Iteration 641, loss = 3.45596635\n",
      "Iteration 642, loss = 3.43897214\n",
      "Iteration 643, loss = 3.42203222\n",
      "Iteration 644, loss = 3.40514662\n",
      "Iteration 645, loss = 3.38831538\n",
      "Iteration 646, loss = 3.37153852\n",
      "Iteration 647, loss = 3.35481606\n",
      "Iteration 648, loss = 3.33814803\n",
      "Iteration 649, loss = 3.32153446\n",
      "Iteration 650, loss = 3.30497537\n",
      "Iteration 651, loss = 3.28847077\n",
      "Iteration 652, loss = 3.27202070\n",
      "Iteration 653, loss = 3.25562516\n",
      "Iteration 654, loss = 3.23928425\n",
      "Iteration 655, loss = 3.22299793\n",
      "Iteration 656, loss = 3.20676621\n",
      "Iteration 657, loss = 3.19058909\n",
      "Iteration 658, loss = 3.17446659\n",
      "Iteration 659, loss = 3.15839871\n",
      "Iteration 660, loss = 3.14238547\n",
      "Iteration 661, loss = 3.12642687\n",
      "Iteration 662, loss = 3.11052293\n",
      "Iteration 663, loss = 3.09467365\n",
      "Iteration 664, loss = 3.07887903\n",
      "Iteration 665, loss = 3.06313909\n",
      "Iteration 666, loss = 3.04745382\n",
      "Iteration 667, loss = 3.03182323\n",
      "Iteration 668, loss = 3.01624732\n",
      "Iteration 669, loss = 3.00072608\n",
      "Iteration 670, loss = 2.98525953\n",
      "Iteration 671, loss = 2.96984765\n",
      "Iteration 672, loss = 2.95449045\n",
      "Iteration 673, loss = 2.93918792\n",
      "Iteration 674, loss = 2.92394005\n",
      "Iteration 675, loss = 2.90874685\n",
      "Iteration 676, loss = 2.89360829\n",
      "Iteration 677, loss = 2.87852437\n",
      "Iteration 678, loss = 2.86349509\n",
      "Iteration 679, loss = 2.84852043\n",
      "Iteration 680, loss = 2.83360039\n",
      "Iteration 681, loss = 2.81873494\n",
      "Iteration 682, loss = 2.80392407\n",
      "Iteration 683, loss = 2.78916777\n",
      "Iteration 684, loss = 2.77446602\n",
      "Iteration 685, loss = 2.75981881\n",
      "Iteration 686, loss = 2.74522612\n",
      "Iteration 687, loss = 2.73068792\n",
      "Iteration 688, loss = 2.71620419\n",
      "Iteration 689, loss = 2.70177492\n",
      "Iteration 690, loss = 2.68740008\n",
      "Iteration 691, loss = 2.67307965\n",
      "Iteration 692, loss = 2.65881360\n",
      "Iteration 693, loss = 2.64460191\n",
      "Iteration 694, loss = 2.63044454\n",
      "Iteration 695, loss = 2.61634148\n",
      "Iteration 696, loss = 2.60229268\n",
      "Iteration 697, loss = 2.58829813\n",
      "Iteration 698, loss = 2.57435778\n",
      "Iteration 699, loss = 2.56047161\n",
      "Iteration 700, loss = 2.54663959\n",
      "Iteration 701, loss = 2.53286167\n",
      "Iteration 702, loss = 2.51913782\n",
      "Iteration 703, loss = 2.50546800\n",
      "Iteration 704, loss = 2.49185218\n",
      "Iteration 705, loss = 2.47829032\n",
      "Iteration 706, loss = 2.46478238\n",
      "Iteration 707, loss = 2.45132831\n",
      "Iteration 708, loss = 2.43792808\n",
      "Iteration 709, loss = 2.42458164\n",
      "Iteration 710, loss = 2.41128894\n",
      "Iteration 711, loss = 2.39804995\n",
      "Iteration 712, loss = 2.38486461\n",
      "Iteration 713, loss = 2.37173288\n",
      "Iteration 714, loss = 2.35865470\n",
      "Iteration 715, loss = 2.34563004\n",
      "Iteration 716, loss = 2.33265884\n",
      "Iteration 717, loss = 2.31974104\n",
      "Iteration 718, loss = 2.30687660\n",
      "Iteration 719, loss = 2.29406547\n",
      "Iteration 720, loss = 2.28130758\n",
      "Iteration 721, loss = 2.26860288\n",
      "Iteration 722, loss = 2.25595132\n",
      "Iteration 723, loss = 2.24335284\n",
      "Iteration 724, loss = 2.23080737\n",
      "Iteration 725, loss = 2.21831487\n",
      "Iteration 726, loss = 2.20587527\n",
      "Iteration 727, loss = 2.19348851\n",
      "Iteration 728, loss = 2.18115452\n",
      "Iteration 729, loss = 2.16887325\n",
      "Iteration 730, loss = 2.15664462\n",
      "Iteration 731, loss = 2.14446858\n",
      "Iteration 732, loss = 2.13234506\n",
      "Iteration 733, loss = 2.12027399\n",
      "Iteration 734, loss = 2.10825531\n",
      "Iteration 735, loss = 2.09628893\n",
      "Iteration 736, loss = 2.08437481\n",
      "Iteration 737, loss = 2.07251285\n",
      "Iteration 738, loss = 2.06070300\n",
      "Iteration 739, loss = 2.04894517\n",
      "Iteration 740, loss = 2.03723930\n",
      "Iteration 741, loss = 2.02558532\n",
      "Iteration 742, loss = 2.01398314\n",
      "Iteration 743, loss = 2.00243268\n",
      "Iteration 744, loss = 1.99093388\n",
      "Iteration 745, loss = 1.97948666\n",
      "Iteration 746, loss = 1.96809093\n",
      "Iteration 747, loss = 1.95674661\n",
      "Iteration 748, loss = 1.94545363\n",
      "Iteration 749, loss = 1.93421191\n",
      "Iteration 750, loss = 1.92302135\n",
      "Iteration 751, loss = 1.91188189\n",
      "Iteration 752, loss = 1.90079342\n",
      "Iteration 753, loss = 1.88975588\n",
      "Iteration 754, loss = 1.87876917\n",
      "Iteration 755, loss = 1.86783320\n",
      "Iteration 756, loss = 1.85694789\n",
      "Iteration 757, loss = 1.84611316\n",
      "Iteration 758, loss = 1.83532890\n",
      "Iteration 759, loss = 1.82459504\n",
      "Iteration 760, loss = 1.81391147\n",
      "Iteration 761, loss = 1.80327812\n",
      "Iteration 762, loss = 1.79269488\n",
      "Iteration 763, loss = 1.78216166\n",
      "Iteration 764, loss = 1.77167838\n",
      "Iteration 765, loss = 1.76124493\n",
      "Iteration 766, loss = 1.75086121\n",
      "Iteration 767, loss = 1.74052715\n",
      "Iteration 768, loss = 1.73024262\n",
      "Iteration 769, loss = 1.72000754\n",
      "Iteration 770, loss = 1.70982182\n",
      "Iteration 771, loss = 1.69968534\n",
      "Iteration 772, loss = 1.68959801\n",
      "Iteration 773, loss = 1.67955973\n",
      "Iteration 774, loss = 1.66957040\n",
      "Iteration 775, loss = 1.65962991\n",
      "Iteration 776, loss = 1.64973817\n",
      "Iteration 777, loss = 1.63989506\n",
      "Iteration 778, loss = 1.63010049\n",
      "Iteration 779, loss = 1.62035434\n",
      "Iteration 780, loss = 1.61065652\n",
      "Iteration 781, loss = 1.60100691\n",
      "Iteration 782, loss = 1.59140541\n",
      "Iteration 783, loss = 1.58185191\n",
      "Iteration 784, loss = 1.57234631\n",
      "Iteration 785, loss = 1.56288848\n",
      "Iteration 786, loss = 1.55347833\n",
      "Iteration 787, loss = 1.54411574\n",
      "Iteration 788, loss = 1.53480060\n",
      "Iteration 789, loss = 1.52553280\n",
      "Iteration 790, loss = 1.51631222\n",
      "Iteration 791, loss = 1.50713876\n",
      "Iteration 792, loss = 1.49801230\n",
      "Iteration 793, loss = 1.48893272\n",
      "Iteration 794, loss = 1.47989991\n",
      "Iteration 795, loss = 1.47091375\n",
      "Iteration 796, loss = 1.46197413\n",
      "Iteration 797, loss = 1.45308093\n",
      "Iteration 798, loss = 1.44423403\n",
      "Iteration 799, loss = 1.43543331\n",
      "Iteration 800, loss = 1.42667867\n",
      "Iteration 801, loss = 1.41796996\n",
      "Iteration 802, loss = 1.40930709\n",
      "Iteration 803, loss = 1.40068992\n",
      "Iteration 804, loss = 1.39211833\n",
      "Iteration 805, loss = 1.38359221\n",
      "Iteration 806, loss = 1.37511143\n",
      "Iteration 807, loss = 1.36667586\n",
      "Iteration 808, loss = 1.35828540\n",
      "Iteration 809, loss = 1.34993990\n",
      "Iteration 810, loss = 1.34163925\n",
      "Iteration 811, loss = 1.33338332\n",
      "Iteration 812, loss = 1.32517200\n",
      "Iteration 813, loss = 1.31700508\n",
      "Iteration 814, loss = 1.30888250\n",
      "Iteration 815, loss = 1.30080412\n",
      "Iteration 816, loss = 1.29276982\n",
      "Iteration 817, loss = 1.28477948\n",
      "Iteration 818, loss = 1.27683297\n",
      "Iteration 819, loss = 1.26893015\n",
      "Iteration 820, loss = 1.26107091\n",
      "Iteration 821, loss = 1.25325510\n",
      "Iteration 822, loss = 1.24548261\n",
      "Iteration 823, loss = 1.23775330\n",
      "Iteration 824, loss = 1.23006703\n",
      "Iteration 825, loss = 1.22242369\n",
      "Iteration 826, loss = 1.21482313\n",
      "Iteration 827, loss = 1.20726523\n",
      "Iteration 828, loss = 1.19974985\n",
      "Iteration 829, loss = 1.19227686\n",
      "Iteration 830, loss = 1.18484613\n",
      "Iteration 831, loss = 1.17745752\n",
      "Iteration 832, loss = 1.17011090\n",
      "Iteration 833, loss = 1.16280614\n",
      "Iteration 834, loss = 1.15554309\n",
      "Iteration 835, loss = 1.14832163\n",
      "Iteration 836, loss = 1.14114162\n",
      "Iteration 837, loss = 1.13400292\n",
      "Iteration 838, loss = 1.12690539\n",
      "Iteration 839, loss = 1.11984891\n",
      "Iteration 840, loss = 1.11283333\n",
      "Iteration 841, loss = 1.10585852\n",
      "Iteration 842, loss = 1.09892433\n",
      "Iteration 843, loss = 1.09203064\n",
      "Iteration 844, loss = 1.08517730\n",
      "Iteration 845, loss = 1.07836417\n",
      "Iteration 846, loss = 1.07159111\n",
      "Iteration 847, loss = 1.06485799\n",
      "Iteration 848, loss = 1.05816467\n",
      "Iteration 849, loss = 1.05151100\n",
      "Iteration 850, loss = 1.04489685\n",
      "Iteration 851, loss = 1.03832208\n",
      "Iteration 852, loss = 1.03178654\n",
      "Iteration 853, loss = 1.02529010\n",
      "Iteration 854, loss = 1.01883261\n",
      "Iteration 855, loss = 1.01241394\n",
      "Iteration 856, loss = 1.00603393\n",
      "Iteration 857, loss = 0.99969246\n",
      "Iteration 858, loss = 0.99338937\n",
      "Iteration 859, loss = 0.98712453\n",
      "Iteration 860, loss = 0.98089779\n",
      "Iteration 861, loss = 0.97470902\n",
      "Iteration 862, loss = 0.96855806\n",
      "Iteration 863, loss = 0.96244477\n",
      "Iteration 864, loss = 0.95636902\n",
      "Iteration 865, loss = 0.95033066\n",
      "Iteration 866, loss = 0.94432954\n",
      "Iteration 867, loss = 0.93836553\n",
      "Iteration 868, loss = 0.93243847\n",
      "Iteration 869, loss = 0.92654823\n",
      "Iteration 870, loss = 0.92069465\n",
      "Iteration 871, loss = 0.91487760\n",
      "Iteration 872, loss = 0.90909694\n",
      "Iteration 873, loss = 0.90335251\n",
      "Iteration 874, loss = 0.89764417\n",
      "Iteration 875, loss = 0.89197178\n",
      "Iteration 876, loss = 0.88633519\n",
      "Iteration 877, loss = 0.88073427\n",
      "Iteration 878, loss = 0.87516885\n",
      "Iteration 879, loss = 0.86963880\n",
      "Iteration 880, loss = 0.86414397\n",
      "Iteration 881, loss = 0.85868423\n",
      "Iteration 882, loss = 0.85325941\n",
      "Iteration 883, loss = 0.84786938\n",
      "Iteration 884, loss = 0.84251399\n",
      "Iteration 885, loss = 0.83719309\n",
      "Iteration 886, loss = 0.83190655\n",
      "Iteration 887, loss = 0.82665421\n",
      "Iteration 888, loss = 0.82143593\n",
      "Iteration 889, loss = 0.81625156\n",
      "Iteration 890, loss = 0.81110095\n",
      "Iteration 891, loss = 0.80598397\n",
      "Iteration 892, loss = 0.80090047\n",
      "Iteration 893, loss = 0.79585029\n",
      "Iteration 894, loss = 0.79083330\n",
      "Iteration 895, loss = 0.78584934\n",
      "Iteration 896, loss = 0.78089828\n",
      "Iteration 897, loss = 0.77597996\n",
      "Iteration 898, loss = 0.77109425\n",
      "Iteration 899, loss = 0.76624099\n",
      "Iteration 900, loss = 0.76142004\n",
      "Iteration 901, loss = 0.75663125\n",
      "Iteration 902, loss = 0.75187448\n",
      "Iteration 903, loss = 0.74714958\n",
      "Iteration 904, loss = 0.74245641\n",
      "Iteration 905, loss = 0.73779482\n",
      "Iteration 906, loss = 0.73316466\n",
      "Iteration 907, loss = 0.72856580\n",
      "Iteration 908, loss = 0.72399808\n",
      "Iteration 909, loss = 0.71946136\n",
      "Iteration 910, loss = 0.71495549\n",
      "Iteration 911, loss = 0.71048034\n",
      "Iteration 912, loss = 0.70603575\n",
      "Iteration 913, loss = 0.70162158\n",
      "Iteration 914, loss = 0.69723768\n",
      "Iteration 915, loss = 0.69288391\n",
      "Iteration 916, loss = 0.68856013\n",
      "Iteration 917, loss = 0.68426619\n",
      "Iteration 918, loss = 0.68000195\n",
      "Iteration 919, loss = 0.67576726\n",
      "Iteration 920, loss = 0.67156198\n",
      "Iteration 921, loss = 0.66738597\n",
      "Iteration 922, loss = 0.66323907\n",
      "Iteration 923, loss = 0.65912116\n",
      "Iteration 924, loss = 0.65503207\n",
      "Iteration 925, loss = 0.65097168\n",
      "Iteration 926, loss = 0.64693984\n",
      "Iteration 927, loss = 0.64293640\n",
      "Iteration 928, loss = 0.63896122\n",
      "Iteration 929, loss = 0.63501416\n",
      "Iteration 930, loss = 0.63109508\n",
      "Iteration 931, loss = 0.62720383\n",
      "Iteration 932, loss = 0.62334027\n",
      "Iteration 933, loss = 0.61950427\n",
      "Iteration 934, loss = 0.61569567\n",
      "Iteration 935, loss = 0.61191434\n",
      "Iteration 936, loss = 0.60816014\n",
      "Iteration 937, loss = 0.60443292\n",
      "Iteration 938, loss = 0.60073254\n",
      "Iteration 939, loss = 0.59705887\n",
      "Iteration 940, loss = 0.59341176\n",
      "Iteration 941, loss = 0.58979108\n",
      "Iteration 942, loss = 0.58619668\n",
      "Iteration 943, loss = 0.58262842\n",
      "Iteration 944, loss = 0.57908616\n",
      "Iteration 945, loss = 0.57556977\n",
      "Iteration 946, loss = 0.57207911\n",
      "Iteration 947, loss = 0.56861403\n",
      "Iteration 948, loss = 0.56517440\n",
      "Iteration 949, loss = 0.56176008\n",
      "Iteration 950, loss = 0.55837093\n",
      "Iteration 951, loss = 0.55500681\n",
      "Iteration 952, loss = 0.55166760\n",
      "Iteration 953, loss = 0.54835314\n",
      "Iteration 954, loss = 0.54506331\n",
      "Iteration 955, loss = 0.54179796\n",
      "Iteration 956, loss = 0.53855697\n",
      "Iteration 957, loss = 0.53534019\n",
      "Iteration 958, loss = 0.53214748\n",
      "Iteration 959, loss = 0.52897872\n",
      "Iteration 960, loss = 0.52583377\n",
      "Iteration 961, loss = 0.52271250\n",
      "Iteration 962, loss = 0.51961476\n",
      "Iteration 963, loss = 0.51654043\n",
      "Iteration 964, loss = 0.51348937\n",
      "Iteration 965, loss = 0.51046144\n",
      "Iteration 966, loss = 0.50745652\n",
      "Iteration 967, loss = 0.50447448\n",
      "Iteration 968, loss = 0.50151517\n",
      "Iteration 969, loss = 0.49857847\n",
      "Iteration 970, loss = 0.49566424\n",
      "Iteration 971, loss = 0.49277236\n",
      "Iteration 972, loss = 0.48990269\n",
      "Iteration 973, loss = 0.48705511\n",
      "Iteration 974, loss = 0.48422947\n",
      "Iteration 975, loss = 0.48142566\n",
      "Iteration 976, loss = 0.47864353\n",
      "Iteration 977, loss = 0.47588297\n",
      "Iteration 978, loss = 0.47314385\n",
      "Iteration 979, loss = 0.47042602\n",
      "Iteration 980, loss = 0.46772938\n",
      "Iteration 981, loss = 0.46505378\n",
      "Iteration 982, loss = 0.46239910\n",
      "Iteration 983, loss = 0.45976522\n",
      "Iteration 984, loss = 0.45715200\n",
      "Iteration 985, loss = 0.45455932\n",
      "Iteration 986, loss = 0.45198706\n",
      "Iteration 987, loss = 0.44943509\n",
      "Iteration 988, loss = 0.44690328\n",
      "Iteration 989, loss = 0.44439151\n",
      "Iteration 990, loss = 0.44189965\n",
      "Iteration 991, loss = 0.43942758\n",
      "Iteration 992, loss = 0.43697518\n",
      "Iteration 993, loss = 0.43454232\n",
      "Iteration 994, loss = 0.43212889\n",
      "Iteration 995, loss = 0.42973475\n",
      "Iteration 996, loss = 0.42735978\n",
      "Iteration 997, loss = 0.42500387\n",
      "Iteration 998, loss = 0.42266689\n",
      "Iteration 999, loss = 0.42034873\n",
      "Iteration 1000, loss = 0.41804926\n",
      "Iteration 1001, loss = 0.41576835\n",
      "Iteration 1002, loss = 0.41350590\n",
      "Iteration 1003, loss = 0.41126179\n",
      "Iteration 1004, loss = 0.40903588\n",
      "Iteration 1005, loss = 0.40682808\n",
      "Iteration 1006, loss = 0.40463825\n",
      "Iteration 1007, loss = 0.40246628\n",
      "Iteration 1008, loss = 0.40031206\n",
      "Iteration 1009, loss = 0.39817546\n",
      "Iteration 1010, loss = 0.39605637\n",
      "Iteration 1011, loss = 0.39395468\n",
      "Iteration 1012, loss = 0.39187027\n",
      "Iteration 1013, loss = 0.38980302\n",
      "Iteration 1014, loss = 0.38775282\n",
      "Iteration 1015, loss = 0.38571957\n",
      "Iteration 1016, loss = 0.38370313\n",
      "Iteration 1017, loss = 0.38170340\n",
      "Iteration 1018, loss = 0.37972028\n",
      "Iteration 1019, loss = 0.37775363\n",
      "Iteration 1020, loss = 0.37580336\n",
      "Iteration 1021, loss = 0.37386936\n",
      "Iteration 1022, loss = 0.37195150\n",
      "Iteration 1023, loss = 0.37004969\n",
      "Iteration 1024, loss = 0.36816381\n",
      "Iteration 1025, loss = 0.36629375\n",
      "Iteration 1026, loss = 0.36443940\n",
      "Iteration 1027, loss = 0.36260066\n",
      "Iteration 1028, loss = 0.36077741\n",
      "Iteration 1029, loss = 0.35896955\n",
      "Iteration 1030, loss = 0.35717698\n",
      "Iteration 1031, loss = 0.35539957\n",
      "Iteration 1032, loss = 0.35363724\n",
      "Iteration 1033, loss = 0.35188987\n",
      "Iteration 1034, loss = 0.35015736\n",
      "Iteration 1035, loss = 0.34843960\n",
      "Iteration 1036, loss = 0.34673649\n",
      "Iteration 1037, loss = 0.34504793\n",
      "Iteration 1038, loss = 0.34337380\n",
      "Iteration 1039, loss = 0.34171402\n",
      "Iteration 1040, loss = 0.34006846\n",
      "Iteration 1041, loss = 0.33843705\n",
      "Iteration 1042, loss = 0.33681966\n",
      "Iteration 1043, loss = 0.33521621\n",
      "Iteration 1044, loss = 0.33362659\n",
      "Iteration 1045, loss = 0.33205069\n",
      "Iteration 1046, loss = 0.33048843\n",
      "Iteration 1047, loss = 0.32893970\n",
      "Iteration 1048, loss = 0.32740440\n",
      "Iteration 1049, loss = 0.32588244\n",
      "Iteration 1050, loss = 0.32437371\n",
      "Iteration 1051, loss = 0.32287812\n",
      "Iteration 1052, loss = 0.32139557\n",
      "Iteration 1053, loss = 0.31992598\n",
      "Iteration 1054, loss = 0.31846923\n",
      "Iteration 1055, loss = 0.31702523\n",
      "Iteration 1056, loss = 0.31559390\n",
      "Iteration 1057, loss = 0.31417513\n",
      "Iteration 1058, loss = 0.31276883\n",
      "Iteration 1059, loss = 0.31137491\n",
      "Iteration 1060, loss = 0.30999328\n",
      "Iteration 1061, loss = 0.30862384\n",
      "Iteration 1062, loss = 0.30726649\n",
      "Iteration 1063, loss = 0.30592116\n",
      "Iteration 1064, loss = 0.30458775\n",
      "Iteration 1065, loss = 0.30326616\n",
      "Iteration 1066, loss = 0.30195630\n",
      "Iteration 1067, loss = 0.30065810\n",
      "Iteration 1068, loss = 0.29937145\n",
      "Iteration 1069, loss = 0.29809627\n",
      "Iteration 1070, loss = 0.29683248\n",
      "Iteration 1071, loss = 0.29557997\n",
      "Iteration 1072, loss = 0.29433867\n",
      "Iteration 1073, loss = 0.29310849\n",
      "Iteration 1074, loss = 0.29188935\n",
      "Iteration 1075, loss = 0.29068114\n",
      "Iteration 1076, loss = 0.28948380\n",
      "Iteration 1077, loss = 0.28829724\n",
      "Iteration 1078, loss = 0.28712136\n",
      "Iteration 1079, loss = 0.28595610\n",
      "Iteration 1080, loss = 0.28480136\n",
      "Iteration 1081, loss = 0.28365705\n",
      "Iteration 1082, loss = 0.28252311\n",
      "Iteration 1083, loss = 0.28139944\n",
      "Iteration 1084, loss = 0.28028597\n",
      "Iteration 1085, loss = 0.27918261\n",
      "Iteration 1086, loss = 0.27808928\n",
      "Iteration 1087, loss = 0.27700590\n",
      "Iteration 1088, loss = 0.27593240\n",
      "Iteration 1089, loss = 0.27486868\n",
      "Iteration 1090, loss = 0.27381469\n",
      "Iteration 1091, loss = 0.27277032\n",
      "Iteration 1092, loss = 0.27173552\n",
      "Iteration 1093, loss = 0.27071019\n",
      "Iteration 1094, loss = 0.26969427\n",
      "Iteration 1095, loss = 0.26868767\n",
      "Iteration 1096, loss = 0.26769032\n",
      "Iteration 1097, loss = 0.26670214\n",
      "Iteration 1098, loss = 0.26572307\n",
      "Iteration 1099, loss = 0.26475302\n",
      "Iteration 1100, loss = 0.26379192\n",
      "Iteration 1101, loss = 0.26283969\n",
      "Iteration 1102, loss = 0.26189627\n",
      "Iteration 1103, loss = 0.26096158\n",
      "Iteration 1104, loss = 0.26003554\n",
      "Iteration 1105, loss = 0.25911809\n",
      "Iteration 1106, loss = 0.25820915\n",
      "Iteration 1107, loss = 0.25730865\n",
      "Iteration 1108, loss = 0.25641653\n",
      "Iteration 1109, loss = 0.25553271\n",
      "Iteration 1110, loss = 0.25465711\n",
      "Iteration 1111, loss = 0.25378968\n",
      "Iteration 1112, loss = 0.25293035\n",
      "Iteration 1113, loss = 0.25207903\n",
      "Iteration 1114, loss = 0.25123568\n",
      "Iteration 1115, loss = 0.25040021\n",
      "Iteration 1116, loss = 0.24957256\n",
      "Iteration 1117, loss = 0.24875267\n",
      "Iteration 1118, loss = 0.24794047\n",
      "Iteration 1119, loss = 0.24713589\n",
      "Iteration 1120, loss = 0.24633886\n",
      "Iteration 1121, loss = 0.24554933\n",
      "Iteration 1122, loss = 0.24476722\n",
      "Iteration 1123, loss = 0.24399248\n",
      "Iteration 1124, loss = 0.24322504\n",
      "Iteration 1125, loss = 0.24246483\n",
      "Iteration 1126, loss = 0.24171180\n",
      "Iteration 1127, loss = 0.24096588\n",
      "Iteration 1128, loss = 0.24022700\n",
      "Iteration 1129, loss = 0.23949512\n",
      "Iteration 1130, loss = 0.23877015\n",
      "Iteration 1131, loss = 0.23805206\n",
      "Iteration 1132, loss = 0.23734077\n",
      "Iteration 1133, loss = 0.23663622\n",
      "Iteration 1134, loss = 0.23593836\n",
      "Iteration 1135, loss = 0.23524712\n",
      "Iteration 1136, loss = 0.23456245\n",
      "Iteration 1137, loss = 0.23388429\n",
      "Iteration 1138, loss = 0.23321254\n",
      "Iteration 1139, loss = 0.23254715\n",
      "Iteration 1140, loss = 0.23188810\n",
      "Iteration 1141, loss = 0.23123533\n",
      "Iteration 1142, loss = 0.23058877\n",
      "Iteration 1143, loss = 0.22994838\n",
      "Iteration 1144, loss = 0.22931410\n",
      "Iteration 1145, loss = 0.22868587\n",
      "Iteration 1146, loss = 0.22806364\n",
      "Iteration 1147, loss = 0.22744736\n",
      "Iteration 1148, loss = 0.22683697\n",
      "Iteration 1149, loss = 0.22623243\n",
      "Iteration 1150, loss = 0.22563366\n",
      "Iteration 1151, loss = 0.22504063\n",
      "Iteration 1152, loss = 0.22445329\n",
      "Iteration 1153, loss = 0.22387157\n",
      "Iteration 1154, loss = 0.22329543\n",
      "Iteration 1155, loss = 0.22272481\n",
      "Iteration 1156, loss = 0.22215968\n",
      "Iteration 1157, loss = 0.22159997\n",
      "Iteration 1158, loss = 0.22104563\n",
      "Iteration 1159, loss = 0.22049662\n",
      "Iteration 1160, loss = 0.21995289\n",
      "Iteration 1161, loss = 0.21941438\n",
      "Iteration 1162, loss = 0.21888106\n",
      "Iteration 1163, loss = 0.21835286\n",
      "Iteration 1164, loss = 0.21782975\n",
      "Iteration 1165, loss = 0.21731168\n",
      "Iteration 1166, loss = 0.21679859\n",
      "Iteration 1167, loss = 0.21629045\n",
      "Iteration 1168, loss = 0.21578720\n",
      "Iteration 1169, loss = 0.21528879\n",
      "Iteration 1170, loss = 0.21479520\n",
      "Iteration 1171, loss = 0.21430636\n",
      "Iteration 1172, loss = 0.21382223\n",
      "Iteration 1173, loss = 0.21334277\n",
      "Iteration 1174, loss = 0.21286794\n",
      "Iteration 1175, loss = 0.21239768\n",
      "Iteration 1176, loss = 0.21193196\n",
      "Iteration 1177, loss = 0.21147073\n",
      "Iteration 1178, loss = 0.21101396\n",
      "Iteration 1179, loss = 0.21056159\n",
      "Iteration 1180, loss = 0.21011358\n",
      "Iteration 1181, loss = 0.20966990\n",
      "Iteration 1182, loss = 0.20923050\n",
      "Iteration 1183, loss = 0.20879534\n",
      "Iteration 1184, loss = 0.20836437\n",
      "Iteration 1185, loss = 0.20793757\n",
      "Iteration 1186, loss = 0.20751488\n",
      "Iteration 1187, loss = 0.20709627\n",
      "Iteration 1188, loss = 0.20668170\n",
      "Iteration 1189, loss = 0.20627113\n",
      "Iteration 1190, loss = 0.20586451\n",
      "Iteration 1191, loss = 0.20546182\n",
      "Iteration 1192, loss = 0.20506301\n",
      "Iteration 1193, loss = 0.20466804\n",
      "Iteration 1194, loss = 0.20427688\n",
      "Iteration 1195, loss = 0.20388949\n",
      "Iteration 1196, loss = 0.20350583\n",
      "Iteration 1197, loss = 0.20312586\n",
      "Iteration 1198, loss = 0.20274956\n",
      "Iteration 1199, loss = 0.20237687\n",
      "Iteration 1200, loss = 0.20200777\n",
      "Iteration 1201, loss = 0.20164222\n",
      "Iteration 1202, loss = 0.20128019\n",
      "Iteration 1203, loss = 0.20092163\n",
      "Iteration 1204, loss = 0.20056652\n",
      "Iteration 1205, loss = 0.20021482\n",
      "Iteration 1206, loss = 0.19986650\n",
      "Iteration 1207, loss = 0.19952152\n",
      "Iteration 1208, loss = 0.19917984\n",
      "Iteration 1209, loss = 0.19884145\n",
      "Iteration 1210, loss = 0.19850629\n",
      "Iteration 1211, loss = 0.19817434\n",
      "Iteration 1212, loss = 0.19784557\n",
      "Iteration 1213, loss = 0.19751995\n",
      "Iteration 1214, loss = 0.19719744\n",
      "Iteration 1215, loss = 0.19687801\n",
      "Iteration 1216, loss = 0.19656162\n",
      "Iteration 1217, loss = 0.19624826\n",
      "Iteration 1218, loss = 0.19593788\n",
      "Iteration 1219, loss = 0.19563047\n",
      "Iteration 1220, loss = 0.19532597\n",
      "Iteration 1221, loss = 0.19502438\n",
      "Iteration 1222, loss = 0.19472565\n",
      "Iteration 1223, loss = 0.19442976\n",
      "Iteration 1224, loss = 0.19413667\n",
      "Iteration 1225, loss = 0.19384637\n",
      "Iteration 1226, loss = 0.19355881\n",
      "Iteration 1227, loss = 0.19327398\n",
      "Iteration 1228, loss = 0.19299184\n",
      "Iteration 1229, loss = 0.19271236\n",
      "Iteration 1230, loss = 0.19243553\n",
      "Iteration 1231, loss = 0.19216130\n",
      "Iteration 1232, loss = 0.19188966\n",
      "Iteration 1233, loss = 0.19162057\n",
      "Iteration 1234, loss = 0.19135401\n",
      "Iteration 1235, loss = 0.19108995\n",
      "Iteration 1236, loss = 0.19082837\n",
      "Iteration 1237, loss = 0.19056924\n",
      "Iteration 1238, loss = 0.19031253\n",
      "Iteration 1239, loss = 0.19005822\n",
      "Iteration 1240, loss = 0.18980628\n",
      "Iteration 1241, loss = 0.18955670\n",
      "Iteration 1242, loss = 0.18930943\n",
      "Iteration 1243, loss = 0.18906447\n",
      "Iteration 1244, loss = 0.18882177\n",
      "Iteration 1245, loss = 0.18858133\n",
      "Iteration 1246, loss = 0.18834312\n",
      "Iteration 1247, loss = 0.18810710\n",
      "Iteration 1248, loss = 0.18787327\n",
      "Iteration 1249, loss = 0.18764158\n",
      "Iteration 1250, loss = 0.18741204\n",
      "Iteration 1251, loss = 0.18718460\n",
      "Iteration 1252, loss = 0.18695924\n",
      "Iteration 1253, loss = 0.18673595\n",
      "Iteration 1254, loss = 0.18651471\n",
      "Iteration 1255, loss = 0.18629548\n",
      "Iteration 1256, loss = 0.18607825\n",
      "Iteration 1257, loss = 0.18586299\n",
      "Iteration 1258, loss = 0.18564969\n",
      "Iteration 1259, loss = 0.18543832\n",
      "Iteration 1260, loss = 0.18522887\n",
      "Iteration 1261, loss = 0.18502131\n",
      "Iteration 1262, loss = 0.18481561\n",
      "Iteration 1263, loss = 0.18461177\n",
      "Iteration 1264, loss = 0.18440976\n",
      "Iteration 1265, loss = 0.18420956\n",
      "Iteration 1266, loss = 0.18401114\n",
      "Iteration 1267, loss = 0.18381450\n",
      "Iteration 1268, loss = 0.18361961\n",
      "Iteration 1269, loss = 0.18342645\n",
      "Iteration 1270, loss = 0.18323500\n",
      "Iteration 1271, loss = 0.18304525\n",
      "Iteration 1272, loss = 0.18285717\n",
      "Iteration 1273, loss = 0.18267075\n",
      "Iteration 1274, loss = 0.18248596\n",
      "Iteration 1275, loss = 0.18230280\n",
      "Iteration 1276, loss = 0.18212123\n",
      "Iteration 1277, loss = 0.18194125\n",
      "Iteration 1278, loss = 0.18176284\n",
      "Iteration 1279, loss = 0.18158597\n",
      "Iteration 1280, loss = 0.18141064\n",
      "Iteration 1281, loss = 0.18123682\n",
      "Iteration 1282, loss = 0.18106450\n",
      "Iteration 1283, loss = 0.18089365\n",
      "Iteration 1284, loss = 0.18072427\n",
      "Iteration 1285, loss = 0.18055634\n",
      "Iteration 1286, loss = 0.18038984\n",
      "Iteration 1287, loss = 0.18022475\n",
      "Iteration 1288, loss = 0.18006106\n",
      "Iteration 1289, loss = 0.17989876\n",
      "Iteration 1290, loss = 0.17973782\n",
      "Iteration 1291, loss = 0.17957823\n",
      "Iteration 1292, loss = 0.17941998\n",
      "Iteration 1293, loss = 0.17926305\n",
      "Iteration 1294, loss = 0.17910742\n",
      "Iteration 1295, loss = 0.17895309\n",
      "Iteration 1296, loss = 0.17880003\n",
      "Iteration 1297, loss = 0.17864823\n",
      "Iteration 1298, loss = 0.17849768\n",
      "Iteration 1299, loss = 0.17834836\n",
      "Iteration 1300, loss = 0.17820027\n",
      "Iteration 1301, loss = 0.17805337\n",
      "Iteration 1302, loss = 0.17790767\n",
      "Iteration 1303, loss = 0.17776314\n",
      "Iteration 1304, loss = 0.17761978\n",
      "Iteration 1305, loss = 0.17747756\n",
      "Iteration 1306, loss = 0.17733648\n",
      "Iteration 1307, loss = 0.17719653\n",
      "Iteration 1308, loss = 0.17705768\n",
      "Iteration 1309, loss = 0.17691994\n",
      "Iteration 1310, loss = 0.17678327\n",
      "Iteration 1311, loss = 0.17664768\n",
      "Iteration 1312, loss = 0.17651315\n",
      "Iteration 1313, loss = 0.17637967\n",
      "Iteration 1314, loss = 0.17624722\n",
      "Iteration 1315, loss = 0.17611579\n",
      "Iteration 1316, loss = 0.17598537\n",
      "Iteration 1317, loss = 0.17585595\n",
      "Iteration 1318, loss = 0.17572752\n",
      "Iteration 1319, loss = 0.17560006\n",
      "Iteration 1320, loss = 0.17547357\n",
      "Iteration 1321, loss = 0.17534803\n",
      "Iteration 1322, loss = 0.17522343\n",
      "Iteration 1323, loss = 0.17509976\n",
      "Iteration 1324, loss = 0.17497701\n",
      "Iteration 1325, loss = 0.17485516\n",
      "Iteration 1326, loss = 0.17473422\n",
      "Iteration 1327, loss = 0.17461416\n",
      "Iteration 1328, loss = 0.17449497\n",
      "Iteration 1329, loss = 0.17437666\n",
      "Iteration 1330, loss = 0.17425919\n",
      "Iteration 1331, loss = 0.17414257\n",
      "Iteration 1332, loss = 0.17402679\n",
      "Iteration 1333, loss = 0.17391183\n",
      "Iteration 1334, loss = 0.17379769\n",
      "Iteration 1335, loss = 0.17368435\n",
      "Iteration 1336, loss = 0.17357180\n",
      "Iteration 1337, loss = 0.17346004\n",
      "Iteration 1338, loss = 0.17334906\n",
      "Iteration 1339, loss = 0.17323884\n",
      "Iteration 1340, loss = 0.17312939\n",
      "Iteration 1341, loss = 0.17302068\n",
      "Iteration 1342, loss = 0.17291271\n",
      "Iteration 1343, loss = 0.17280547\n",
      "Iteration 1344, loss = 0.17269895\n",
      "Iteration 1345, loss = 0.17259314\n",
      "Iteration 1346, loss = 0.17248804\n",
      "Iteration 1347, loss = 0.17238363\n",
      "Iteration 1348, loss = 0.17227991\n",
      "Iteration 1349, loss = 0.17217687\n",
      "Iteration 1350, loss = 0.17207450\n",
      "Iteration 1351, loss = 0.17197279\n",
      "Iteration 1352, loss = 0.17187173\n",
      "Iteration 1353, loss = 0.17177132\n",
      "Iteration 1354, loss = 0.17167154\n",
      "Iteration 1355, loss = 0.17157240\n",
      "Iteration 1356, loss = 0.17147388\n",
      "Iteration 1357, loss = 0.17137597\n",
      "Iteration 1358, loss = 0.17127867\n",
      "Iteration 1359, loss = 0.17118197\n",
      "Iteration 1360, loss = 0.17108586\n",
      "Iteration 1361, loss = 0.17099034\n",
      "Iteration 1362, loss = 0.17089539\n",
      "Iteration 1363, loss = 0.17080101\n",
      "Iteration 1364, loss = 0.17070720\n",
      "Iteration 1365, loss = 0.17061394\n",
      "Iteration 1366, loss = 0.17052123\n",
      "Iteration 1367, loss = 0.17042906\n",
      "Iteration 1368, loss = 0.17033743\n",
      "Iteration 1369, loss = 0.17024633\n",
      "Iteration 1370, loss = 0.17015575\n",
      "Iteration 1371, loss = 0.17006568\n",
      "Iteration 1372, loss = 0.16997612\n",
      "Iteration 1373, loss = 0.16988707\n",
      "Iteration 1374, loss = 0.16979851\n",
      "Iteration 1375, loss = 0.16971044\n",
      "Iteration 1376, loss = 0.16962285\n",
      "Iteration 1377, loss = 0.16953574\n",
      "Iteration 1378, loss = 0.16944911\n",
      "Iteration 1379, loss = 0.16936293\n",
      "Iteration 1380, loss = 0.16927722\n",
      "Iteration 1381, loss = 0.16919196\n",
      "Iteration 1382, loss = 0.16910715\n",
      "Iteration 1383, loss = 0.16902278\n",
      "Iteration 1384, loss = 0.16893885\n",
      "Iteration 1385, loss = 0.16885535\n",
      "Iteration 1386, loss = 0.16877228\n",
      "Iteration 1387, loss = 0.16868962\n",
      "Iteration 1388, loss = 0.16860738\n",
      "Iteration 1389, loss = 0.16852555\n",
      "Iteration 1390, loss = 0.16844412\n",
      "Iteration 1391, loss = 0.16836309\n",
      "Iteration 1392, loss = 0.16828246\n",
      "Iteration 1393, loss = 0.16820221\n",
      "Iteration 1394, loss = 0.16812235\n",
      "Iteration 1395, loss = 0.16804286\n",
      "Iteration 1396, loss = 0.16796375\n",
      "Iteration 1397, loss = 0.16788501\n",
      "Iteration 1398, loss = 0.16780663\n",
      "Iteration 1399, loss = 0.16772861\n",
      "Iteration 1400, loss = 0.16765095\n",
      "Iteration 1401, loss = 0.16757363\n",
      "Iteration 1402, loss = 0.16749666\n",
      "Iteration 1403, loss = 0.16742004\n",
      "Iteration 1404, loss = 0.16734374\n",
      "Iteration 1405, loss = 0.16726778\n",
      "Iteration 1406, loss = 0.16719215\n",
      "Iteration 1407, loss = 0.16711684\n",
      "Iteration 1408, loss = 0.16704185\n",
      "Iteration 1409, loss = 0.16696718\n",
      "Iteration 1410, loss = 0.16689281\n",
      "Iteration 1411, loss = 0.16681876\n",
      "Iteration 1412, loss = 0.16674500\n",
      "Iteration 1413, loss = 0.16667155\n",
      "Iteration 1414, loss = 0.16659838\n",
      "Iteration 1415, loss = 0.16652551\n",
      "Iteration 1416, loss = 0.16645293\n",
      "Iteration 1417, loss = 0.16638063\n",
      "Iteration 1418, loss = 0.16630861\n",
      "Iteration 1419, loss = 0.16623686\n",
      "Iteration 1420, loss = 0.16616539\n",
      "Iteration 1421, loss = 0.16609418\n",
      "Iteration 1422, loss = 0.16602324\n",
      "Iteration 1423, loss = 0.16595256\n",
      "Iteration 1424, loss = 0.16588214\n",
      "Iteration 1425, loss = 0.16581197\n",
      "Iteration 1426, loss = 0.16574205\n",
      "Iteration 1427, loss = 0.16567238\n",
      "Iteration 1428, loss = 0.16560295\n",
      "Iteration 1429, loss = 0.16553377\n",
      "Iteration 1430, loss = 0.16546482\n",
      "Iteration 1431, loss = 0.16539611\n",
      "Iteration 1432, loss = 0.16532763\n",
      "Iteration 1433, loss = 0.16525937\n",
      "Iteration 1434, loss = 0.16519134\n",
      "Iteration 1435, loss = 0.16512353\n",
      "Iteration 1436, loss = 0.16505594\n",
      "Iteration 1437, loss = 0.16498857\n",
      "Iteration 1438, loss = 0.16492141\n",
      "Iteration 1439, loss = 0.16485446\n",
      "Iteration 1440, loss = 0.16478772\n",
      "Iteration 1441, loss = 0.16472118\n",
      "Iteration 1442, loss = 0.16465484\n",
      "Iteration 1443, loss = 0.16458870\n",
      "Iteration 1444, loss = 0.16452276\n",
      "Iteration 1445, loss = 0.16445701\n",
      "Iteration 1446, loss = 0.16439145\n",
      "Iteration 1447, loss = 0.16432608\n",
      "Iteration 1448, loss = 0.16426090\n",
      "Iteration 1449, loss = 0.16419589\n",
      "Iteration 1450, loss = 0.16413107\n",
      "Iteration 1451, loss = 0.16406643\n",
      "Iteration 1452, loss = 0.16400196\n",
      "Iteration 1453, loss = 0.16393766\n",
      "Iteration 1454, loss = 0.16387354\n",
      "Iteration 1455, loss = 0.16380958\n",
      "Iteration 1456, loss = 0.16374579\n",
      "Iteration 1457, loss = 0.16368216\n",
      "Iteration 1458, loss = 0.16361870\n",
      "Iteration 1459, loss = 0.16355539\n",
      "Iteration 1460, loss = 0.16349224\n",
      "Iteration 1461, loss = 0.16342924\n",
      "Iteration 1462, loss = 0.16336640\n",
      "Iteration 1463, loss = 0.16330370\n",
      "Iteration 1464, loss = 0.16324116\n",
      "Iteration 1465, loss = 0.16317876\n",
      "Iteration 1466, loss = 0.16311650\n",
      "Iteration 1467, loss = 0.16305439\n",
      "Iteration 1468, loss = 0.16299242\n",
      "Iteration 1469, loss = 0.16293058\n",
      "Iteration 1470, loss = 0.16286888\n",
      "Iteration 1471, loss = 0.16280731\n",
      "Iteration 1472, loss = 0.16274588\n",
      "Iteration 1473, loss = 0.16268458\n",
      "Iteration 1474, loss = 0.16262340\n",
      "Iteration 1475, loss = 0.16256236\n",
      "Iteration 1476, loss = 0.16250143\n",
      "Iteration 1477, loss = 0.16244063\n",
      "Iteration 1478, loss = 0.16237995\n",
      "Iteration 1479, loss = 0.16231939\n",
      "Iteration 1480, loss = 0.16225895\n",
      "Iteration 1481, loss = 0.16219863\n",
      "Iteration 1482, loss = 0.16213841\n",
      "Iteration 1483, loss = 0.16207832\n",
      "Iteration 1484, loss = 0.16201833\n",
      "Iteration 1485, loss = 0.16195845\n",
      "Iteration 1486, loss = 0.16189868\n",
      "Iteration 1487, loss = 0.16183902\n",
      "Iteration 1488, loss = 0.16177946\n",
      "Iteration 1489, loss = 0.16172001\n",
      "Iteration 1490, loss = 0.16166065\n",
      "Iteration 1491, loss = 0.16160140\n",
      "Iteration 1492, loss = 0.16154225\n",
      "Iteration 1493, loss = 0.16148319\n",
      "Iteration 1494, loss = 0.16142423\n",
      "Iteration 1495, loss = 0.16136536\n",
      "Iteration 1496, loss = 0.16130659\n",
      "Iteration 1497, loss = 0.16124791\n",
      "Iteration 1498, loss = 0.16118932\n",
      "Iteration 1499, loss = 0.16113082\n",
      "Iteration 1500, loss = 0.16107241\n",
      "Iteration 1501, loss = 0.16101408\n",
      "Iteration 1502, loss = 0.16095584\n",
      "Iteration 1503, loss = 0.16089768\n",
      "Iteration 1504, loss = 0.16083961\n",
      "Iteration 1505, loss = 0.16078162\n",
      "Iteration 1506, loss = 0.16072370\n",
      "Iteration 1507, loss = 0.16066587\n",
      "Iteration 1508, loss = 0.16060812\n",
      "Iteration 1509, loss = 0.16055044\n",
      "Iteration 1510, loss = 0.16049284\n",
      "Iteration 1511, loss = 0.16043531\n",
      "Iteration 1512, loss = 0.16037786\n",
      "Iteration 1513, loss = 0.16032047\n",
      "Iteration 1514, loss = 0.16026316\n",
      "Iteration 1515, loss = 0.16020592\n",
      "Iteration 1516, loss = 0.16014875\n",
      "Iteration 1517, loss = 0.16009165\n",
      "Iteration 1518, loss = 0.16003461\n",
      "Iteration 1519, loss = 0.15997764\n",
      "Iteration 1520, loss = 0.15992074\n",
      "Iteration 1521, loss = 0.15986390\n",
      "Iteration 1522, loss = 0.15980712\n",
      "Iteration 1523, loss = 0.15975041\n",
      "Iteration 1524, loss = 0.15969376\n",
      "Iteration 1525, loss = 0.15963716\n",
      "Iteration 1526, loss = 0.15958063\n",
      "Iteration 1527, loss = 0.15952415\n",
      "Iteration 1528, loss = 0.15946774\n",
      "Iteration 1529, loss = 0.15941138\n",
      "Iteration 1530, loss = 0.15935507\n",
      "Iteration 1531, loss = 0.15929882\n",
      "Iteration 1532, loss = 0.15924262\n",
      "Iteration 1533, loss = 0.15918648\n",
      "Iteration 1534, loss = 0.15913039\n",
      "Iteration 1535, loss = 0.15907436\n",
      "Iteration 1536, loss = 0.15901837\n",
      "Iteration 1537, loss = 0.15896243\n",
      "Iteration 1538, loss = 0.15890654\n",
      "Iteration 1539, loss = 0.15885071\n",
      "Iteration 1540, loss = 0.15879492\n",
      "Iteration 1541, loss = 0.15873917\n",
      "Iteration 1542, loss = 0.15868347\n",
      "Iteration 1543, loss = 0.15862782\n",
      "Iteration 1544, loss = 0.15857222\n",
      "Iteration 1545, loss = 0.15851665\n",
      "Iteration 1546, loss = 0.15846114\n",
      "Iteration 1547, loss = 0.15840566\n",
      "Iteration 1548, loss = 0.15835023\n",
      "Iteration 1549, loss = 0.15829484\n",
      "Iteration 1550, loss = 0.15823949\n",
      "Iteration 1551, loss = 0.15818417\n",
      "Iteration 1552, loss = 0.15812890\n",
      "Iteration 1553, loss = 0.15807367\n",
      "Iteration 1554, loss = 0.15801848\n",
      "Iteration 1555, loss = 0.15796332\n",
      "Iteration 1556, loss = 0.15790821\n",
      "Iteration 1557, loss = 0.15785313\n",
      "Iteration 1558, loss = 0.15779808\n",
      "Iteration 1559, loss = 0.15774307\n",
      "Iteration 1560, loss = 0.15768810\n",
      "Iteration 1561, loss = 0.15763316\n",
      "Iteration 1562, loss = 0.15757825\n",
      "Iteration 1563, loss = 0.15752338\n",
      "Iteration 1564, loss = 0.15746854\n",
      "Iteration 1565, loss = 0.15741373\n",
      "Iteration 1566, loss = 0.15735895\n",
      "Iteration 1567, loss = 0.15730421\n",
      "Iteration 1568, loss = 0.15724950\n",
      "Iteration 1569, loss = 0.15719481\n",
      "Iteration 1570, loss = 0.15714016\n",
      "Iteration 1571, loss = 0.15708553\n",
      "Iteration 1572, loss = 0.15703094\n",
      "Iteration 1573, loss = 0.15697637\n",
      "Iteration 1574, loss = 0.15692183\n",
      "Iteration 1575, loss = 0.15686737\n",
      "Iteration 1576, loss = 0.15681301\n",
      "Iteration 1577, loss = 0.15675868\n",
      "Iteration 1578, loss = 0.15670438\n",
      "Iteration 1579, loss = 0.15665012\n",
      "Iteration 1580, loss = 0.15659590\n",
      "Iteration 1581, loss = 0.15654170\n",
      "Iteration 1582, loss = 0.15648754\n",
      "Iteration 1583, loss = 0.15643340\n",
      "Iteration 1584, loss = 0.15637930\n",
      "Iteration 1585, loss = 0.15632522\n",
      "Iteration 1586, loss = 0.15627118\n",
      "Iteration 1587, loss = 0.15621715\n",
      "Iteration 1588, loss = 0.15616316\n",
      "Iteration 1589, loss = 0.15610919\n",
      "Iteration 1590, loss = 0.15605525\n",
      "Iteration 1591, loss = 0.15600133\n",
      "Iteration 1592, loss = 0.15594743\n",
      "Iteration 1593, loss = 0.15589356\n",
      "Iteration 1594, loss = 0.15583971\n",
      "Iteration 1595, loss = 0.15578588\n",
      "Iteration 1596, loss = 0.15573208\n",
      "Iteration 1597, loss = 0.15567829\n",
      "Iteration 1598, loss = 0.15562453\n",
      "Iteration 1599, loss = 0.15557079\n",
      "Iteration 1600, loss = 0.15551707\n",
      "Iteration 1601, loss = 0.15546337\n",
      "Iteration 1602, loss = 0.15540969\n",
      "Iteration 1603, loss = 0.15535603\n",
      "Iteration 1604, loss = 0.15530239\n",
      "Iteration 1605, loss = 0.15524876\n",
      "Iteration 1606, loss = 0.15519516\n",
      "Iteration 1607, loss = 0.15514157\n",
      "Iteration 1608, loss = 0.15508800\n",
      "Iteration 1609, loss = 0.15503445\n",
      "Iteration 1610, loss = 0.15498091\n",
      "Iteration 1611, loss = 0.15492739\n",
      "Iteration 1612, loss = 0.15487389\n",
      "Iteration 1613, loss = 0.15482040\n",
      "Iteration 1614, loss = 0.15476693\n",
      "Iteration 1615, loss = 0.15471348\n",
      "Iteration 1616, loss = 0.15466004\n",
      "Iteration 1617, loss = 0.15460662\n",
      "Iteration 1618, loss = 0.15455321\n",
      "Iteration 1619, loss = 0.15449982\n",
      "Iteration 1620, loss = 0.15444644\n",
      "Iteration 1621, loss = 0.15439308\n",
      "Iteration 1622, loss = 0.15433973\n",
      "Iteration 1623, loss = 0.15428639\n",
      "Iteration 1624, loss = 0.15423307\n",
      "Iteration 1625, loss = 0.15417976\n",
      "Iteration 1626, loss = 0.15412647\n",
      "Iteration 1627, loss = 0.15407319\n",
      "Iteration 1628, loss = 0.15401992\n",
      "Iteration 1629, loss = 0.15396666\n",
      "Iteration 1630, loss = 0.15391342\n",
      "Iteration 1631, loss = 0.15386019\n",
      "Iteration 1632, loss = 0.15380697\n",
      "Iteration 1633, loss = 0.15375377\n",
      "Iteration 1634, loss = 0.15370057\n",
      "Iteration 1635, loss = 0.15364739\n",
      "Iteration 1636, loss = 0.15359422\n",
      "Iteration 1637, loss = 0.15354106\n",
      "Iteration 1638, loss = 0.15348791\n",
      "Iteration 1639, loss = 0.15343478\n",
      "Iteration 1640, loss = 0.15338165\n",
      "Iteration 1641, loss = 0.15332853\n",
      "Iteration 1642, loss = 0.15327543\n",
      "Iteration 1643, loss = 0.15322234\n",
      "Iteration 1644, loss = 0.15316925\n",
      "Iteration 1645, loss = 0.15311618\n",
      "Iteration 1646, loss = 0.15306312\n",
      "Iteration 1647, loss = 0.15301007\n",
      "Iteration 1648, loss = 0.15295702\n",
      "Iteration 1649, loss = 0.15290399\n",
      "Iteration 1650, loss = 0.15285097\n",
      "Iteration 1651, loss = 0.15279795\n",
      "Iteration 1652, loss = 0.15274495\n",
      "Iteration 1653, loss = 0.15269195\n",
      "Iteration 1654, loss = 0.15263897\n",
      "Iteration 1655, loss = 0.15258599\n",
      "Iteration 1656, loss = 0.15253302\n",
      "Iteration 1657, loss = 0.15248006\n",
      "Iteration 1658, loss = 0.15242711\n",
      "Iteration 1659, loss = 0.15237417\n",
      "Iteration 1660, loss = 0.15232124\n",
      "Iteration 1661, loss = 0.15226831\n",
      "Iteration 1662, loss = 0.15221540\n",
      "Iteration 1663, loss = 0.15216249\n",
      "Iteration 1664, loss = 0.15210959\n",
      "Iteration 1665, loss = 0.15205670\n",
      "Iteration 1666, loss = 0.15200382\n",
      "Iteration 1667, loss = 0.15195094\n",
      "Iteration 1668, loss = 0.15189807\n",
      "Iteration 1669, loss = 0.15184521\n",
      "Iteration 1670, loss = 0.15179236\n",
      "Iteration 1671, loss = 0.15173952\n",
      "Iteration 1672, loss = 0.15168668\n",
      "Iteration 1673, loss = 0.15163385\n",
      "Iteration 1674, loss = 0.15158103\n",
      "Iteration 1675, loss = 0.15152821\n",
      "Iteration 1676, loss = 0.15147540\n",
      "Iteration 1677, loss = 0.15142260\n",
      "Iteration 1678, loss = 0.15136981\n",
      "Iteration 1679, loss = 0.15131702\n",
      "Iteration 1680, loss = 0.15126425\n",
      "Iteration 1681, loss = 0.15121147\n",
      "Iteration 1682, loss = 0.15115871\n",
      "Iteration 1683, loss = 0.15110595\n",
      "Iteration 1684, loss = 0.15105320\n",
      "Iteration 1685, loss = 0.15100045\n",
      "Iteration 1686, loss = 0.15094771\n",
      "Iteration 1687, loss = 0.15089498\n",
      "Iteration 1688, loss = 0.15084226\n",
      "Iteration 1689, loss = 0.15078954\n",
      "Iteration 1690, loss = 0.15073683\n",
      "Iteration 1691, loss = 0.15068412\n",
      "Iteration 1692, loss = 0.15063142\n",
      "Iteration 1693, loss = 0.15057873\n",
      "Iteration 1694, loss = 0.15052604\n",
      "Iteration 1695, loss = 0.15047336\n",
      "Iteration 1696, loss = 0.15042069\n",
      "Iteration 1697, loss = 0.15036802\n",
      "Iteration 1698, loss = 0.15031536\n",
      "Iteration 1699, loss = 0.15026270\n",
      "Iteration 1700, loss = 0.15021005\n",
      "Iteration 1701, loss = 0.15015741\n",
      "Iteration 1702, loss = 0.15010477\n",
      "Iteration 1703, loss = 0.15005214\n",
      "Iteration 1704, loss = 0.14999951\n",
      "Iteration 1705, loss = 0.14994689\n",
      "Iteration 1706, loss = 0.14989428\n",
      "Iteration 1707, loss = 0.14984167\n",
      "Iteration 1708, loss = 0.14978907\n",
      "Iteration 1709, loss = 0.14973647\n",
      "Iteration 1710, loss = 0.14968388\n",
      "Iteration 1711, loss = 0.14963130\n",
      "Iteration 1712, loss = 0.14957872\n",
      "Iteration 1713, loss = 0.14952615\n",
      "Iteration 1714, loss = 0.14947358\n",
      "Iteration 1715, loss = 0.14942102\n",
      "Iteration 1716, loss = 0.14936846\n",
      "Iteration 1717, loss = 0.14931591\n",
      "Iteration 1718, loss = 0.14926337\n",
      "Iteration 1719, loss = 0.14921083\n",
      "Iteration 1720, loss = 0.14915829\n",
      "Iteration 1721, loss = 0.14910577\n",
      "Iteration 1722, loss = 0.14905325\n",
      "Iteration 1723, loss = 0.14900073\n",
      "Iteration 1724, loss = 0.14894822\n",
      "Iteration 1725, loss = 0.14889571\n",
      "Iteration 1726, loss = 0.14884321\n",
      "Iteration 1727, loss = 0.14879072\n",
      "Iteration 1728, loss = 0.14873823\n",
      "Iteration 1729, loss = 0.14868575\n",
      "Iteration 1730, loss = 0.14863327\n",
      "Iteration 1731, loss = 0.14858080\n",
      "Iteration 1732, loss = 0.14852834\n",
      "Iteration 1733, loss = 0.14847588\n",
      "Iteration 1734, loss = 0.14842342\n",
      "Iteration 1735, loss = 0.14837097\n",
      "Iteration 1736, loss = 0.14831853\n",
      "Iteration 1737, loss = 0.14826609\n",
      "Iteration 1738, loss = 0.14821366\n",
      "Iteration 1739, loss = 0.14816123\n",
      "Iteration 1740, loss = 0.14810881\n",
      "Iteration 1741, loss = 0.14805640\n",
      "Iteration 1742, loss = 0.14800399\n",
      "Iteration 1743, loss = 0.14795158\n",
      "Iteration 1744, loss = 0.14789918\n",
      "Iteration 1745, loss = 0.14784679\n",
      "Iteration 1746, loss = 0.14779440\n",
      "Iteration 1747, loss = 0.14774202\n",
      "Iteration 1748, loss = 0.14768965\n",
      "Iteration 1749, loss = 0.14763727\n",
      "Iteration 1750, loss = 0.14758491\n",
      "Iteration 1751, loss = 0.14753255\n",
      "Iteration 1752, loss = 0.14748020\n",
      "Iteration 1753, loss = 0.14742785\n",
      "Iteration 1754, loss = 0.14737551\n",
      "Iteration 1755, loss = 0.14732317\n",
      "Iteration 1756, loss = 0.14727084\n",
      "Iteration 1757, loss = 0.14721852\n",
      "Iteration 1758, loss = 0.14716620\n",
      "Iteration 1759, loss = 0.14711389\n",
      "Iteration 1760, loss = 0.14706158\n",
      "Iteration 1761, loss = 0.14700928\n",
      "Iteration 1762, loss = 0.14695698\n",
      "Iteration 1763, loss = 0.14690469\n",
      "Iteration 1764, loss = 0.14685241\n",
      "Iteration 1765, loss = 0.14680013\n",
      "Iteration 1766, loss = 0.14674786\n",
      "Iteration 1767, loss = 0.14669559\n",
      "Iteration 1768, loss = 0.14664333\n",
      "Iteration 1769, loss = 0.14659108\n",
      "Iteration 1770, loss = 0.14653883\n",
      "Iteration 1771, loss = 0.14648659\n",
      "Iteration 1772, loss = 0.14643435\n",
      "Iteration 1773, loss = 0.14638212\n",
      "Iteration 1774, loss = 0.14632990\n",
      "Iteration 1775, loss = 0.14627768\n",
      "Iteration 1776, loss = 0.14622547\n",
      "Iteration 1777, loss = 0.14617326\n",
      "Iteration 1778, loss = 0.14612106\n",
      "Iteration 1779, loss = 0.14606887\n",
      "Iteration 1780, loss = 0.14601669\n",
      "Iteration 1781, loss = 0.14596450\n",
      "Iteration 1782, loss = 0.14591233\n",
      "Iteration 1783, loss = 0.14586016\n",
      "Iteration 1784, loss = 0.14580800\n",
      "Iteration 1785, loss = 0.14575585\n",
      "Iteration 1786, loss = 0.14570370\n",
      "Iteration 1787, loss = 0.14565155\n",
      "Iteration 1788, loss = 0.14559942\n",
      "Iteration 1789, loss = 0.14554729\n",
      "Iteration 1790, loss = 0.14549517\n",
      "Iteration 1791, loss = 0.14544305\n",
      "Iteration 1792, loss = 0.14539094\n",
      "Iteration 1793, loss = 0.14533884\n",
      "Iteration 1794, loss = 0.14528674\n",
      "Iteration 1795, loss = 0.14523465\n",
      "Iteration 1796, loss = 0.14518257\n",
      "Iteration 1797, loss = 0.14513049\n",
      "Iteration 1798, loss = 0.14507842\n",
      "Iteration 1799, loss = 0.14502636\n",
      "Iteration 1800, loss = 0.14497430\n",
      "Iteration 1801, loss = 0.14492225\n",
      "Iteration 1802, loss = 0.14487021\n",
      "Iteration 1803, loss = 0.14481818\n",
      "Iteration 1804, loss = 0.14476615\n",
      "Iteration 1805, loss = 0.14471413\n",
      "Iteration 1806, loss = 0.14466211\n",
      "Iteration 1807, loss = 0.14461010\n",
      "Iteration 1808, loss = 0.14455810\n",
      "Iteration 1809, loss = 0.14450611\n",
      "Iteration 1810, loss = 0.14445412\n",
      "Iteration 1811, loss = 0.14440215\n",
      "Iteration 1812, loss = 0.14435017\n",
      "Iteration 1813, loss = 0.14429821\n",
      "Iteration 1814, loss = 0.14424625\n",
      "Iteration 1815, loss = 0.14419430\n",
      "Iteration 1816, loss = 0.14414236\n",
      "Iteration 1817, loss = 0.14409043\n",
      "Iteration 1818, loss = 0.14403850\n",
      "Iteration 1819, loss = 0.14398658\n",
      "Iteration 1820, loss = 0.14393467\n",
      "Iteration 1821, loss = 0.14388276\n",
      "Iteration 1822, loss = 0.14383086\n",
      "Iteration 1823, loss = 0.14377897\n",
      "Iteration 1824, loss = 0.14372709\n",
      "Iteration 1825, loss = 0.14367522\n",
      "Iteration 1826, loss = 0.14362335\n",
      "Iteration 1827, loss = 0.14357149\n",
      "Iteration 1828, loss = 0.14351964\n",
      "Iteration 1829, loss = 0.14346780\n",
      "Iteration 1830, loss = 0.14341596\n",
      "Iteration 1831, loss = 0.14336414\n",
      "Iteration 1832, loss = 0.14331232\n",
      "Iteration 1833, loss = 0.14326051\n",
      "Iteration 1834, loss = 0.14320870\n",
      "Iteration 1835, loss = 0.14315691\n",
      "Iteration 1836, loss = 0.14310512\n",
      "Iteration 1837, loss = 0.14305334\n",
      "Iteration 1838, loss = 0.14300157\n",
      "Iteration 1839, loss = 0.14294981\n",
      "Iteration 1840, loss = 0.14289806\n",
      "Iteration 1841, loss = 0.14284631\n",
      "Iteration 1842, loss = 0.14279458\n",
      "Iteration 1843, loss = 0.14274285\n",
      "Iteration 1844, loss = 0.14269113\n",
      "Iteration 1845, loss = 0.14263942\n",
      "Iteration 1846, loss = 0.14258771\n",
      "Iteration 1847, loss = 0.14253602\n",
      "Iteration 1848, loss = 0.14248433\n",
      "Iteration 1849, loss = 0.14243266\n",
      "Iteration 1850, loss = 0.14238099\n",
      "Iteration 1851, loss = 0.14232933\n",
      "Iteration 1852, loss = 0.14227768\n",
      "Iteration 1853, loss = 0.14222604\n",
      "Iteration 1854, loss = 0.14217440\n",
      "Iteration 1855, loss = 0.14212278\n",
      "Iteration 1856, loss = 0.14207117\n",
      "Iteration 1857, loss = 0.14201956\n",
      "Iteration 1858, loss = 0.14196796\n",
      "Iteration 1859, loss = 0.14191638\n",
      "Iteration 1860, loss = 0.14186480\n",
      "Iteration 1861, loss = 0.14181323\n",
      "Iteration 1862, loss = 0.14176167\n",
      "Iteration 1863, loss = 0.14171012\n",
      "Iteration 1864, loss = 0.14165858\n",
      "Iteration 1865, loss = 0.14160705\n",
      "Iteration 1866, loss = 0.14155553\n",
      "Iteration 1867, loss = 0.14150401\n",
      "Iteration 1868, loss = 0.14145251\n",
      "Iteration 1869, loss = 0.14140102\n",
      "Iteration 1870, loss = 0.14134954\n",
      "Iteration 1871, loss = 0.14129806\n",
      "Iteration 1872, loss = 0.14124660\n",
      "Iteration 1873, loss = 0.14119514\n",
      "Iteration 1874, loss = 0.14114370\n",
      "Iteration 1875, loss = 0.14109226\n",
      "Iteration 1876, loss = 0.14104084\n",
      "Iteration 1877, loss = 0.14098943\n",
      "Iteration 1878, loss = 0.14093802\n",
      "Iteration 1879, loss = 0.14088663\n",
      "Iteration 1880, loss = 0.14083524\n",
      "Iteration 1881, loss = 0.14078387\n",
      "Iteration 1882, loss = 0.14073250\n",
      "Iteration 1883, loss = 0.14068115\n",
      "Iteration 1884, loss = 0.14062981\n",
      "Iteration 1885, loss = 0.14057847\n",
      "Iteration 1886, loss = 0.14052715\n",
      "Iteration 1887, loss = 0.14047584\n",
      "Iteration 1888, loss = 0.14042454\n",
      "Iteration 1889, loss = 0.14037325\n",
      "Iteration 1890, loss = 0.14032197\n",
      "Iteration 1891, loss = 0.14027070\n",
      "Iteration 1892, loss = 0.14021944\n",
      "Iteration 1893, loss = 0.14016819\n",
      "Iteration 1894, loss = 0.14011695\n",
      "Iteration 1895, loss = 0.14006573\n",
      "Iteration 1896, loss = 0.14001451\n",
      "Iteration 1897, loss = 0.13996389\n",
      "Iteration 1898, loss = 0.13991339\n",
      "Iteration 1899, loss = 0.13986295\n",
      "Iteration 1900, loss = 0.13981257\n",
      "Iteration 1901, loss = 0.13976224\n",
      "Iteration 1902, loss = 0.13971195\n",
      "Iteration 1903, loss = 0.13966171\n",
      "Iteration 1904, loss = 0.13961151\n",
      "Iteration 1905, loss = 0.13956135\n",
      "Iteration 1906, loss = 0.13951123\n",
      "Iteration 1907, loss = 0.13946114\n",
      "Iteration 1908, loss = 0.13941108\n",
      "Iteration 1909, loss = 0.13936105\n",
      "Iteration 1910, loss = 0.13931106\n",
      "Iteration 1911, loss = 0.13926108\n",
      "Iteration 1912, loss = 0.13921114\n",
      "Iteration 1913, loss = 0.13916122\n",
      "Iteration 1914, loss = 0.13911132\n",
      "Iteration 1915, loss = 0.13906145\n",
      "Iteration 1916, loss = 0.13901160\n",
      "Iteration 1917, loss = 0.13896177\n",
      "Iteration 1918, loss = 0.13891196\n",
      "Iteration 1919, loss = 0.13886218\n",
      "Iteration 1920, loss = 0.13881241\n",
      "Iteration 1921, loss = 0.13876266\n",
      "Iteration 1922, loss = 0.13871294\n",
      "Iteration 1923, loss = 0.13866323\n",
      "Iteration 1924, loss = 0.13861354\n",
      "Iteration 1925, loss = 0.13856386\n",
      "Iteration 1926, loss = 0.13851421\n",
      "Iteration 1927, loss = 0.13846457\n",
      "Iteration 1928, loss = 0.13841495\n",
      "Iteration 1929, loss = 0.13836535\n",
      "Iteration 1930, loss = 0.13831576\n",
      "Iteration 1931, loss = 0.13826620\n",
      "Iteration 1932, loss = 0.13821665\n",
      "Iteration 1933, loss = 0.13816711\n",
      "Iteration 1934, loss = 0.13811759\n",
      "Iteration 1935, loss = 0.13806809\n",
      "Iteration 1936, loss = 0.13801861\n",
      "Iteration 1937, loss = 0.13796914\n",
      "Iteration 1938, loss = 0.13791969\n",
      "Iteration 1939, loss = 0.13787026\n",
      "Iteration 1940, loss = 0.13782084\n",
      "Iteration 1941, loss = 0.13777144\n",
      "Iteration 1942, loss = 0.13772206\n",
      "Iteration 1943, loss = 0.13767269\n",
      "Iteration 1944, loss = 0.13762334\n",
      "Iteration 1945, loss = 0.13757400\n",
      "Iteration 1946, loss = 0.13752469\n",
      "Iteration 1947, loss = 0.13747538\n",
      "Iteration 1948, loss = 0.13742610\n",
      "Iteration 1949, loss = 0.13737683\n",
      "Iteration 1950, loss = 0.13732758\n",
      "Iteration 1951, loss = 0.13727835\n",
      "Iteration 1952, loss = 0.13722913\n",
      "Iteration 1953, loss = 0.13717993\n",
      "Iteration 1954, loss = 0.13713074\n",
      "Iteration 1955, loss = 0.13708157\n",
      "Iteration 1956, loss = 0.13703242\n",
      "Iteration 1957, loss = 0.13698329\n",
      "Iteration 1958, loss = 0.13693417\n",
      "Iteration 1959, loss = 0.13688507\n",
      "Iteration 1960, loss = 0.13683599\n",
      "Iteration 1961, loss = 0.13678693\n",
      "Iteration 1962, loss = 0.13673788\n",
      "Iteration 1963, loss = 0.13668885\n",
      "Iteration 1964, loss = 0.13663983\n",
      "Iteration 1965, loss = 0.13659083\n",
      "Iteration 1966, loss = 0.13654186\n",
      "Iteration 1967, loss = 0.13649289\n",
      "Iteration 1968, loss = 0.13644395\n",
      "Iteration 1969, loss = 0.13639502\n",
      "Iteration 1970, loss = 0.13634611\n",
      "Iteration 1971, loss = 0.13629722\n",
      "Iteration 1972, loss = 0.13624834\n",
      "Iteration 1973, loss = 0.13619949\n",
      "Iteration 1974, loss = 0.13615065\n",
      "Iteration 1975, loss = 0.13610183\n",
      "Iteration 1976, loss = 0.13605302\n",
      "Iteration 1977, loss = 0.13600424\n",
      "Iteration 1978, loss = 0.13595547\n",
      "Iteration 1979, loss = 0.13590672\n",
      "Iteration 1980, loss = 0.13585799\n",
      "Iteration 1981, loss = 0.13580927\n",
      "Iteration 1982, loss = 0.13576058\n",
      "Iteration 1983, loss = 0.13571190\n",
      "Iteration 1984, loss = 0.13566324\n",
      "Iteration 1985, loss = 0.13561460\n",
      "Iteration 1986, loss = 0.13556598\n",
      "Iteration 1987, loss = 0.13551737\n",
      "Iteration 1988, loss = 0.13546879\n",
      "Iteration 1989, loss = 0.13542022\n",
      "Iteration 1990, loss = 0.13537167\n",
      "Iteration 1991, loss = 0.13532314\n",
      "Iteration 1992, loss = 0.13527463\n",
      "Iteration 1993, loss = 0.13522614\n",
      "Iteration 1994, loss = 0.13517766\n",
      "Iteration 1995, loss = 0.13512921\n",
      "Iteration 1996, loss = 0.13508077\n",
      "Iteration 1997, loss = 0.13503235\n",
      "Iteration 1998, loss = 0.13498395\n",
      "Iteration 1999, loss = 0.13493557\n",
      "Iteration 2000, loss = 0.13488721\n",
      "Iteration 2001, loss = 0.13483887\n",
      "Iteration 2002, loss = 0.13479055\n",
      "Iteration 2003, loss = 0.13474225\n",
      "Iteration 2004, loss = 0.13469396\n",
      "Iteration 2005, loss = 0.13464570\n",
      "Iteration 2006, loss = 0.13459745\n",
      "Iteration 2007, loss = 0.13454922\n",
      "Iteration 2008, loss = 0.13450102\n",
      "Iteration 2009, loss = 0.13445283\n",
      "Iteration 2010, loss = 0.13440466\n",
      "Iteration 2011, loss = 0.13435652\n",
      "Iteration 2012, loss = 0.13430839\n",
      "Iteration 2013, loss = 0.13426028\n",
      "Iteration 2014, loss = 0.13421219\n",
      "Iteration 2015, loss = 0.13416412\n",
      "Iteration 2016, loss = 0.13411607\n",
      "Iteration 2017, loss = 0.13406805\n",
      "Iteration 2018, loss = 0.13402004\n",
      "Iteration 2019, loss = 0.13397205\n",
      "Iteration 2020, loss = 0.13392408\n",
      "Iteration 2021, loss = 0.13387613\n",
      "Iteration 2022, loss = 0.13382820\n",
      "Iteration 2023, loss = 0.13378029\n",
      "Iteration 2024, loss = 0.13373241\n",
      "Iteration 2025, loss = 0.13368454\n",
      "Iteration 2026, loss = 0.13363669\n",
      "Iteration 2027, loss = 0.13358886\n",
      "Iteration 2028, loss = 0.13354106\n",
      "Iteration 2029, loss = 0.13349327\n",
      "Iteration 2030, loss = 0.13344551\n",
      "Iteration 2031, loss = 0.13339776\n",
      "Iteration 2032, loss = 0.13335004\n",
      "Iteration 2033, loss = 0.13330234\n",
      "Iteration 2034, loss = 0.13325466\n",
      "Iteration 2035, loss = 0.13320700\n",
      "Iteration 2036, loss = 0.13315936\n",
      "Iteration 2037, loss = 0.13311174\n",
      "Iteration 2038, loss = 0.13306414\n",
      "Iteration 2039, loss = 0.13301656\n",
      "Iteration 2040, loss = 0.13296901\n",
      "Iteration 2041, loss = 0.13292147\n",
      "Iteration 2042, loss = 0.13287396\n",
      "Iteration 2043, loss = 0.13282647\n",
      "Iteration 2044, loss = 0.13277900\n",
      "Iteration 2045, loss = 0.13273155\n",
      "Iteration 2046, loss = 0.13268412\n",
      "Iteration 2047, loss = 0.13263671\n",
      "Iteration 2048, loss = 0.13258933\n",
      "Iteration 2049, loss = 0.13254197\n",
      "Iteration 2050, loss = 0.13249463\n",
      "Iteration 2051, loss = 0.13244731\n",
      "Iteration 2052, loss = 0.13240001\n",
      "Iteration 2053, loss = 0.13235273\n",
      "Iteration 2054, loss = 0.13230548\n",
      "Iteration 2055, loss = 0.13225825\n",
      "Iteration 2056, loss = 0.13221104\n",
      "Iteration 2057, loss = 0.13216385\n",
      "Iteration 2058, loss = 0.13211668\n",
      "Iteration 2059, loss = 0.13206954\n",
      "Iteration 2060, loss = 0.13202242\n",
      "Iteration 2061, loss = 0.13197532\n",
      "Iteration 2062, loss = 0.13192824\n",
      "Iteration 2063, loss = 0.13188119\n",
      "Iteration 2064, loss = 0.13183416\n",
      "Iteration 2065, loss = 0.13178715\n",
      "Iteration 2066, loss = 0.13174016\n",
      "Iteration 2067, loss = 0.13169320\n",
      "Iteration 2068, loss = 0.13164625\n",
      "Iteration 2069, loss = 0.13159933\n",
      "Iteration 2070, loss = 0.13155244\n",
      "Iteration 2071, loss = 0.13150556\n",
      "Iteration 2072, loss = 0.13145871\n",
      "Iteration 2073, loss = 0.13141189\n",
      "Iteration 2074, loss = 0.13136508\n",
      "Iteration 2075, loss = 0.13131830\n",
      "Iteration 2076, loss = 0.13127154\n",
      "Iteration 2077, loss = 0.13122481\n",
      "Iteration 2078, loss = 0.13117809\n",
      "Iteration 2079, loss = 0.13113140\n",
      "Iteration 2080, loss = 0.13108474\n",
      "Iteration 2081, loss = 0.13103809\n",
      "Iteration 2082, loss = 0.13099148\n",
      "Iteration 2083, loss = 0.13094488\n",
      "Iteration 2084, loss = 0.13089831\n",
      "Iteration 2085, loss = 0.13085176\n",
      "Iteration 2086, loss = 0.13080523\n",
      "Iteration 2087, loss = 0.13075873\n",
      "Iteration 2088, loss = 0.13071225\n",
      "Iteration 2089, loss = 0.13066580\n",
      "Iteration 2090, loss = 0.13061937\n",
      "Iteration 2091, loss = 0.13057296\n",
      "Iteration 2092, loss = 0.13052658\n",
      "Iteration 2093, loss = 0.13048022\n",
      "Iteration 2094, loss = 0.13043389\n",
      "Iteration 2095, loss = 0.13038757\n",
      "Iteration 2096, loss = 0.13034129\n",
      "Iteration 2097, loss = 0.13029503\n",
      "Iteration 2098, loss = 0.13024879\n",
      "Iteration 2099, loss = 0.13020257\n",
      "Iteration 2100, loss = 0.13015638\n",
      "Iteration 2101, loss = 0.13011022\n",
      "Iteration 2102, loss = 0.13006408\n",
      "Iteration 2103, loss = 0.13001796\n",
      "Iteration 2104, loss = 0.12997187\n",
      "Iteration 2105, loss = 0.12992580\n",
      "Iteration 2106, loss = 0.12987976\n",
      "Iteration 2107, loss = 0.12983374\n",
      "Iteration 2108, loss = 0.12978775\n",
      "Iteration 2109, loss = 0.12974178\n",
      "Iteration 2110, loss = 0.12969584\n",
      "Iteration 2111, loss = 0.12964992\n",
      "Iteration 2112, loss = 0.12960402\n",
      "Iteration 2113, loss = 0.12955815\n",
      "Iteration 2114, loss = 0.12951231\n",
      "Iteration 2115, loss = 0.12946649\n",
      "Iteration 2116, loss = 0.12942070\n",
      "Iteration 2117, loss = 0.12937493\n",
      "Iteration 2118, loss = 0.12932919\n",
      "Iteration 2119, loss = 0.12928347\n",
      "Iteration 2120, loss = 0.12923778\n",
      "Iteration 2121, loss = 0.12919211\n",
      "Iteration 2122, loss = 0.12914647\n",
      "Iteration 2123, loss = 0.12910085\n",
      "Iteration 2124, loss = 0.12905526\n",
      "Iteration 2125, loss = 0.12900970\n",
      "Iteration 2126, loss = 0.12896416\n",
      "Iteration 2127, loss = 0.12891864\n",
      "Iteration 2128, loss = 0.12887316\n",
      "Iteration 2129, loss = 0.12882769\n",
      "Iteration 2130, loss = 0.12878226\n",
      "Iteration 2131, loss = 0.12873685\n",
      "Iteration 2132, loss = 0.12869146\n",
      "Iteration 2133, loss = 0.12864611\n",
      "Iteration 2134, loss = 0.12860077\n",
      "Iteration 2135, loss = 0.12855547\n",
      "Iteration 2136, loss = 0.12851019\n",
      "Iteration 2137, loss = 0.12846493\n",
      "Iteration 2138, loss = 0.12841971\n",
      "Iteration 2139, loss = 0.12837451\n",
      "Iteration 2140, loss = 0.12832933\n",
      "Iteration 2141, loss = 0.12828418\n",
      "Iteration 2142, loss = 0.12823906\n",
      "Iteration 2143, loss = 0.12819397\n",
      "Iteration 2144, loss = 0.12814890\n",
      "Iteration 2145, loss = 0.12810386\n",
      "Iteration 2146, loss = 0.12805884\n",
      "Iteration 2147, loss = 0.12801385\n",
      "Iteration 2148, loss = 0.12796889\n",
      "Iteration 2149, loss = 0.12792396\n",
      "Iteration 2150, loss = 0.12787905\n",
      "Iteration 2151, loss = 0.12783417\n",
      "Iteration 2152, loss = 0.12778931\n",
      "Iteration 2153, loss = 0.12774449\n",
      "Iteration 2154, loss = 0.12769969\n",
      "Iteration 2155, loss = 0.12765491\n",
      "Iteration 2156, loss = 0.12761017\n",
      "Iteration 2157, loss = 0.12756545\n",
      "Iteration 2158, loss = 0.12752076\n",
      "Iteration 2159, loss = 0.12747610\n",
      "Iteration 2160, loss = 0.12743146\n",
      "Iteration 2161, loss = 0.12738685\n",
      "Iteration 2162, loss = 0.12734227\n",
      "Iteration 2163, loss = 0.12729772\n",
      "Iteration 2164, loss = 0.12725319\n",
      "Iteration 2165, loss = 0.12720869\n",
      "Iteration 2166, loss = 0.12716422\n",
      "Iteration 2167, loss = 0.12711978\n",
      "Iteration 2168, loss = 0.12707536\n",
      "Iteration 2169, loss = 0.12703098\n",
      "Iteration 2170, loss = 0.12698662\n",
      "Iteration 2171, loss = 0.12694228\n",
      "Iteration 2172, loss = 0.12689798\n",
      "Iteration 2173, loss = 0.12685371\n",
      "Iteration 2174, loss = 0.12680946\n",
      "Iteration 2175, loss = 0.12676524\n",
      "Iteration 2176, loss = 0.12672105\n",
      "Iteration 2177, loss = 0.12667688\n",
      "Iteration 2178, loss = 0.12663275\n",
      "Iteration 2179, loss = 0.12658864\n",
      "Iteration 2180, loss = 0.12654457\n",
      "Iteration 2181, loss = 0.12650052\n",
      "Iteration 2182, loss = 0.12645650\n",
      "Iteration 2183, loss = 0.12641250\n",
      "Iteration 2184, loss = 0.12636854\n",
      "Iteration 2185, loss = 0.12632460\n",
      "Iteration 2186, loss = 0.12628070\n",
      "Iteration 2187, loss = 0.12623682\n",
      "Iteration 2188, loss = 0.12619297\n",
      "Iteration 2189, loss = 0.12614915\n",
      "Iteration 2190, loss = 0.12610536\n",
      "Iteration 2191, loss = 0.12606160\n",
      "Iteration 2192, loss = 0.12601786\n",
      "Iteration 2193, loss = 0.12597416\n",
      "Iteration 2194, loss = 0.12593048\n",
      "Iteration 2195, loss = 0.12588684\n",
      "Iteration 2196, loss = 0.12584322\n",
      "Iteration 2197, loss = 0.12579963\n",
      "Iteration 2198, loss = 0.12575608\n",
      "Iteration 2199, loss = 0.12571255\n",
      "Iteration 2200, loss = 0.12566905\n",
      "Iteration 2201, loss = 0.12562558\n",
      "Iteration 2202, loss = 0.12558214\n",
      "Iteration 2203, loss = 0.12553872\n",
      "Iteration 2204, loss = 0.12549534\n",
      "Iteration 2205, loss = 0.12545199\n",
      "Iteration 2206, loss = 0.12540867\n",
      "Iteration 2207, loss = 0.12536537\n",
      "Iteration 2208, loss = 0.12532211\n",
      "Iteration 2209, loss = 0.12527888\n",
      "Iteration 2210, loss = 0.12523567\n",
      "Iteration 2211, loss = 0.12519250\n",
      "Iteration 2212, loss = 0.12514936\n",
      "Iteration 2213, loss = 0.12510624\n",
      "Iteration 2214, loss = 0.12506316\n",
      "Iteration 2215, loss = 0.12502011\n",
      "Iteration 2216, loss = 0.12497708\n",
      "Iteration 2217, loss = 0.12493409\n",
      "Iteration 2218, loss = 0.12489113\n",
      "Iteration 2219, loss = 0.12484819\n",
      "Iteration 2220, loss = 0.12480529\n",
      "Iteration 2221, loss = 0.12476242\n",
      "Iteration 2222, loss = 0.12471958\n",
      "Iteration 2223, loss = 0.12467676\n",
      "Iteration 2224, loss = 0.12463398\n",
      "Iteration 2225, loss = 0.12459123\n",
      "Iteration 2226, loss = 0.12454851\n",
      "Iteration 2227, loss = 0.12450582\n",
      "Iteration 2228, loss = 0.12446317\n",
      "Iteration 2229, loss = 0.12442054\n",
      "Iteration 2230, loss = 0.12437794\n",
      "Iteration 2231, loss = 0.12433537\n",
      "Iteration 2232, loss = 0.12429284\n",
      "Iteration 2233, loss = 0.12425033\n",
      "Iteration 2234, loss = 0.12420786\n",
      "Iteration 2235, loss = 0.12416542\n",
      "Iteration 2236, loss = 0.12412301\n",
      "Iteration 2237, loss = 0.12408063\n",
      "Iteration 2238, loss = 0.12403828\n",
      "Iteration 2239, loss = 0.12399596\n",
      "Iteration 2240, loss = 0.12395367\n",
      "Iteration 2241, loss = 0.12391142\n",
      "Iteration 2242, loss = 0.12386919\n",
      "Iteration 2243, loss = 0.12382700\n",
      "Iteration 2244, loss = 0.12378484\n",
      "Iteration 2245, loss = 0.12374271\n",
      "Iteration 2246, loss = 0.12370061\n",
      "Iteration 2247, loss = 0.12365854\n",
      "Iteration 2248, loss = 0.12361651\n",
      "Iteration 2249, loss = 0.12357450\n",
      "Iteration 2250, loss = 0.12353253\n",
      "Iteration 2251, loss = 0.12349059\n",
      "Iteration 2252, loss = 0.12344868\n",
      "Iteration 2253, loss = 0.12340680\n",
      "Iteration 2254, loss = 0.12336496\n",
      "Iteration 2255, loss = 0.12332315\n",
      "Iteration 2256, loss = 0.12328137\n",
      "Iteration 2257, loss = 0.12323962\n",
      "Iteration 2258, loss = 0.12319790\n",
      "Iteration 2259, loss = 0.12315621\n",
      "Iteration 2260, loss = 0.12311456\n",
      "Iteration 2261, loss = 0.12307294\n",
      "Iteration 2262, loss = 0.12303135\n",
      "Iteration 2263, loss = 0.12298979\n",
      "Iteration 2264, loss = 0.12294827\n",
      "Iteration 2265, loss = 0.12290678\n",
      "Iteration 2266, loss = 0.12286532\n",
      "Iteration 2267, loss = 0.12282335\n",
      "Iteration 2268, loss = 0.12278036\n",
      "Iteration 2269, loss = 0.12273718\n",
      "Iteration 2270, loss = 0.12269382\n",
      "Iteration 2271, loss = 0.12265031\n",
      "Iteration 2272, loss = 0.12260667\n",
      "Iteration 2273, loss = 0.12256292\n",
      "Iteration 2274, loss = 0.12251906\n",
      "Iteration 2275, loss = 0.12247511\n",
      "Iteration 2276, loss = 0.12243109\n",
      "Iteration 2277, loss = 0.12238699\n",
      "Iteration 2278, loss = 0.12234285\n",
      "Iteration 2279, loss = 0.12229865\n",
      "Iteration 2280, loss = 0.12225442\n",
      "Iteration 2281, loss = 0.12221015\n",
      "Iteration 2282, loss = 0.12216586\n",
      "Iteration 2283, loss = 0.12212155\n",
      "Iteration 2284, loss = 0.12207722\n",
      "Iteration 2285, loss = 0.12203288\n",
      "Iteration 2286, loss = 0.12198854\n",
      "Iteration 2287, loss = 0.12194419\n",
      "Iteration 2288, loss = 0.12189984\n",
      "Iteration 2289, loss = 0.12185550\n",
      "Iteration 2290, loss = 0.12181117\n",
      "Iteration 2291, loss = 0.12176684\n",
      "Iteration 2292, loss = 0.12172253\n",
      "Iteration 2293, loss = 0.12167823\n",
      "Iteration 2294, loss = 0.12163394\n",
      "Iteration 2295, loss = 0.12158968\n",
      "Iteration 2296, loss = 0.12154543\n",
      "Iteration 2297, loss = 0.12150120\n",
      "Iteration 2298, loss = 0.12145700\n",
      "Iteration 2299, loss = 0.12141282\n",
      "Iteration 2300, loss = 0.12136866\n",
      "Iteration 2301, loss = 0.12132452\n",
      "Iteration 2302, loss = 0.12128042\n",
      "Iteration 2303, loss = 0.12123633\n",
      "Iteration 2304, loss = 0.12119228\n",
      "Iteration 2305, loss = 0.12114825\n",
      "Iteration 2306, loss = 0.12110425\n",
      "Iteration 2307, loss = 0.12106028\n",
      "Iteration 2308, loss = 0.12101634\n",
      "Iteration 2309, loss = 0.12097243\n",
      "Iteration 2310, loss = 0.12092855\n",
      "Iteration 2311, loss = 0.12088470\n",
      "Iteration 2312, loss = 0.12084088\n",
      "Iteration 2313, loss = 0.12079709\n",
      "Iteration 2314, loss = 0.12075334\n",
      "Iteration 2315, loss = 0.12070961\n",
      "Iteration 2316, loss = 0.12066592\n",
      "Iteration 2317, loss = 0.12062226\n",
      "Iteration 2318, loss = 0.12057863\n",
      "Iteration 2319, loss = 0.12053503\n",
      "Iteration 2320, loss = 0.12049147\n",
      "Iteration 2321, loss = 0.12044794\n",
      "Iteration 2322, loss = 0.12040444\n",
      "Iteration 2323, loss = 0.12036098\n",
      "Iteration 2324, loss = 0.12031755\n",
      "Iteration 2325, loss = 0.12027415\n",
      "Iteration 2326, loss = 0.12023079\n",
      "Iteration 2327, loss = 0.12018746\n",
      "Iteration 2328, loss = 0.12014417\n",
      "Iteration 2329, loss = 0.12010090\n",
      "Iteration 2330, loss = 0.12005768\n",
      "Iteration 2331, loss = 0.12001448\n",
      "Iteration 2332, loss = 0.11997133\n",
      "Iteration 2333, loss = 0.11992820\n",
      "Iteration 2334, loss = 0.11988511\n",
      "Iteration 2335, loss = 0.11984206\n",
      "Iteration 2336, loss = 0.11979904\n",
      "Iteration 2337, loss = 0.11975605\n",
      "Iteration 2338, loss = 0.11971310\n",
      "Iteration 2339, loss = 0.11967018\n",
      "Iteration 2340, loss = 0.11962730\n",
      "Iteration 2341, loss = 0.11958446\n",
      "Iteration 2342, loss = 0.11954165\n",
      "Iteration 2343, loss = 0.11949887\n",
      "Iteration 2344, loss = 0.11945613\n",
      "Iteration 2345, loss = 0.11941343\n",
      "Iteration 2346, loss = 0.11937076\n",
      "Iteration 2347, loss = 0.11932813\n",
      "Iteration 2348, loss = 0.11928553\n",
      "Iteration 2349, loss = 0.11924297\n",
      "Iteration 2350, loss = 0.11920044\n",
      "Iteration 2351, loss = 0.11915795\n",
      "Iteration 2352, loss = 0.11911550\n",
      "Iteration 2353, loss = 0.11907308\n",
      "Iteration 2354, loss = 0.11903069\n",
      "Iteration 2355, loss = 0.11898835\n",
      "Iteration 2356, loss = 0.11894604\n",
      "Iteration 2357, loss = 0.11890376\n",
      "Iteration 2358, loss = 0.11886153\n",
      "Iteration 2359, loss = 0.11881932\n",
      "Iteration 2360, loss = 0.11877716\n",
      "Iteration 2361, loss = 0.11873503\n",
      "Iteration 2362, loss = 0.11869294\n",
      "Iteration 2363, loss = 0.11865088\n",
      "Iteration 2364, loss = 0.11860887\n",
      "Iteration 2365, loss = 0.11856688\n",
      "Iteration 2366, loss = 0.11852494\n",
      "Iteration 2367, loss = 0.11848303\n",
      "Iteration 2368, loss = 0.11844116\n",
      "Iteration 2369, loss = 0.11839933\n",
      "Iteration 2370, loss = 0.11835753\n",
      "Iteration 2371, loss = 0.11831577\n",
      "Iteration 2372, loss = 0.11827405\n",
      "Iteration 2373, loss = 0.11823236\n",
      "Iteration 2374, loss = 0.11819072\n",
      "Iteration 2375, loss = 0.11814911\n",
      "Iteration 2376, loss = 0.11810753\n",
      "Iteration 2377, loss = 0.11806600\n",
      "Iteration 2378, loss = 0.11802450\n",
      "Iteration 2379, loss = 0.11798304\n",
      "Iteration 2380, loss = 0.11794162\n",
      "Iteration 2381, loss = 0.11790023\n",
      "Iteration 2382, loss = 0.11785889\n",
      "Iteration 2383, loss = 0.11781758\n",
      "Iteration 2384, loss = 0.11777631\n",
      "Iteration 2385, loss = 0.11773508\n",
      "Iteration 2386, loss = 0.11769388\n",
      "Iteration 2387, loss = 0.11765272\n",
      "Iteration 2388, loss = 0.11761161\n",
      "Iteration 2389, loss = 0.11757053\n",
      "Iteration 2390, loss = 0.11752949\n",
      "Iteration 2391, loss = 0.11748848\n",
      "Iteration 2392, loss = 0.11744752\n",
      "Iteration 2393, loss = 0.11740659\n",
      "Iteration 2394, loss = 0.11736571\n",
      "Iteration 2395, loss = 0.11732333\n",
      "Iteration 2396, loss = 0.11728015\n",
      "Iteration 2397, loss = 0.11723675\n",
      "Iteration 2398, loss = 0.11719315\n",
      "Iteration 2399, loss = 0.11714936\n",
      "Iteration 2400, loss = 0.11710542\n",
      "Iteration 2401, loss = 0.11706134\n",
      "Iteration 2402, loss = 0.11701714\n",
      "Iteration 2403, loss = 0.11697283\n",
      "Iteration 2404, loss = 0.11692843\n",
      "Iteration 2405, loss = 0.11688395\n",
      "Iteration 2406, loss = 0.11683941\n",
      "Iteration 2407, loss = 0.11679481\n",
      "Iteration 2408, loss = 0.11675016\n",
      "Iteration 2409, loss = 0.11670547\n",
      "Iteration 2410, loss = 0.11666076\n",
      "Iteration 2411, loss = 0.11661601\n",
      "Iteration 2412, loss = 0.11657125\n",
      "Iteration 2413, loss = 0.11652648\n",
      "Iteration 2414, loss = 0.11648170\n",
      "Iteration 2415, loss = 0.11643691\n",
      "Iteration 2416, loss = 0.11639213\n",
      "Iteration 2417, loss = 0.11634735\n",
      "Iteration 2418, loss = 0.11630257\n",
      "Iteration 2419, loss = 0.11625781\n",
      "Iteration 2420, loss = 0.11621307\n",
      "Iteration 2421, loss = 0.11616833\n",
      "Iteration 2422, loss = 0.11612362\n",
      "Iteration 2423, loss = 0.11607893\n",
      "Iteration 2424, loss = 0.11603425\n",
      "Iteration 2425, loss = 0.11598961\n",
      "Iteration 2426, loss = 0.11594498\n",
      "Iteration 2427, loss = 0.11590039\n",
      "Iteration 2428, loss = 0.11585582\n",
      "Iteration 2429, loss = 0.11581153\n",
      "Iteration 2430, loss = 0.11576801\n",
      "Iteration 2431, loss = 0.11572460\n",
      "Iteration 2432, loss = 0.11568128\n",
      "Iteration 2433, loss = 0.11563806\n",
      "Iteration 2434, loss = 0.11559493\n",
      "Iteration 2435, loss = 0.11555188\n",
      "Iteration 2436, loss = 0.11550891\n",
      "Iteration 2437, loss = 0.11546602\n",
      "Iteration 2438, loss = 0.11542320\n",
      "Iteration 2439, loss = 0.11538045\n",
      "Iteration 2440, loss = 0.11533776\n",
      "Iteration 2441, loss = 0.11529513\n",
      "Iteration 2442, loss = 0.11525257\n",
      "Iteration 2443, loss = 0.11521007\n",
      "Iteration 2444, loss = 0.11516762\n",
      "Iteration 2445, loss = 0.11512523\n",
      "Iteration 2446, loss = 0.11508289\n",
      "Iteration 2447, loss = 0.11504061\n",
      "Iteration 2448, loss = 0.11499838\n",
      "Iteration 2449, loss = 0.11495620\n",
      "Iteration 2450, loss = 0.11491406\n",
      "Iteration 2451, loss = 0.11487198\n",
      "Iteration 2452, loss = 0.11482994\n",
      "Iteration 2453, loss = 0.11478796\n",
      "Iteration 2454, loss = 0.11474601\n",
      "Iteration 2455, loss = 0.11470412\n",
      "Iteration 2456, loss = 0.11466227\n",
      "Iteration 2457, loss = 0.11462046\n",
      "Iteration 2458, loss = 0.11457870\n",
      "Iteration 2459, loss = 0.11453699\n",
      "Iteration 2460, loss = 0.11449532\n",
      "Iteration 2461, loss = 0.11445369\n",
      "Iteration 2462, loss = 0.11441211\n",
      "Iteration 2463, loss = 0.11437057\n",
      "Iteration 2464, loss = 0.11432907\n",
      "Iteration 2465, loss = 0.11428762\n",
      "Iteration 2466, loss = 0.11424621\n",
      "Iteration 2467, loss = 0.11420485\n",
      "Iteration 2468, loss = 0.11416352\n",
      "Iteration 2469, loss = 0.11412224\n",
      "Iteration 2470, loss = 0.11408101\n",
      "Iteration 2471, loss = 0.11403981\n",
      "Iteration 2472, loss = 0.11399866\n",
      "Iteration 2473, loss = 0.11395755\n",
      "Iteration 2474, loss = 0.11391649\n",
      "Iteration 2475, loss = 0.11387546\n",
      "Iteration 2476, loss = 0.11383448\n",
      "Iteration 2477, loss = 0.11379355\n",
      "Iteration 2478, loss = 0.11375265\n",
      "Iteration 2479, loss = 0.11371180\n",
      "Iteration 2480, loss = 0.11367099\n",
      "Iteration 2481, loss = 0.11363023\n",
      "Iteration 2482, loss = 0.11358950\n",
      "Iteration 2483, loss = 0.11354882\n",
      "Iteration 2484, loss = 0.11350819\n",
      "Iteration 2485, loss = 0.11346759\n",
      "Iteration 2486, loss = 0.11342704\n",
      "Iteration 2487, loss = 0.11338653\n",
      "Iteration 2488, loss = 0.11334607\n",
      "Iteration 2489, loss = 0.11330565\n",
      "Iteration 2490, loss = 0.11326527\n",
      "Iteration 2491, loss = 0.11322494\n",
      "Iteration 2492, loss = 0.11318465\n",
      "Iteration 2493, loss = 0.11314440\n",
      "Iteration 2494, loss = 0.11310419\n",
      "Iteration 2495, loss = 0.11306403\n",
      "Iteration 2496, loss = 0.11302392\n",
      "Iteration 2497, loss = 0.11298384\n",
      "Iteration 2498, loss = 0.11294381\n",
      "Iteration 2499, loss = 0.11290383\n",
      "Iteration 2500, loss = 0.11286389\n",
      "Iteration 2501, loss = 0.11282399\n",
      "Iteration 2502, loss = 0.11278414\n",
      "Iteration 2503, loss = 0.11274433\n",
      "Iteration 2504, loss = 0.11270456\n",
      "Iteration 2505, loss = 0.11266484\n",
      "Iteration 2506, loss = 0.11262516\n",
      "Iteration 2507, loss = 0.11258553\n",
      "Iteration 2508, loss = 0.11254594\n",
      "Iteration 2509, loss = 0.11250640\n",
      "Iteration 2510, loss = 0.11246690\n",
      "Iteration 2511, loss = 0.11242745\n",
      "Iteration 2512, loss = 0.11238804\n",
      "Iteration 2513, loss = 0.11234867\n",
      "Iteration 2514, loss = 0.11230935\n",
      "Iteration 2515, loss = 0.11227008\n",
      "Iteration 2516, loss = 0.11223085\n",
      "Iteration 2517, loss = 0.11219166\n",
      "Iteration 2518, loss = 0.11215252\n",
      "Iteration 2519, loss = 0.11211343\n",
      "Iteration 2520, loss = 0.11207438\n",
      "Iteration 2521, loss = 0.11203537\n",
      "Iteration 2522, loss = 0.11199641\n",
      "Iteration 2523, loss = 0.11195750\n",
      "Iteration 2524, loss = 0.11191863\n",
      "Iteration 2525, loss = 0.11187981\n",
      "Iteration 2526, loss = 0.11184103\n",
      "Iteration 2527, loss = 0.11180230\n",
      "Iteration 2528, loss = 0.11176361\n",
      "Iteration 2529, loss = 0.11172497\n",
      "Iteration 2530, loss = 0.11168638\n",
      "Iteration 2531, loss = 0.11164783\n",
      "Iteration 2532, loss = 0.11160933\n",
      "Iteration 2533, loss = 0.11157087\n",
      "Iteration 2534, loss = 0.11153246\n",
      "Iteration 2535, loss = 0.11149410\n",
      "Iteration 2536, loss = 0.11145578\n",
      "Iteration 2537, loss = 0.11141751\n",
      "Iteration 2538, loss = 0.11137928\n",
      "Iteration 2539, loss = 0.11134111\n",
      "Iteration 2540, loss = 0.11130297\n",
      "Iteration 2541, loss = 0.11126489\n",
      "Iteration 2542, loss = 0.11122685\n",
      "Iteration 2543, loss = 0.11118886\n",
      "Iteration 2544, loss = 0.11115091\n",
      "Iteration 2545, loss = 0.11111301\n",
      "Iteration 2546, loss = 0.11107516\n",
      "Iteration 2547, loss = 0.11103735\n",
      "Iteration 2548, loss = 0.11099960\n",
      "Iteration 2549, loss = 0.11096189\n",
      "Iteration 2550, loss = 0.11092422\n",
      "Iteration 2551, loss = 0.11088660\n",
      "Iteration 2552, loss = 0.11084903\n",
      "Iteration 2553, loss = 0.11081151\n",
      "Iteration 2554, loss = 0.11077404\n",
      "Iteration 2555, loss = 0.11073661\n",
      "Iteration 2556, loss = 0.11069923\n",
      "Iteration 2557, loss = 0.11066190\n",
      "Iteration 2558, loss = 0.11062461\n",
      "Iteration 2559, loss = 0.11058737\n",
      "Iteration 2560, loss = 0.11055018\n",
      "Iteration 2561, loss = 0.11051296\n",
      "Iteration 2562, loss = 0.11047560\n",
      "Iteration 2563, loss = 0.11043826\n",
      "Iteration 2564, loss = 0.11040095\n",
      "Iteration 2565, loss = 0.11036367\n",
      "Iteration 2566, loss = 0.11032642\n",
      "Iteration 2567, loss = 0.11028920\n",
      "Iteration 2568, loss = 0.11025201\n",
      "Iteration 2569, loss = 0.11021486\n",
      "Iteration 2570, loss = 0.11017775\n",
      "Iteration 2571, loss = 0.11014068\n",
      "Iteration 2572, loss = 0.11010364\n",
      "Iteration 2573, loss = 0.11006665\n",
      "Iteration 2574, loss = 0.11002969\n",
      "Iteration 2575, loss = 0.10999278\n",
      "Iteration 2576, loss = 0.10995591\n",
      "Iteration 2577, loss = 0.10991908\n",
      "Iteration 2578, loss = 0.10988229\n",
      "Iteration 2579, loss = 0.10984555\n",
      "Iteration 2580, loss = 0.10980885\n",
      "Iteration 2581, loss = 0.10977220\n",
      "Iteration 2582, loss = 0.10973559\n",
      "Iteration 2583, loss = 0.10969903\n",
      "Iteration 2584, loss = 0.10966251\n",
      "Iteration 2585, loss = 0.10962604\n",
      "Iteration 2586, loss = 0.10958962\n",
      "Iteration 2587, loss = 0.10955324\n",
      "Iteration 2588, loss = 0.10951691\n",
      "Iteration 2589, loss = 0.10948062\n",
      "Iteration 2590, loss = 0.10944439\n",
      "Iteration 2591, loss = 0.10940820\n",
      "Iteration 2592, loss = 0.10937205\n",
      "Iteration 2593, loss = 0.10933596\n",
      "Iteration 2594, loss = 0.10929991\n",
      "Iteration 2595, loss = 0.10926391\n",
      "Iteration 2596, loss = 0.10922796\n",
      "Iteration 2597, loss = 0.10919206\n",
      "Iteration 2598, loss = 0.10915620\n",
      "Iteration 2599, loss = 0.10912040\n",
      "Iteration 2600, loss = 0.10908464\n",
      "Iteration 2601, loss = 0.10904893\n",
      "Iteration 2602, loss = 0.10901327\n",
      "Iteration 2603, loss = 0.10897766\n",
      "Iteration 2604, loss = 0.10894210\n",
      "Iteration 2605, loss = 0.10890659\n",
      "Iteration 2606, loss = 0.10887112\n",
      "Iteration 2607, loss = 0.10883571\n",
      "Iteration 2608, loss = 0.10880025\n",
      "Iteration 2609, loss = 0.10876441\n",
      "Iteration 2610, loss = 0.10872857\n",
      "Iteration 2611, loss = 0.10869273\n",
      "Iteration 2612, loss = 0.10865691\n",
      "Iteration 2613, loss = 0.10862110\n",
      "Iteration 2614, loss = 0.10858531\n",
      "Iteration 2615, loss = 0.10854954\n",
      "Iteration 2616, loss = 0.10851380\n",
      "Iteration 2617, loss = 0.10847808\n",
      "Iteration 2618, loss = 0.10844240\n",
      "Iteration 2619, loss = 0.10840674\n",
      "Iteration 2620, loss = 0.10837112\n",
      "Iteration 2621, loss = 0.10833553\n",
      "Iteration 2622, loss = 0.10829998\n",
      "Iteration 2623, loss = 0.10826446\n",
      "Iteration 2624, loss = 0.10822899\n",
      "Iteration 2625, loss = 0.10819355\n",
      "Iteration 2626, loss = 0.10815815\n",
      "Iteration 2627, loss = 0.10812280\n",
      "Iteration 2628, loss = 0.10808749\n",
      "Iteration 2629, loss = 0.10805222\n",
      "Iteration 2630, loss = 0.10801700\n",
      "Iteration 2631, loss = 0.10798182\n",
      "Iteration 2632, loss = 0.10794668\n",
      "Iteration 2633, loss = 0.10791159\n",
      "Iteration 2634, loss = 0.10787655\n",
      "Iteration 2635, loss = 0.10784155\n",
      "Iteration 2636, loss = 0.10780660\n",
      "Iteration 2637, loss = 0.10777170\n",
      "Iteration 2638, loss = 0.10773684\n",
      "Iteration 2639, loss = 0.10770204\n",
      "Iteration 2640, loss = 0.10766728\n",
      "Iteration 2641, loss = 0.10763256\n",
      "Iteration 2642, loss = 0.10759790\n",
      "Iteration 2643, loss = 0.10756328\n",
      "Iteration 2644, loss = 0.10752872\n",
      "Iteration 2645, loss = 0.10749420\n",
      "Iteration 2646, loss = 0.10745973\n",
      "Iteration 2647, loss = 0.10742532\n",
      "Iteration 2648, loss = 0.10739095\n",
      "Iteration 2649, loss = 0.10735663\n",
      "Iteration 2650, loss = 0.10732236\n",
      "Iteration 2651, loss = 0.10728814\n",
      "Iteration 2652, loss = 0.10725396\n",
      "Iteration 2653, loss = 0.10721984\n",
      "Iteration 2654, loss = 0.10718577\n",
      "Iteration 2655, loss = 0.10715175\n",
      "Iteration 2656, loss = 0.10711778\n",
      "Iteration 2657, loss = 0.10708286\n",
      "Iteration 2658, loss = 0.10704636\n",
      "Iteration 2659, loss = 0.10700964\n",
      "Iteration 2660, loss = 0.10697272\n",
      "Iteration 2661, loss = 0.10693562\n",
      "Iteration 2662, loss = 0.10689837\n",
      "Iteration 2663, loss = 0.10686099\n",
      "Iteration 2664, loss = 0.10682349\n",
      "Iteration 2665, loss = 0.10678590\n",
      "Iteration 2666, loss = 0.10674822\n",
      "Iteration 2667, loss = 0.10671048\n",
      "Iteration 2668, loss = 0.10667267\n",
      "Iteration 2669, loss = 0.10663482\n",
      "Iteration 2670, loss = 0.10659693\n",
      "Iteration 2671, loss = 0.10655902\n",
      "Iteration 2672, loss = 0.10652108\n",
      "Iteration 2673, loss = 0.10648313\n",
      "Iteration 2674, loss = 0.10644518\n",
      "Iteration 2675, loss = 0.10640722\n",
      "Iteration 2676, loss = 0.10636927\n",
      "Iteration 2677, loss = 0.10633133\n",
      "Iteration 2678, loss = 0.10629340\n",
      "Iteration 2679, loss = 0.10625549\n",
      "Iteration 2680, loss = 0.10621760\n",
      "Iteration 2681, loss = 0.10617974\n",
      "Iteration 2682, loss = 0.10614190\n",
      "Iteration 2683, loss = 0.10610409\n",
      "Iteration 2684, loss = 0.10606631\n",
      "Iteration 2685, loss = 0.10602856\n",
      "Iteration 2686, loss = 0.10599085\n",
      "Iteration 2687, loss = 0.10595318\n",
      "Iteration 2688, loss = 0.10591555\n",
      "Iteration 2689, loss = 0.10587796\n",
      "Iteration 2690, loss = 0.10584040\n",
      "Iteration 2691, loss = 0.10580289\n",
      "Iteration 2692, loss = 0.10576543\n",
      "Iteration 2693, loss = 0.10572800\n",
      "Iteration 2694, loss = 0.10569063\n",
      "Iteration 2695, loss = 0.10565329\n",
      "Iteration 2696, loss = 0.10561601\n",
      "Iteration 2697, loss = 0.10557877\n",
      "Iteration 2698, loss = 0.10554158\n",
      "Iteration 2699, loss = 0.10550444\n",
      "Iteration 2700, loss = 0.10546734\n",
      "Iteration 2701, loss = 0.10543030\n",
      "Iteration 2702, loss = 0.10539330\n",
      "Iteration 2703, loss = 0.10535635\n",
      "Iteration 2704, loss = 0.10531946\n",
      "Iteration 2705, loss = 0.10528261\n",
      "Iteration 2706, loss = 0.10524582\n",
      "Iteration 2707, loss = 0.10520907\n",
      "Iteration 2708, loss = 0.10517238\n",
      "Iteration 2709, loss = 0.10513549\n",
      "Iteration 2710, loss = 0.10509683\n",
      "Iteration 2711, loss = 0.10505803\n",
      "Iteration 2712, loss = 0.10501910\n",
      "Iteration 2713, loss = 0.10498007\n",
      "Iteration 2714, loss = 0.10494095\n",
      "Iteration 2715, loss = 0.10490175\n",
      "Iteration 2716, loss = 0.10486249\n",
      "Iteration 2717, loss = 0.10482317\n",
      "Iteration 2718, loss = 0.10478382\n",
      "Iteration 2719, loss = 0.10474287\n",
      "Iteration 2720, loss = 0.10470129\n",
      "Iteration 2721, loss = 0.10465947\n",
      "Iteration 2722, loss = 0.10461745\n",
      "Iteration 2723, loss = 0.10457658\n",
      "Iteration 2724, loss = 0.10453695\n",
      "Iteration 2725, loss = 0.10449871\n",
      "Iteration 2726, loss = 0.10446067\n",
      "Iteration 2727, loss = 0.10442281\n",
      "Iteration 2728, loss = 0.10438512\n",
      "Iteration 2729, loss = 0.10434759\n",
      "Iteration 2730, loss = 0.10431020\n",
      "Iteration 2731, loss = 0.10427295\n",
      "Iteration 2732, loss = 0.10423582\n",
      "Iteration 2733, loss = 0.10419881\n",
      "Iteration 2734, loss = 0.10416191\n",
      "Iteration 2735, loss = 0.10412512\n",
      "Iteration 2736, loss = 0.10408842\n",
      "Iteration 2737, loss = 0.10405182\n",
      "Iteration 2738, loss = 0.10401530\n",
      "Iteration 2739, loss = 0.10397887\n",
      "Iteration 2740, loss = 0.10394251\n",
      "Iteration 2741, loss = 0.10390623\n",
      "Iteration 2742, loss = 0.10387002\n",
      "Iteration 2743, loss = 0.10383388\n",
      "Iteration 2744, loss = 0.10379781\n",
      "Iteration 2745, loss = 0.10376180\n",
      "Iteration 2746, loss = 0.10372586\n",
      "Iteration 2747, loss = 0.10368997\n",
      "Iteration 2748, loss = 0.10365414\n",
      "Iteration 2749, loss = 0.10361837\n",
      "Iteration 2750, loss = 0.10358266\n",
      "Iteration 2751, loss = 0.10354700\n",
      "Iteration 2752, loss = 0.10351139\n",
      "Iteration 2753, loss = 0.10347583\n",
      "Iteration 2754, loss = 0.10344033\n",
      "Iteration 2755, loss = 0.10340487\n",
      "Iteration 2756, loss = 0.10336947\n",
      "Iteration 2757, loss = 0.10333411\n",
      "Iteration 2758, loss = 0.10329880\n",
      "Iteration 2759, loss = 0.10326354\n",
      "Iteration 2760, loss = 0.10322833\n",
      "Iteration 2761, loss = 0.10319316\n",
      "Iteration 2762, loss = 0.10315804\n",
      "Iteration 2763, loss = 0.10312297\n",
      "Iteration 2764, loss = 0.10308794\n",
      "Iteration 2765, loss = 0.10305296\n",
      "Iteration 2766, loss = 0.10301802\n",
      "Iteration 2767, loss = 0.10298313\n",
      "Iteration 2768, loss = 0.10294828\n",
      "Iteration 2769, loss = 0.10291348\n",
      "Iteration 2770, loss = 0.10287872\n",
      "Iteration 2771, loss = 0.10284401\n",
      "Iteration 2772, loss = 0.10280934\n",
      "Iteration 2773, loss = 0.10277472\n",
      "Iteration 2774, loss = 0.10274014\n",
      "Iteration 2775, loss = 0.10270560\n",
      "Iteration 2776, loss = 0.10267111\n",
      "Iteration 2777, loss = 0.10263666\n",
      "Iteration 2778, loss = 0.10260225\n",
      "Iteration 2779, loss = 0.10256789\n",
      "Iteration 2780, loss = 0.10253357\n",
      "Iteration 2781, loss = 0.10249929\n",
      "Iteration 2782, loss = 0.10246506\n",
      "Iteration 2783, loss = 0.10243088\n",
      "Iteration 2784, loss = 0.10239673\n",
      "Iteration 2785, loss = 0.10236263\n",
      "Iteration 2786, loss = 0.10232858\n",
      "Iteration 2787, loss = 0.10229456\n",
      "Iteration 2788, loss = 0.10226059\n",
      "Iteration 2789, loss = 0.10222667\n",
      "Iteration 2790, loss = 0.10219278\n",
      "Iteration 2791, loss = 0.10215894\n",
      "Iteration 2792, loss = 0.10212515\n",
      "Iteration 2793, loss = 0.10209140\n",
      "Iteration 2794, loss = 0.10205769\n",
      "Iteration 2795, loss = 0.10202402\n",
      "Iteration 2796, loss = 0.10199040\n",
      "Iteration 2797, loss = 0.10195683\n",
      "Iteration 2798, loss = 0.10192329\n",
      "Iteration 2799, loss = 0.10188980\n",
      "Iteration 2800, loss = 0.10185636\n",
      "Iteration 2801, loss = 0.10182295\n",
      "Iteration 2802, loss = 0.10178959\n",
      "Iteration 2803, loss = 0.10175628\n",
      "Iteration 2804, loss = 0.10172301\n",
      "Iteration 2805, loss = 0.10168978\n",
      "Iteration 2806, loss = 0.10165660\n",
      "Iteration 2807, loss = 0.10162346\n",
      "Iteration 2808, loss = 0.10159036\n",
      "Iteration 2809, loss = 0.10155731\n",
      "Iteration 2810, loss = 0.10152430\n",
      "Iteration 2811, loss = 0.10149134\n",
      "Iteration 2812, loss = 0.10145842\n",
      "Iteration 2813, loss = 0.10142554\n",
      "Iteration 2814, loss = 0.10139271\n",
      "Iteration 2815, loss = 0.10135992\n",
      "Iteration 2816, loss = 0.10132718\n",
      "Iteration 2817, loss = 0.10129448\n",
      "Iteration 2818, loss = 0.10126183\n",
      "Iteration 2819, loss = 0.10122921\n",
      "Iteration 2820, loss = 0.10119665\n",
      "Iteration 2821, loss = 0.10116413\n",
      "Iteration 2822, loss = 0.10113165\n",
      "Iteration 2823, loss = 0.10109921\n",
      "Iteration 2824, loss = 0.10106683\n",
      "Iteration 2825, loss = 0.10103448\n",
      "Iteration 2826, loss = 0.10100218\n",
      "Iteration 2827, loss = 0.10096992\n",
      "Iteration 2828, loss = 0.10093771\n",
      "Iteration 2829, loss = 0.10090555\n",
      "Iteration 2830, loss = 0.10087342\n",
      "Iteration 2831, loss = 0.10084135\n",
      "Iteration 2832, loss = 0.10080931\n",
      "Iteration 2833, loss = 0.10077733\n",
      "Iteration 2834, loss = 0.10074538\n",
      "Iteration 2835, loss = 0.10071348\n",
      "Iteration 2836, loss = 0.10068163\n",
      "Iteration 2837, loss = 0.10064982\n",
      "Iteration 2838, loss = 0.10061805\n",
      "Iteration 2839, loss = 0.10058634\n",
      "Iteration 2840, loss = 0.10055466\n",
      "Iteration 2841, loss = 0.10052303\n",
      "Iteration 2842, loss = 0.10049145\n",
      "Iteration 2843, loss = 0.10045991\n",
      "Iteration 2844, loss = 0.10042841\n",
      "Iteration 2845, loss = 0.10039696\n",
      "Iteration 2846, loss = 0.10036556\n",
      "Iteration 2847, loss = 0.10033420\n",
      "Iteration 2848, loss = 0.10030288\n",
      "Iteration 2849, loss = 0.10027161\n",
      "Iteration 2850, loss = 0.10024039\n",
      "Iteration 2851, loss = 0.10020921\n",
      "Iteration 2852, loss = 0.10017808\n",
      "Iteration 2853, loss = 0.10014699\n",
      "Iteration 2854, loss = 0.10011594\n",
      "Iteration 2855, loss = 0.10008495\n",
      "Iteration 2856, loss = 0.10005399\n",
      "Iteration 2857, loss = 0.10002309\n",
      "Iteration 2858, loss = 0.09999222\n",
      "Iteration 2859, loss = 0.09996141\n",
      "Iteration 2860, loss = 0.09993064\n",
      "Iteration 2861, loss = 0.09989991\n",
      "Iteration 2862, loss = 0.09986923\n",
      "Iteration 2863, loss = 0.09983860\n",
      "Iteration 2864, loss = 0.09980801\n",
      "Iteration 2865, loss = 0.09977746\n",
      "Iteration 2866, loss = 0.09974697\n",
      "Iteration 2867, loss = 0.09971651\n",
      "Iteration 2868, loss = 0.09968611\n",
      "Iteration 2869, loss = 0.09965575\n",
      "Iteration 2870, loss = 0.09962543\n",
      "Iteration 2871, loss = 0.09959516\n",
      "Iteration 2872, loss = 0.09956494\n",
      "Iteration 2873, loss = 0.09953476\n",
      "Iteration 2874, loss = 0.09950463\n",
      "Iteration 2875, loss = 0.09947455\n",
      "Iteration 2876, loss = 0.09944451\n",
      "Iteration 2877, loss = 0.09941451\n",
      "Iteration 2878, loss = 0.09938456\n",
      "Iteration 2879, loss = 0.09935466\n",
      "Iteration 2880, loss = 0.09932480\n",
      "Iteration 2881, loss = 0.09929499\n",
      "Iteration 2882, loss = 0.09926523\n",
      "Iteration 2883, loss = 0.09923551\n",
      "Iteration 2884, loss = 0.09920584\n",
      "Iteration 2885, loss = 0.09917621\n",
      "Iteration 2886, loss = 0.09914663\n",
      "Iteration 2887, loss = 0.09911710\n",
      "Iteration 2888, loss = 0.09908761\n",
      "Iteration 2889, loss = 0.09905817\n",
      "Iteration 2890, loss = 0.09902878\n",
      "Iteration 2891, loss = 0.09899943\n",
      "Iteration 2892, loss = 0.09897012\n",
      "Iteration 2893, loss = 0.09894087\n",
      "Iteration 2894, loss = 0.09891166\n",
      "Iteration 2895, loss = 0.09888249\n",
      "Iteration 2896, loss = 0.09885338\n",
      "Iteration 2897, loss = 0.09882431\n",
      "Iteration 2898, loss = 0.09879528\n",
      "Iteration 2899, loss = 0.09876630\n",
      "Iteration 2900, loss = 0.09873737\n",
      "Iteration 2901, loss = 0.09870848\n",
      "Iteration 2902, loss = 0.09867965\n",
      "Iteration 2903, loss = 0.09865085\n",
      "Iteration 2904, loss = 0.09862211\n",
      "Iteration 2905, loss = 0.09859341\n",
      "Iteration 2906, loss = 0.09856475\n",
      "Iteration 2907, loss = 0.09853615\n",
      "Iteration 2908, loss = 0.09850759\n",
      "Iteration 2909, loss = 0.09847907\n",
      "Iteration 2910, loss = 0.09845061\n",
      "Iteration 2911, loss = 0.09842219\n",
      "Iteration 2912, loss = 0.09839381\n",
      "Iteration 2913, loss = 0.09836548\n",
      "Iteration 2914, loss = 0.09833720\n",
      "Iteration 2915, loss = 0.09830897\n",
      "Iteration 2916, loss = 0.09828078\n",
      "Iteration 2917, loss = 0.09825264\n",
      "Iteration 2918, loss = 0.09822455\n",
      "Iteration 2919, loss = 0.09819650\n",
      "Iteration 2920, loss = 0.09816850\n",
      "Iteration 2921, loss = 0.09814055\n",
      "Iteration 2922, loss = 0.09811264\n",
      "Iteration 2923, loss = 0.09808478\n",
      "Iteration 2924, loss = 0.09805697\n",
      "Iteration 2925, loss = 0.09802920\n",
      "Iteration 2926, loss = 0.09800148\n",
      "Iteration 2927, loss = 0.09797381\n",
      "Iteration 2928, loss = 0.09794618\n",
      "Iteration 2929, loss = 0.09791860\n",
      "Iteration 2930, loss = 0.09789107\n",
      "Iteration 2931, loss = 0.09786383\n",
      "Iteration 2932, loss = 0.09783832\n",
      "Iteration 2933, loss = 0.09781298\n",
      "Iteration 2934, loss = 0.09778780\n",
      "Iteration 2935, loss = 0.09776276\n",
      "Iteration 2936, loss = 0.09773786\n",
      "Iteration 2937, loss = 0.09771308\n",
      "Iteration 2938, loss = 0.09768842\n",
      "Iteration 2939, loss = 0.09766387\n",
      "Iteration 2940, loss = 0.09763943\n",
      "Iteration 2941, loss = 0.09761508\n",
      "Iteration 2942, loss = 0.09759082\n",
      "Iteration 2943, loss = 0.09756665\n",
      "Iteration 2944, loss = 0.09754256\n",
      "Iteration 2945, loss = 0.09751855\n",
      "Iteration 2946, loss = 0.09749461\n",
      "Iteration 2947, loss = 0.09747074\n",
      "Iteration 2948, loss = 0.09744694\n",
      "Iteration 2949, loss = 0.09742321\n",
      "Iteration 2950, loss = 0.09739954\n",
      "Iteration 2951, loss = 0.09737593\n",
      "Iteration 2952, loss = 0.09735238\n",
      "Iteration 2953, loss = 0.09732889\n",
      "Iteration 2954, loss = 0.09730545\n",
      "Iteration 2955, loss = 0.09728206\n",
      "Iteration 2956, loss = 0.09725873\n",
      "Iteration 2957, loss = 0.09723545\n",
      "Iteration 2958, loss = 0.09721222\n",
      "Iteration 2959, loss = 0.09718904\n",
      "Iteration 2960, loss = 0.09716591\n",
      "Iteration 2961, loss = 0.09714283\n",
      "Iteration 2962, loss = 0.09711980\n",
      "Iteration 2963, loss = 0.09709681\n",
      "Iteration 2964, loss = 0.09707386\n",
      "Iteration 2965, loss = 0.09705097\n",
      "Iteration 2966, loss = 0.09702812\n",
      "Iteration 2967, loss = 0.09700531\n",
      "Iteration 2968, loss = 0.09698255\n",
      "Iteration 2969, loss = 0.09695983\n",
      "Iteration 2970, loss = 0.09693709\n",
      "Iteration 2971, loss = 0.09691423\n",
      "Iteration 2972, loss = 0.09689138\n",
      "Iteration 2973, loss = 0.09686856\n",
      "Iteration 2974, loss = 0.09684577\n",
      "Iteration 2975, loss = 0.09682408\n",
      "Iteration 2976, loss = 0.09680261\n",
      "Iteration 2977, loss = 0.09678125\n",
      "Iteration 2978, loss = 0.09676000\n",
      "Iteration 2979, loss = 0.09673885\n",
      "Iteration 2980, loss = 0.09671778\n",
      "Iteration 2981, loss = 0.09669680\n",
      "Iteration 2982, loss = 0.09667590\n",
      "Iteration 2983, loss = 0.09665507\n",
      "Iteration 2984, loss = 0.09663431\n",
      "Iteration 2985, loss = 0.09661362\n",
      "Iteration 2986, loss = 0.09659299\n",
      "Iteration 2987, loss = 0.09657242\n",
      "Iteration 2988, loss = 0.09655190\n",
      "Iteration 2989, loss = 0.09653144\n",
      "Iteration 2990, loss = 0.09651104\n",
      "Iteration 2991, loss = 0.09649068\n",
      "Iteration 2992, loss = 0.09647125\n",
      "Iteration 2993, loss = 0.09645386\n",
      "Iteration 2994, loss = 0.09643655\n",
      "Iteration 2995, loss = 0.09641931\n",
      "Iteration 2996, loss = 0.09640214\n",
      "Iteration 2997, loss = 0.09638505\n",
      "Iteration 2998, loss = 0.09636802\n",
      "Iteration 2999, loss = 0.09635106\n",
      "Iteration 3000, loss = 0.09633416\n",
      "Iteration 3001, loss = 0.09631732\n",
      "Iteration 3002, loss = 0.09630055\n",
      "Iteration 3003, loss = 0.09628383\n",
      "Iteration 3004, loss = 0.09626717\n",
      "Iteration 3005, loss = 0.09625056\n",
      "Iteration 3006, loss = 0.09623401\n",
      "Iteration 3007, loss = 0.09621751\n",
      "Iteration 3008, loss = 0.09620107\n",
      "Iteration 3009, loss = 0.09618467\n",
      "Iteration 3010, loss = 0.09616833\n",
      "Iteration 3011, loss = 0.09615204\n",
      "Iteration 3012, loss = 0.09613580\n",
      "Iteration 3013, loss = 0.09611961\n",
      "Iteration 3014, loss = 0.09610347\n",
      "Iteration 3015, loss = 0.09608737\n",
      "Iteration 3016, loss = 0.09607132\n",
      "Iteration 3017, loss = 0.09605532\n",
      "Iteration 3018, loss = 0.09603937\n",
      "Iteration 3019, loss = 0.09602346\n",
      "Iteration 3020, loss = 0.09600759\n",
      "Iteration 3021, loss = 0.09599177\n",
      "Iteration 3022, loss = 0.09597599\n",
      "Iteration 3023, loss = 0.09596026\n",
      "Iteration 3024, loss = 0.09594457\n",
      "Iteration 3025, loss = 0.09592892\n",
      "Iteration 3026, loss = 0.09591332\n",
      "Iteration 3027, loss = 0.09589775\n",
      "Iteration 3028, loss = 0.09588223\n",
      "Iteration 3029, loss = 0.09586675\n",
      "Iteration 3030, loss = 0.09585130\n",
      "Iteration 3031, loss = 0.09583590\n",
      "Iteration 3032, loss = 0.09582054\n",
      "Iteration 3033, loss = 0.09580521\n",
      "Iteration 3034, loss = 0.09578993\n",
      "Iteration 3035, loss = 0.09577468\n",
      "Iteration 3036, loss = 0.09575947\n",
      "Iteration 3037, loss = 0.09574430\n",
      "Iteration 3038, loss = 0.09572917\n",
      "Iteration 3039, loss = 0.09571407\n",
      "Iteration 3040, loss = 0.09569901\n",
      "Iteration 3041, loss = 0.09568399\n",
      "Iteration 3042, loss = 0.09566900\n",
      "Iteration 3043, loss = 0.09565405\n",
      "Iteration 3044, loss = 0.09563914\n",
      "Iteration 3045, loss = 0.09562426\n",
      "Iteration 3046, loss = 0.09560941\n",
      "Iteration 3047, loss = 0.09559461\n",
      "Iteration 3048, loss = 0.09557983\n",
      "Iteration 3049, loss = 0.09556510\n",
      "Iteration 3050, loss = 0.09555039\n",
      "Iteration 3051, loss = 0.09553572\n",
      "Iteration 3052, loss = 0.09552109\n",
      "Iteration 3053, loss = 0.09550649\n",
      "Iteration 3054, loss = 0.09549192\n",
      "Iteration 3055, loss = 0.09547739\n",
      "Iteration 3056, loss = 0.09546289\n",
      "Iteration 3057, loss = 0.09544842\n",
      "Iteration 3058, loss = 0.09543399\n",
      "Iteration 3059, loss = 0.09541959\n",
      "Iteration 3060, loss = 0.09540522\n",
      "Iteration 3061, loss = 0.09539089\n",
      "Iteration 3062, loss = 0.09537659\n",
      "Iteration 3063, loss = 0.09536232\n",
      "Iteration 3064, loss = 0.09534808\n",
      "Iteration 3065, loss = 0.09533388\n",
      "Iteration 3066, loss = 0.09531971\n",
      "Iteration 3067, loss = 0.09530557\n",
      "Iteration 3068, loss = 0.09529146\n",
      "Iteration 3069, loss = 0.09527739\n",
      "Iteration 3070, loss = 0.09526335\n",
      "Iteration 3071, loss = 0.09524934\n",
      "Iteration 3072, loss = 0.09523536\n",
      "Iteration 3073, loss = 0.09522141\n",
      "Iteration 3074, loss = 0.09520750\n",
      "Iteration 3075, loss = 0.09519361\n",
      "Iteration 3076, loss = 0.09517976\n",
      "Iteration 3077, loss = 0.09516594\n",
      "Iteration 3078, loss = 0.09515215\n",
      "Iteration 3079, loss = 0.09513839\n",
      "Iteration 3080, loss = 0.09512466\n",
      "Iteration 3081, loss = 0.09511096\n",
      "Iteration 3082, loss = 0.09509730\n",
      "Iteration 3083, loss = 0.09508366\n",
      "Iteration 3084, loss = 0.09507006\n",
      "Iteration 3085, loss = 0.09505649\n",
      "Iteration 3086, loss = 0.09504294\n",
      "Iteration 3087, loss = 0.09502943\n",
      "Iteration 3088, loss = 0.09501595\n",
      "Iteration 3089, loss = 0.09500250\n",
      "Iteration 3090, loss = 0.09498908\n",
      "Iteration 3091, loss = 0.09497569\n",
      "Iteration 3092, loss = 0.09496233\n",
      "Iteration 3093, loss = 0.09494900\n",
      "Iteration 3094, loss = 0.09493570\n",
      "Iteration 3095, loss = 0.09492244\n",
      "Iteration 3096, loss = 0.09490920\n",
      "Iteration 3097, loss = 0.09489599\n",
      "Iteration 3098, loss = 0.09488281\n",
      "Iteration 3099, loss = 0.09486966\n",
      "Iteration 3100, loss = 0.09485654\n",
      "Iteration 3101, loss = 0.09484346\n",
      "Iteration 3102, loss = 0.09483040\n",
      "Iteration 3103, loss = 0.09481737\n",
      "Iteration 3104, loss = 0.09480437\n",
      "Iteration 3105, loss = 0.09479140\n",
      "Iteration 3106, loss = 0.09477849\n",
      "Iteration 3107, loss = 0.09476561\n",
      "Iteration 3108, loss = 0.09475276\n",
      "Iteration 3109, loss = 0.09473994\n",
      "Iteration 3110, loss = 0.09472715\n",
      "Iteration 3111, loss = 0.09471440\n",
      "Iteration 3112, loss = 0.09470167\n",
      "Iteration 3113, loss = 0.09468898\n",
      "Iteration 3114, loss = 0.09467631\n",
      "Iteration 3115, loss = 0.09466367\n",
      "Iteration 3116, loss = 0.09465106\n",
      "Iteration 3117, loss = 0.09463848\n",
      "Iteration 3118, loss = 0.09462593\n",
      "Iteration 3119, loss = 0.09461341\n",
      "Iteration 3120, loss = 0.09460092\n",
      "Iteration 3121, loss = 0.09458846\n",
      "Iteration 3122, loss = 0.09457602\n",
      "Iteration 3123, loss = 0.09456361\n",
      "Iteration 3124, loss = 0.09455123\n",
      "Iteration 3125, loss = 0.09453888\n",
      "Iteration 3126, loss = 0.09452656\n",
      "Iteration 3127, loss = 0.09451426\n",
      "Iteration 3128, loss = 0.09450199\n",
      "Iteration 3129, loss = 0.09448975\n",
      "Iteration 3130, loss = 0.09447754\n",
      "Iteration 3131, loss = 0.09446535\n",
      "Iteration 3132, loss = 0.09445319\n",
      "Iteration 3133, loss = 0.09444106\n",
      "Iteration 3134, loss = 0.09442896\n",
      "Iteration 3135, loss = 0.09441688\n",
      "Iteration 3136, loss = 0.09440483\n",
      "Iteration 3137, loss = 0.09439281\n",
      "Iteration 3138, loss = 0.09438081\n",
      "Iteration 3139, loss = 0.09436884\n",
      "Iteration 3140, loss = 0.09435690\n",
      "Iteration 3141, loss = 0.09434499\n",
      "Iteration 3142, loss = 0.09433310\n",
      "Iteration 3143, loss = 0.09432124\n",
      "Iteration 3144, loss = 0.09430940\n",
      "Iteration 3145, loss = 0.09429759\n",
      "Iteration 3146, loss = 0.09428581\n",
      "Iteration 3147, loss = 0.09427406\n",
      "Iteration 3148, loss = 0.09426233\n",
      "Iteration 3149, loss = 0.09425062\n",
      "Iteration 3150, loss = 0.09423895\n",
      "Iteration 3151, loss = 0.09422730\n",
      "Iteration 3152, loss = 0.09421567\n",
      "Iteration 3153, loss = 0.09420408\n",
      "Iteration 3154, loss = 0.09419250\n",
      "Iteration 3155, loss = 0.09418096\n",
      "Iteration 3156, loss = 0.09416944\n",
      "Iteration 3157, loss = 0.09415795\n",
      "Iteration 3158, loss = 0.09414648\n",
      "Iteration 3159, loss = 0.09413504\n",
      "Iteration 3160, loss = 0.09412362\n",
      "Iteration 3161, loss = 0.09411223\n",
      "Iteration 3162, loss = 0.09410086\n",
      "Iteration 3163, loss = 0.09408952\n",
      "Iteration 3164, loss = 0.09407821\n",
      "Iteration 3165, loss = 0.09406692\n",
      "Iteration 3166, loss = 0.09405566\n",
      "Iteration 3167, loss = 0.09404442\n",
      "Iteration 3168, loss = 0.09403321\n",
      "Iteration 3169, loss = 0.09402202\n",
      "Iteration 3170, loss = 0.09401086\n",
      "Iteration 3171, loss = 0.09399973\n",
      "Iteration 3172, loss = 0.09398862\n",
      "Iteration 3173, loss = 0.09397753\n",
      "Iteration 3174, loss = 0.09396647\n",
      "Iteration 3175, loss = 0.09395543\n",
      "Iteration 3176, loss = 0.09394442\n",
      "Iteration 3177, loss = 0.09393344\n",
      "Iteration 3178, loss = 0.09392248\n",
      "Iteration 3179, loss = 0.09391154\n",
      "Iteration 3180, loss = 0.09390063\n",
      "Iteration 3181, loss = 0.09388975\n",
      "Iteration 3182, loss = 0.09387888\n",
      "Iteration 3183, loss = 0.09386805\n",
      "Iteration 3184, loss = 0.09385723\n",
      "Iteration 3185, loss = 0.09384645\n",
      "Iteration 3186, loss = 0.09383568\n",
      "Iteration 3187, loss = 0.09382495\n",
      "Iteration 3188, loss = 0.09381423\n",
      "Iteration 3189, loss = 0.09380354\n",
      "Iteration 3190, loss = 0.09379288\n",
      "Iteration 3191, loss = 0.09378223\n",
      "Iteration 3192, loss = 0.09377162\n",
      "Iteration 3193, loss = 0.09376102\n",
      "Iteration 3194, loss = 0.09375046\n",
      "Iteration 3195, loss = 0.09373991\n",
      "Iteration 3196, loss = 0.09372939\n",
      "Iteration 3197, loss = 0.09371889\n",
      "Iteration 3198, loss = 0.09370842\n",
      "Iteration 3199, loss = 0.09369797\n",
      "Iteration 3200, loss = 0.09368755\n",
      "Iteration 3201, loss = 0.09367715\n",
      "Iteration 3202, loss = 0.09366677\n",
      "Iteration 3203, loss = 0.09365641\n",
      "Iteration 3204, loss = 0.09364608\n",
      "Iteration 3205, loss = 0.09363578\n",
      "Iteration 3206, loss = 0.09362549\n",
      "Iteration 3207, loss = 0.09361523\n",
      "Iteration 3208, loss = 0.09360500\n",
      "Iteration 3209, loss = 0.09359479\n",
      "Iteration 3210, loss = 0.09358460\n",
      "Iteration 3211, loss = 0.09357443\n",
      "Iteration 3212, loss = 0.09356429\n",
      "Iteration 3213, loss = 0.09355417\n",
      "Iteration 3214, loss = 0.09354407\n",
      "Iteration 3215, loss = 0.09353400\n",
      "Iteration 3216, loss = 0.09352395\n",
      "Iteration 3217, loss = 0.09351392\n",
      "Iteration 3218, loss = 0.09350392\n",
      "Iteration 3219, loss = 0.09349394\n",
      "Iteration 3220, loss = 0.09348398\n",
      "Iteration 3221, loss = 0.09347404\n",
      "Iteration 3222, loss = 0.09346413\n",
      "Iteration 3223, loss = 0.09345424\n",
      "Iteration 3224, loss = 0.09344437\n",
      "Iteration 3225, loss = 0.09343453\n",
      "Iteration 3226, loss = 0.09342471\n",
      "Iteration 3227, loss = 0.09341491\n",
      "Iteration 3228, loss = 0.09340513\n",
      "Iteration 3229, loss = 0.09339537\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "MLP.score(X, y)=0.73\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PlotMLP(mlp, X, y):\n",
    "\n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(mlp, X, y):\n",
    "        y_pred_model1 = mlp.predict(X)\n",
    "\n",
    "        # call r2\n",
    "        score_model1 = r2_score(y, y_pred_model1)\n",
    "\n",
    "        return y_pred_model1, score_model1\n",
    "\n",
    "    y_pred_model1, score_model1 = CalcPredAndScore(\n",
    "        mlp, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([\"MLP\", \"X OECD data\"])\n",
    "    \n",
    "    print(f\"MLP.score(X, y)={score_model1:0.2f}\")\n",
    "\n",
    "normalizedX = (X-min(X))/(max(X)-min(X))\n",
    "\n",
    "# Setup MLPRegressor\n",
    "mlp.fit(normalizedX, y)\n",
    "\n",
    "# lets make a MLP regressor prediction and redo the plots\n",
    "\n",
    "PlotMLP(mlp, normalizedX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Scikit-learn Pipelines\n",
    "\n",
    "Now, rescale again, but use the `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "When this works put both the MLP and the scaler into a composite construction via `sklearn.pipeline.Pipeline`. This composite is just a new Scikit-learn estimator, and can be used just like any other `fit-predict` models, try it, and document it for the journal.\n",
    "\n",
    "(You could reuse the `PlotModels()` function by also retraining the linear regressor on the scaled data, or just write your own plot code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.42339562\n",
      "Iteration 2, loss = 19.39423028\n",
      "Iteration 3, loss = 19.36508278\n",
      "Iteration 4, loss = 19.33595344\n",
      "Iteration 5, loss = 19.30684256\n",
      "Iteration 6, loss = 19.27775039\n",
      "Iteration 7, loss = 19.24867567\n",
      "Iteration 8, loss = 19.21960129\n",
      "Iteration 9, loss = 19.19054516\n",
      "Iteration 10, loss = 19.16150760\n",
      "Iteration 11, loss = 19.13248884\n",
      "Iteration 12, loss = 19.10348908\n",
      "Iteration 13, loss = 19.07450849\n",
      "Iteration 14, loss = 19.04554720\n",
      "Iteration 15, loss = 19.01660532\n",
      "Iteration 16, loss = 18.98766703\n",
      "Iteration 17, loss = 18.95873930\n",
      "Iteration 18, loss = 18.92982942\n",
      "Iteration 19, loss = 18.90093751\n",
      "Iteration 20, loss = 18.87206368\n",
      "Iteration 21, loss = 18.84320798\n",
      "Iteration 22, loss = 18.81437042\n",
      "Iteration 23, loss = 18.78555099\n",
      "Iteration 24, loss = 18.75672912\n",
      "Iteration 25, loss = 18.72790735\n",
      "Iteration 26, loss = 18.69910061\n",
      "Iteration 27, loss = 18.67030901\n",
      "Iteration 28, loss = 18.64152294\n",
      "Iteration 29, loss = 18.61271952\n",
      "Iteration 30, loss = 18.58390099\n",
      "Iteration 31, loss = 18.55508202\n",
      "Iteration 32, loss = 18.52627233\n",
      "Iteration 33, loss = 18.49747224\n",
      "Iteration 34, loss = 18.46868195\n",
      "Iteration 35, loss = 18.43988355\n",
      "Iteration 36, loss = 18.41106456\n",
      "Iteration 37, loss = 18.38222300\n",
      "Iteration 38, loss = 18.35336474\n",
      "Iteration 39, loss = 18.32447083\n",
      "Iteration 40, loss = 18.29556000\n",
      "Iteration 41, loss = 18.26663534\n",
      "Iteration 42, loss = 18.23756210\n",
      "Iteration 43, loss = 18.20840336\n",
      "Iteration 44, loss = 18.17922880\n",
      "Iteration 45, loss = 18.15004001\n",
      "Iteration 46, loss = 18.12083834\n",
      "Iteration 47, loss = 18.09162489\n",
      "Iteration 48, loss = 18.06240053\n",
      "Iteration 49, loss = 18.03316596\n",
      "Iteration 50, loss = 18.00392173\n",
      "Iteration 51, loss = 17.97464999\n",
      "Iteration 52, loss = 17.94530685\n",
      "Iteration 53, loss = 17.91579165\n",
      "Iteration 54, loss = 17.88613803\n",
      "Iteration 55, loss = 17.85638643\n",
      "Iteration 56, loss = 17.82659949\n",
      "Iteration 57, loss = 17.79677956\n",
      "Iteration 58, loss = 17.76692856\n",
      "Iteration 59, loss = 17.73696081\n",
      "Iteration 60, loss = 17.70695099\n",
      "Iteration 61, loss = 17.67690722\n",
      "Iteration 62, loss = 17.64683103\n",
      "Iteration 63, loss = 17.61672368\n",
      "Iteration 64, loss = 17.58658620\n",
      "Iteration 65, loss = 17.55641946\n",
      "Iteration 66, loss = 17.52622415\n",
      "Iteration 67, loss = 17.49600085\n",
      "Iteration 68, loss = 17.46575001\n",
      "Iteration 69, loss = 17.43547203\n",
      "Iteration 70, loss = 17.40516718\n",
      "Iteration 71, loss = 17.37483572\n",
      "Iteration 72, loss = 17.34447783\n",
      "Iteration 73, loss = 17.31409363\n",
      "Iteration 74, loss = 17.28368323\n",
      "Iteration 75, loss = 17.25324669\n",
      "Iteration 76, loss = 17.22278406\n",
      "Iteration 77, loss = 17.19229534\n",
      "Iteration 78, loss = 17.16178053\n",
      "Iteration 79, loss = 17.13123962\n",
      "Iteration 80, loss = 17.10067256\n",
      "Iteration 81, loss = 17.07007933\n",
      "Iteration 82, loss = 17.03945986\n",
      "Iteration 83, loss = 17.00881409\n",
      "Iteration 84, loss = 16.97814197\n",
      "Iteration 85, loss = 16.94744341\n",
      "Iteration 86, loss = 16.91671836\n",
      "Iteration 87, loss = 16.88596673\n",
      "Iteration 88, loss = 16.85518845\n",
      "Iteration 89, loss = 16.82438344\n",
      "Iteration 90, loss = 16.79355162\n",
      "Iteration 91, loss = 16.76269293\n",
      "Iteration 92, loss = 16.73180728\n",
      "Iteration 93, loss = 16.70089460\n",
      "Iteration 94, loss = 16.66995482\n",
      "Iteration 95, loss = 16.63898787\n",
      "Iteration 96, loss = 16.60799369\n",
      "Iteration 97, loss = 16.57697221\n",
      "Iteration 98, loss = 16.54592336\n",
      "Iteration 99, loss = 16.51484709\n",
      "Iteration 100, loss = 16.48374334\n",
      "Iteration 101, loss = 16.45261206\n",
      "Iteration 102, loss = 16.42145320\n",
      "Iteration 103, loss = 16.39026671\n",
      "Iteration 104, loss = 16.35905255\n",
      "Iteration 105, loss = 16.32781069\n",
      "Iteration 106, loss = 16.29654107\n",
      "Iteration 107, loss = 16.26524368\n",
      "Iteration 108, loss = 16.23391848\n",
      "Iteration 109, loss = 16.20256545\n",
      "Iteration 110, loss = 16.17118455\n",
      "Iteration 111, loss = 16.13977579\n",
      "Iteration 112, loss = 16.10833913\n",
      "Iteration 113, loss = 16.07687457\n",
      "Iteration 114, loss = 16.04538210\n",
      "Iteration 115, loss = 16.01386171\n",
      "Iteration 116, loss = 15.98231341\n",
      "Iteration 117, loss = 15.95073719\n",
      "Iteration 118, loss = 15.91913306\n",
      "Iteration 119, loss = 15.88750103\n",
      "Iteration 120, loss = 15.85584110\n",
      "Iteration 121, loss = 15.82415331\n",
      "Iteration 122, loss = 15.79243765\n",
      "Iteration 123, loss = 15.76069416\n",
      "Iteration 124, loss = 15.72892287\n",
      "Iteration 125, loss = 15.69712379\n",
      "Iteration 126, loss = 15.66529696\n",
      "Iteration 127, loss = 15.63344241\n",
      "Iteration 128, loss = 15.60156019\n",
      "Iteration 129, loss = 15.56965033\n",
      "Iteration 130, loss = 15.53771288\n",
      "Iteration 131, loss = 15.50574789\n",
      "Iteration 132, loss = 15.47375540\n",
      "Iteration 133, loss = 15.44173547\n",
      "Iteration 134, loss = 15.40968815\n",
      "Iteration 135, loss = 15.37761351\n",
      "Iteration 136, loss = 15.34551160\n",
      "Iteration 137, loss = 15.31338250\n",
      "Iteration 138, loss = 15.28122626\n",
      "Iteration 139, loss = 15.24904297\n",
      "Iteration 140, loss = 15.21683269\n",
      "Iteration 141, loss = 15.18459550\n",
      "Iteration 142, loss = 15.15233148\n",
      "Iteration 143, loss = 15.12004072\n",
      "Iteration 144, loss = 15.08772329\n",
      "Iteration 145, loss = 15.05537929\n",
      "Iteration 146, loss = 15.02300881\n",
      "Iteration 147, loss = 14.99061194\n",
      "Iteration 148, loss = 14.95818878\n",
      "Iteration 149, loss = 14.92573943\n",
      "Iteration 150, loss = 14.89326399\n",
      "Iteration 151, loss = 14.86076255\n",
      "Iteration 152, loss = 14.82823524\n",
      "Iteration 153, loss = 14.79568217\n",
      "Iteration 154, loss = 14.76310343\n",
      "Iteration 155, loss = 14.73049915\n",
      "Iteration 156, loss = 14.69786945\n",
      "Iteration 157, loss = 14.66521445\n",
      "Iteration 158, loss = 14.63253427\n",
      "Iteration 159, loss = 14.59982903\n",
      "Iteration 160, loss = 14.56709886\n",
      "Iteration 161, loss = 14.53434390\n",
      "Iteration 162, loss = 14.50156428\n",
      "Iteration 163, loss = 14.46876013\n",
      "Iteration 164, loss = 14.43593159\n",
      "Iteration 165, loss = 14.40307881\n",
      "Iteration 166, loss = 14.37020192\n",
      "Iteration 167, loss = 14.33730107\n",
      "Iteration 168, loss = 14.30437641\n",
      "Iteration 169, loss = 14.27142810\n",
      "Iteration 170, loss = 14.23845627\n",
      "Iteration 171, loss = 14.20546110\n",
      "Iteration 172, loss = 14.17244273\n",
      "Iteration 173, loss = 14.13940133\n",
      "Iteration 174, loss = 14.10633706\n",
      "Iteration 175, loss = 14.07325008\n",
      "Iteration 176, loss = 14.04014057\n",
      "Iteration 177, loss = 14.00700869\n",
      "Iteration 178, loss = 13.97385462\n",
      "Iteration 179, loss = 13.94067852\n",
      "Iteration 180, loss = 13.90748058\n",
      "Iteration 181, loss = 13.87426098\n",
      "Iteration 182, loss = 13.84101989\n",
      "Iteration 183, loss = 13.80775750\n",
      "Iteration 184, loss = 13.77447400\n",
      "Iteration 185, loss = 13.74116957\n",
      "Iteration 186, loss = 13.70784441\n",
      "Iteration 187, loss = 13.67449870\n",
      "Iteration 188, loss = 13.64113265\n",
      "Iteration 189, loss = 13.60774645\n",
      "Iteration 190, loss = 13.57434029\n",
      "Iteration 191, loss = 13.54091439\n",
      "Iteration 192, loss = 13.50746894\n",
      "Iteration 193, loss = 13.47400415\n",
      "Iteration 194, loss = 13.44052023\n",
      "Iteration 195, loss = 13.40701739\n",
      "Iteration 196, loss = 13.37349584\n",
      "Iteration 197, loss = 13.33995579\n",
      "Iteration 198, loss = 13.30639747\n",
      "Iteration 199, loss = 13.27282109\n",
      "Iteration 200, loss = 13.23922687\n",
      "Iteration 201, loss = 13.20561503\n",
      "Iteration 202, loss = 13.17198581\n",
      "Iteration 203, loss = 13.13833942\n",
      "Iteration 204, loss = 13.10467610\n",
      "Iteration 205, loss = 13.07099607\n",
      "Iteration 206, loss = 13.03729958\n",
      "Iteration 207, loss = 13.00358685\n",
      "Iteration 208, loss = 12.96985812\n",
      "Iteration 209, loss = 12.93611364\n",
      "Iteration 210, loss = 12.90235364\n",
      "Iteration 211, loss = 12.86857836\n",
      "Iteration 212, loss = 12.83478806\n",
      "Iteration 213, loss = 12.80098298\n",
      "Iteration 214, loss = 12.76716336\n",
      "Iteration 215, loss = 12.73332947\n",
      "Iteration 216, loss = 12.69948154\n",
      "Iteration 217, loss = 12.66561985\n",
      "Iteration 218, loss = 12.63174463\n",
      "Iteration 219, loss = 12.59785616\n",
      "Iteration 220, loss = 12.56395470\n",
      "Iteration 221, loss = 12.53004050\n",
      "Iteration 222, loss = 12.49611383\n",
      "Iteration 223, loss = 12.46217495\n",
      "Iteration 224, loss = 12.42822414\n",
      "Iteration 225, loss = 12.39426166\n",
      "Iteration 226, loss = 12.36028779\n",
      "Iteration 227, loss = 12.32630280\n",
      "Iteration 228, loss = 12.29230697\n",
      "Iteration 229, loss = 12.25830056\n",
      "Iteration 230, loss = 12.22428387\n",
      "Iteration 231, loss = 12.19025716\n",
      "Iteration 232, loss = 12.15622073\n",
      "Iteration 233, loss = 12.12217486\n",
      "Iteration 234, loss = 12.08811983\n",
      "Iteration 235, loss = 12.05405594\n",
      "Iteration 236, loss = 12.01998346\n",
      "Iteration 237, loss = 11.98590269\n",
      "Iteration 238, loss = 11.95181393\n",
      "Iteration 239, loss = 11.91771747\n",
      "Iteration 240, loss = 11.88361360\n",
      "Iteration 241, loss = 11.84950262\n",
      "Iteration 242, loss = 11.81538483\n",
      "Iteration 243, loss = 11.78126054\n",
      "Iteration 244, loss = 11.74713004\n",
      "Iteration 245, loss = 11.71299364\n",
      "Iteration 246, loss = 11.67885164\n",
      "Iteration 247, loss = 11.64470436\n",
      "Iteration 248, loss = 11.61055210\n",
      "Iteration 249, loss = 11.57639516\n",
      "Iteration 250, loss = 11.54223387\n",
      "Iteration 251, loss = 11.50806854\n",
      "Iteration 252, loss = 11.47389948\n",
      "Iteration 253, loss = 11.43972701\n",
      "Iteration 254, loss = 11.40555145\n",
      "Iteration 255, loss = 11.37137311\n",
      "Iteration 256, loss = 11.33719232\n",
      "Iteration 257, loss = 11.30300940\n",
      "Iteration 258, loss = 11.26882467\n",
      "Iteration 259, loss = 11.23463846\n",
      "Iteration 260, loss = 11.20045110\n",
      "Iteration 261, loss = 11.16626291\n",
      "Iteration 262, loss = 11.13207423\n",
      "Iteration 263, loss = 11.09788538\n",
      "Iteration 264, loss = 11.06369670\n",
      "Iteration 265, loss = 11.02950853\n",
      "Iteration 266, loss = 10.99532118\n",
      "Iteration 267, loss = 10.96113502\n",
      "Iteration 268, loss = 10.92695036\n",
      "Iteration 269, loss = 10.89276755\n",
      "Iteration 270, loss = 10.85858694\n",
      "Iteration 271, loss = 10.82440885\n",
      "Iteration 272, loss = 10.79023365\n",
      "Iteration 273, loss = 10.75606166\n",
      "Iteration 274, loss = 10.72189324\n",
      "Iteration 275, loss = 10.68772873\n",
      "Iteration 276, loss = 10.65356848\n",
      "Iteration 277, loss = 10.61941285\n",
      "Iteration 278, loss = 10.58526217\n",
      "Iteration 279, loss = 10.55111681\n",
      "Iteration 280, loss = 10.51697711\n",
      "Iteration 281, loss = 10.48284343\n",
      "Iteration 282, loss = 10.44871612\n",
      "Iteration 283, loss = 10.41459555\n",
      "Iteration 284, loss = 10.38048206\n",
      "Iteration 285, loss = 10.34637602\n",
      "Iteration 286, loss = 10.31227779\n",
      "Iteration 287, loss = 10.27818773\n",
      "Iteration 288, loss = 10.24410619\n",
      "Iteration 289, loss = 10.21003355\n",
      "Iteration 290, loss = 10.17597016\n",
      "Iteration 291, loss = 10.14191640\n",
      "Iteration 292, loss = 10.10787262\n",
      "Iteration 293, loss = 10.07383920\n",
      "Iteration 294, loss = 10.03981650\n",
      "Iteration 295, loss = 10.00580489\n",
      "Iteration 296, loss = 9.97180475\n",
      "Iteration 297, loss = 9.93781643\n",
      "Iteration 298, loss = 9.90384032\n",
      "Iteration 299, loss = 9.86987679\n",
      "Iteration 300, loss = 9.83592621\n",
      "Iteration 301, loss = 9.80198895\n",
      "Iteration 302, loss = 9.76806540\n",
      "Iteration 303, loss = 9.73415592\n",
      "Iteration 304, loss = 9.70026090\n",
      "Iteration 305, loss = 9.66638071\n",
      "Iteration 306, loss = 9.63251574\n",
      "Iteration 307, loss = 9.59866635\n",
      "Iteration 308, loss = 9.56483294\n",
      "Iteration 309, loss = 9.53101588\n",
      "Iteration 310, loss = 9.49721556\n",
      "Iteration 311, loss = 9.46343235\n",
      "Iteration 312, loss = 9.42966665\n",
      "Iteration 313, loss = 9.39591883\n",
      "Iteration 314, loss = 9.36218928\n",
      "Iteration 315, loss = 9.32847839\n",
      "Iteration 316, loss = 9.29478655\n",
      "Iteration 317, loss = 9.26111413\n",
      "Iteration 318, loss = 9.22746153\n",
      "Iteration 319, loss = 9.19382914\n",
      "Iteration 320, loss = 9.16021735\n",
      "Iteration 321, loss = 9.12662653\n",
      "Iteration 322, loss = 9.09305710\n",
      "Iteration 323, loss = 9.05950943\n",
      "Iteration 324, loss = 9.02598391\n",
      "Iteration 325, loss = 8.99248095\n",
      "Iteration 326, loss = 8.95900092\n",
      "Iteration 327, loss = 8.92554423\n",
      "Iteration 328, loss = 8.89211127\n",
      "Iteration 329, loss = 8.85870243\n",
      "Iteration 330, loss = 8.82531810\n",
      "Iteration 331, loss = 8.79195869\n",
      "Iteration 332, loss = 8.75862458\n",
      "Iteration 333, loss = 8.72531617\n",
      "Iteration 334, loss = 8.69203385\n",
      "Iteration 335, loss = 8.65877804\n",
      "Iteration 336, loss = 8.62554911\n",
      "Iteration 337, loss = 8.59234746\n",
      "Iteration 338, loss = 8.55917351\n",
      "Iteration 339, loss = 8.52602763\n",
      "Iteration 340, loss = 8.49291023\n",
      "Iteration 341, loss = 8.45982172\n",
      "Iteration 342, loss = 8.42676248\n",
      "Iteration 343, loss = 8.39373291\n",
      "Iteration 344, loss = 8.36073343\n",
      "Iteration 345, loss = 8.32776441\n",
      "Iteration 346, loss = 8.29482627\n",
      "Iteration 347, loss = 8.26191940\n",
      "Iteration 348, loss = 8.22904421\n",
      "Iteration 349, loss = 8.19620109\n",
      "Iteration 350, loss = 8.16339045\n",
      "Iteration 351, loss = 8.13061267\n",
      "Iteration 352, loss = 8.09786817\n",
      "Iteration 353, loss = 8.06515735\n",
      "Iteration 354, loss = 8.03248060\n",
      "Iteration 355, loss = 7.99983833\n",
      "Iteration 356, loss = 7.96723093\n",
      "Iteration 357, loss = 7.93465881\n",
      "Iteration 358, loss = 7.90212237\n",
      "Iteration 359, loss = 7.86962200\n",
      "Iteration 360, loss = 7.83715811\n",
      "Iteration 361, loss = 7.80473110\n",
      "Iteration 362, loss = 7.77234137\n",
      "Iteration 363, loss = 7.73998932\n",
      "Iteration 364, loss = 7.70767535\n",
      "Iteration 365, loss = 7.67539986\n",
      "Iteration 366, loss = 7.64316324\n",
      "Iteration 367, loss = 7.61096591\n",
      "Iteration 368, loss = 7.57880825\n",
      "Iteration 369, loss = 7.54669067\n",
      "Iteration 370, loss = 7.51461357\n",
      "Iteration 371, loss = 7.48257734\n",
      "Iteration 372, loss = 7.45058238\n",
      "Iteration 373, loss = 7.41862910\n",
      "Iteration 374, loss = 7.38671789\n",
      "Iteration 375, loss = 7.35484914\n",
      "Iteration 376, loss = 7.32302326\n",
      "Iteration 377, loss = 7.29124064\n",
      "Iteration 378, loss = 7.25950168\n",
      "Iteration 379, loss = 7.22780677\n",
      "Iteration 380, loss = 7.19615631\n",
      "Iteration 381, loss = 7.16455070\n",
      "Iteration 382, loss = 7.13299033\n",
      "Iteration 383, loss = 7.10147559\n",
      "Iteration 384, loss = 7.07000688\n",
      "Iteration 385, loss = 7.03858459\n",
      "Iteration 386, loss = 7.00720911\n",
      "Iteration 387, loss = 6.97588084\n",
      "Iteration 388, loss = 6.94460017\n",
      "Iteration 389, loss = 6.91336749\n",
      "Iteration 390, loss = 6.88218318\n",
      "Iteration 391, loss = 6.85104765\n",
      "Iteration 392, loss = 6.81996127\n",
      "Iteration 393, loss = 6.78892444\n",
      "Iteration 394, loss = 6.75793754\n",
      "Iteration 395, loss = 6.72700096\n",
      "Iteration 396, loss = 6.69611510\n",
      "Iteration 397, loss = 6.66528032\n",
      "Iteration 398, loss = 6.63449702\n",
      "Iteration 399, loss = 6.60376559\n",
      "Iteration 400, loss = 6.57308640\n",
      "Iteration 401, loss = 6.54245983\n",
      "Iteration 402, loss = 6.51188628\n",
      "Iteration 403, loss = 6.48136611\n",
      "Iteration 404, loss = 6.45089971\n",
      "Iteration 405, loss = 6.42048746\n",
      "Iteration 406, loss = 6.39012973\n",
      "Iteration 407, loss = 6.35982691\n",
      "Iteration 408, loss = 6.32957936\n",
      "Iteration 409, loss = 6.29938747\n",
      "Iteration 410, loss = 6.26925160\n",
      "Iteration 411, loss = 6.23917213\n",
      "Iteration 412, loss = 6.20914943\n",
      "Iteration 413, loss = 6.17918387\n",
      "Iteration 414, loss = 6.14927582\n",
      "Iteration 415, loss = 6.11942566\n",
      "Iteration 416, loss = 6.08963373\n",
      "Iteration 417, loss = 6.05990042\n",
      "Iteration 418, loss = 6.03022609\n",
      "Iteration 419, loss = 6.00061110\n",
      "Iteration 420, loss = 5.97105581\n",
      "Iteration 421, loss = 5.94156058\n",
      "Iteration 422, loss = 5.91212578\n",
      "Iteration 423, loss = 5.88275175\n",
      "Iteration 424, loss = 5.85343887\n",
      "Iteration 425, loss = 5.82418748\n",
      "Iteration 426, loss = 5.79499795\n",
      "Iteration 427, loss = 5.76587061\n",
      "Iteration 428, loss = 5.73680583\n",
      "Iteration 429, loss = 5.70780395\n",
      "Iteration 430, loss = 5.67886533\n",
      "Iteration 431, loss = 5.64999031\n",
      "Iteration 432, loss = 5.62117923\n",
      "Iteration 433, loss = 5.59243244\n",
      "Iteration 434, loss = 5.56375029\n",
      "Iteration 435, loss = 5.53513311\n",
      "Iteration 436, loss = 5.50658124\n",
      "Iteration 437, loss = 5.47809503\n",
      "Iteration 438, loss = 5.44967480\n",
      "Iteration 439, loss = 5.42132090\n",
      "Iteration 440, loss = 5.39303366\n",
      "Iteration 441, loss = 5.36481340\n",
      "Iteration 442, loss = 5.33666046\n",
      "Iteration 443, loss = 5.30857516\n",
      "Iteration 444, loss = 5.28055783\n",
      "Iteration 445, loss = 5.25260880\n",
      "Iteration 446, loss = 5.22472839\n",
      "Iteration 447, loss = 5.19691692\n",
      "Iteration 448, loss = 5.16917470\n",
      "Iteration 449, loss = 5.14150206\n",
      "Iteration 450, loss = 5.11389932\n",
      "Iteration 451, loss = 5.08636677\n",
      "Iteration 452, loss = 5.05890474\n",
      "Iteration 453, loss = 5.03151353\n",
      "Iteration 454, loss = 5.00419346\n",
      "Iteration 455, loss = 4.97694482\n",
      "Iteration 456, loss = 4.94976793\n",
      "Iteration 457, loss = 4.92266308\n",
      "Iteration 458, loss = 4.89563057\n",
      "Iteration 459, loss = 4.86867070\n",
      "Iteration 460, loss = 4.84178376\n",
      "Iteration 461, loss = 4.81497006\n",
      "Iteration 462, loss = 4.78822987\n",
      "Iteration 463, loss = 4.76156349\n",
      "Iteration 464, loss = 4.73497120\n",
      "Iteration 465, loss = 4.70845330\n",
      "Iteration 466, loss = 4.68201006\n",
      "Iteration 467, loss = 4.65564176\n",
      "Iteration 468, loss = 4.62934868\n",
      "Iteration 469, loss = 4.60313109\n",
      "Iteration 470, loss = 4.57698928\n",
      "Iteration 471, loss = 4.55092350\n",
      "Iteration 472, loss = 4.52493404\n",
      "Iteration 473, loss = 4.49902115\n",
      "Iteration 474, loss = 4.47318510\n",
      "Iteration 475, loss = 4.44742615\n",
      "Iteration 476, loss = 4.42174456\n",
      "Iteration 477, loss = 4.39614059\n",
      "Iteration 478, loss = 4.37061449\n",
      "Iteration 479, loss = 4.34516652\n",
      "Iteration 480, loss = 4.31979692\n",
      "Iteration 481, loss = 4.29450594\n",
      "Iteration 482, loss = 4.26929382\n",
      "Iteration 483, loss = 4.24416081\n",
      "Iteration 484, loss = 4.21910715\n",
      "Iteration 485, loss = 4.19413308\n",
      "Iteration 486, loss = 4.16923882\n",
      "Iteration 487, loss = 4.14442461\n",
      "Iteration 488, loss = 4.11969068\n",
      "Iteration 489, loss = 4.09503725\n",
      "Iteration 490, loss = 4.07046455\n",
      "Iteration 491, loss = 4.04597281\n",
      "Iteration 492, loss = 4.02156223\n",
      "Iteration 493, loss = 3.99723304\n",
      "Iteration 494, loss = 3.97298544\n",
      "Iteration 495, loss = 3.94881966\n",
      "Iteration 496, loss = 3.92473588\n",
      "Iteration 497, loss = 3.90073433\n",
      "Iteration 498, loss = 3.87681521\n",
      "Iteration 499, loss = 3.85297870\n",
      "Iteration 500, loss = 3.82922501\n",
      "Iteration 501, loss = 3.80555434\n",
      "Iteration 502, loss = 3.78196687\n",
      "Iteration 503, loss = 3.75846279\n",
      "Iteration 504, loss = 3.73504228\n",
      "Iteration 505, loss = 3.71170553\n",
      "Iteration 506, loss = 3.68845272\n",
      "Iteration 507, loss = 3.66528403\n",
      "Iteration 508, loss = 3.64219962\n",
      "Iteration 509, loss = 3.61919967\n",
      "Iteration 510, loss = 3.59628435\n",
      "Iteration 511, loss = 3.57345382\n",
      "Iteration 512, loss = 3.55070824\n",
      "Iteration 513, loss = 3.52804777\n",
      "Iteration 514, loss = 3.50547256\n",
      "Iteration 515, loss = 3.48298278\n",
      "Iteration 516, loss = 3.46057857\n",
      "Iteration 517, loss = 3.43826007\n",
      "Iteration 518, loss = 3.41602743\n",
      "Iteration 519, loss = 3.39388079\n",
      "Iteration 520, loss = 3.37182028\n",
      "Iteration 521, loss = 3.34984605\n",
      "Iteration 522, loss = 3.32795822\n",
      "Iteration 523, loss = 3.30615692\n",
      "Iteration 524, loss = 3.28444228\n",
      "Iteration 525, loss = 3.26281442\n",
      "Iteration 526, loss = 3.24127345\n",
      "Iteration 527, loss = 3.21981950\n",
      "Iteration 528, loss = 3.19845267\n",
      "Iteration 529, loss = 3.17717307\n",
      "Iteration 530, loss = 3.15598082\n",
      "Iteration 531, loss = 3.13487601\n",
      "Iteration 532, loss = 3.11385874\n",
      "Iteration 533, loss = 3.09292912\n",
      "Iteration 534, loss = 3.07208722\n",
      "Iteration 535, loss = 3.05133316\n",
      "Iteration 536, loss = 3.03066700\n",
      "Iteration 537, loss = 3.01008884\n",
      "Iteration 538, loss = 2.98959875\n",
      "Iteration 539, loss = 2.96919681\n",
      "Iteration 540, loss = 2.94888311\n",
      "Iteration 541, loss = 2.92865770\n",
      "Iteration 542, loss = 2.90852066\n",
      "Iteration 543, loss = 2.88847204\n",
      "Iteration 544, loss = 2.86851192\n",
      "Iteration 545, loss = 2.84864035\n",
      "Iteration 546, loss = 2.82885738\n",
      "Iteration 547, loss = 2.80916307\n",
      "Iteration 548, loss = 2.78955745\n",
      "Iteration 549, loss = 2.77004059\n",
      "Iteration 550, loss = 2.75061252\n",
      "Iteration 551, loss = 2.73127327\n",
      "Iteration 552, loss = 2.71202289\n",
      "Iteration 553, loss = 2.69286140\n",
      "Iteration 554, loss = 2.67378884\n",
      "Iteration 555, loss = 2.65480522\n",
      "Iteration 556, loss = 2.63591057\n",
      "Iteration 557, loss = 2.61710491\n",
      "Iteration 558, loss = 2.59838825\n",
      "Iteration 559, loss = 2.57976061\n",
      "Iteration 560, loss = 2.56122200\n",
      "Iteration 561, loss = 2.54277241\n",
      "Iteration 562, loss = 2.52441185\n",
      "Iteration 563, loss = 2.50614032\n",
      "Iteration 564, loss = 2.48795782\n",
      "Iteration 565, loss = 2.46986434\n",
      "Iteration 566, loss = 2.45185986\n",
      "Iteration 567, loss = 2.43394438\n",
      "Iteration 568, loss = 2.41611787\n",
      "Iteration 569, loss = 2.39838031\n",
      "Iteration 570, loss = 2.38073168\n",
      "Iteration 571, loss = 2.36317196\n",
      "Iteration 572, loss = 2.34570110\n",
      "Iteration 573, loss = 2.32831908\n",
      "Iteration 574, loss = 2.31102586\n",
      "Iteration 575, loss = 2.29382141\n",
      "Iteration 576, loss = 2.27670566\n",
      "Iteration 577, loss = 2.25967859\n",
      "Iteration 578, loss = 2.24274013\n",
      "Iteration 579, loss = 2.22589024\n",
      "Iteration 580, loss = 2.20912885\n",
      "Iteration 581, loss = 2.19245591\n",
      "Iteration 582, loss = 2.17587136\n",
      "Iteration 583, loss = 2.15937513\n",
      "Iteration 584, loss = 2.14296714\n",
      "Iteration 585, loss = 2.12664733\n",
      "Iteration 586, loss = 2.11041562\n",
      "Iteration 587, loss = 2.09427193\n",
      "Iteration 588, loss = 2.07821617\n",
      "Iteration 589, loss = 2.06224827\n",
      "Iteration 590, loss = 2.04636814\n",
      "Iteration 591, loss = 2.03057568\n",
      "Iteration 592, loss = 2.01487079\n",
      "Iteration 593, loss = 1.99925338\n",
      "Iteration 594, loss = 1.98372336\n",
      "Iteration 595, loss = 1.96828060\n",
      "Iteration 596, loss = 1.95292502\n",
      "Iteration 597, loss = 1.93765649\n",
      "Iteration 598, loss = 1.92247490\n",
      "Iteration 599, loss = 1.90738014\n",
      "Iteration 600, loss = 1.89237208\n",
      "Iteration 601, loss = 1.87745061\n",
      "Iteration 602, loss = 1.86261560\n",
      "Iteration 603, loss = 1.84786692\n",
      "Iteration 604, loss = 1.83320444\n",
      "Iteration 605, loss = 1.81862802\n",
      "Iteration 606, loss = 1.80413752\n",
      "Iteration 607, loss = 1.78973282\n",
      "Iteration 608, loss = 1.77541375\n",
      "Iteration 609, loss = 1.76118019\n",
      "Iteration 610, loss = 1.74703197\n",
      "Iteration 611, loss = 1.73296894\n",
      "Iteration 612, loss = 1.71899096\n",
      "Iteration 613, loss = 1.70509785\n",
      "Iteration 614, loss = 1.69128947\n",
      "Iteration 615, loss = 1.67756565\n",
      "Iteration 616, loss = 1.66392622\n",
      "Iteration 617, loss = 1.65037101\n",
      "Iteration 618, loss = 1.63689985\n",
      "Iteration 619, loss = 1.62351257\n",
      "Iteration 620, loss = 1.61020899\n",
      "Iteration 621, loss = 1.59698893\n",
      "Iteration 622, loss = 1.58385220\n",
      "Iteration 623, loss = 1.57079863\n",
      "Iteration 624, loss = 1.55782802\n",
      "Iteration 625, loss = 1.54494018\n",
      "Iteration 626, loss = 1.53213492\n",
      "Iteration 627, loss = 1.51941204\n",
      "Iteration 628, loss = 1.50677135\n",
      "Iteration 629, loss = 1.49421264\n",
      "Iteration 630, loss = 1.48173571\n",
      "Iteration 631, loss = 1.46934035\n",
      "Iteration 632, loss = 1.45702636\n",
      "Iteration 633, loss = 1.44479353\n",
      "Iteration 634, loss = 1.43264163\n",
      "Iteration 635, loss = 1.42057046\n",
      "Iteration 636, loss = 1.40857980\n",
      "Iteration 637, loss = 1.39666942\n",
      "Iteration 638, loss = 1.38483910\n",
      "Iteration 639, loss = 1.37308863\n",
      "Iteration 640, loss = 1.36141776\n",
      "Iteration 641, loss = 1.34982626\n",
      "Iteration 642, loss = 1.33831392\n",
      "Iteration 643, loss = 1.32688049\n",
      "Iteration 644, loss = 1.31552573\n",
      "Iteration 645, loss = 1.30424941\n",
      "Iteration 646, loss = 1.29305128\n",
      "Iteration 647, loss = 1.28193110\n",
      "Iteration 648, loss = 1.27088863\n",
      "Iteration 649, loss = 1.25992362\n",
      "Iteration 650, loss = 1.24903582\n",
      "Iteration 651, loss = 1.23822497\n",
      "Iteration 652, loss = 1.22749082\n",
      "Iteration 653, loss = 1.21683312\n",
      "Iteration 654, loss = 1.20625161\n",
      "Iteration 655, loss = 1.19574603\n",
      "Iteration 656, loss = 1.18531612\n",
      "Iteration 657, loss = 1.17496161\n",
      "Iteration 658, loss = 1.16468224\n",
      "Iteration 659, loss = 1.15447774\n",
      "Iteration 660, loss = 1.14434783\n",
      "Iteration 661, loss = 1.13429226\n",
      "Iteration 662, loss = 1.12431075\n",
      "Iteration 663, loss = 1.11440302\n",
      "Iteration 664, loss = 1.10456880\n",
      "Iteration 665, loss = 1.09480780\n",
      "Iteration 666, loss = 1.08511976\n",
      "Iteration 667, loss = 1.07550438\n",
      "Iteration 668, loss = 1.06596138\n",
      "Iteration 669, loss = 1.05649049\n",
      "Iteration 670, loss = 1.04709140\n",
      "Iteration 671, loss = 1.03776385\n",
      "Iteration 672, loss = 1.02850753\n",
      "Iteration 673, loss = 1.01932216\n",
      "Iteration 674, loss = 1.01020744\n",
      "Iteration 675, loss = 1.00116308\n",
      "Iteration 676, loss = 0.99218879\n",
      "Iteration 677, loss = 0.98328426\n",
      "Iteration 678, loss = 0.97444921\n",
      "Iteration 679, loss = 0.96568333\n",
      "Iteration 680, loss = 0.95698632\n",
      "Iteration 681, loss = 0.94835788\n",
      "Iteration 682, loss = 0.93979771\n",
      "Iteration 683, loss = 0.93130550\n",
      "Iteration 684, loss = 0.92288095\n",
      "Iteration 685, loss = 0.91452375\n",
      "Iteration 686, loss = 0.90623360\n",
      "Iteration 687, loss = 0.89801018\n",
      "Iteration 688, loss = 0.88985319\n",
      "Iteration 689, loss = 0.88176231\n",
      "Iteration 690, loss = 0.87373724\n",
      "Iteration 691, loss = 0.86577766\n",
      "Iteration 692, loss = 0.85788325\n",
      "Iteration 693, loss = 0.85005371\n",
      "Iteration 694, loss = 0.84228871\n",
      "Iteration 695, loss = 0.83458795\n",
      "Iteration 696, loss = 0.82695110\n",
      "Iteration 697, loss = 0.81937784\n",
      "Iteration 698, loss = 0.81186786\n",
      "Iteration 699, loss = 0.80442084\n",
      "Iteration 700, loss = 0.79703645\n",
      "Iteration 701, loss = 0.78971438\n",
      "Iteration 702, loss = 0.78245430\n",
      "Iteration 703, loss = 0.77525590\n",
      "Iteration 704, loss = 0.76811884\n",
      "Iteration 705, loss = 0.76104280\n",
      "Iteration 706, loss = 0.75402747\n",
      "Iteration 707, loss = 0.74707251\n",
      "Iteration 708, loss = 0.74017760\n",
      "Iteration 709, loss = 0.73334242\n",
      "Iteration 710, loss = 0.72656664\n",
      "Iteration 711, loss = 0.71984992\n",
      "Iteration 712, loss = 0.71319195\n",
      "Iteration 713, loss = 0.70659240\n",
      "Iteration 714, loss = 0.70005094\n",
      "Iteration 715, loss = 0.69356724\n",
      "Iteration 716, loss = 0.68714097\n",
      "Iteration 717, loss = 0.68077181\n",
      "Iteration 718, loss = 0.67445942\n",
      "Iteration 719, loss = 0.66820348\n",
      "Iteration 720, loss = 0.66200365\n",
      "Iteration 721, loss = 0.65585961\n",
      "Iteration 722, loss = 0.64977103\n",
      "Iteration 723, loss = 0.64373758\n",
      "Iteration 724, loss = 0.63775893\n",
      "Iteration 725, loss = 0.63183474\n",
      "Iteration 726, loss = 0.62596469\n",
      "Iteration 727, loss = 0.62014845\n",
      "Iteration 728, loss = 0.61438568\n",
      "Iteration 729, loss = 0.60867607\n",
      "Iteration 730, loss = 0.60301927\n",
      "Iteration 731, loss = 0.59741496\n",
      "Iteration 732, loss = 0.59186281\n",
      "Iteration 733, loss = 0.58636248\n",
      "Iteration 734, loss = 0.58091366\n",
      "Iteration 735, loss = 0.57551601\n",
      "Iteration 736, loss = 0.57016919\n",
      "Iteration 737, loss = 0.56487289\n",
      "Iteration 738, loss = 0.55962678\n",
      "Iteration 739, loss = 0.55443052\n",
      "Iteration 740, loss = 0.54928379\n",
      "Iteration 741, loss = 0.54418625\n",
      "Iteration 742, loss = 0.53913759\n",
      "Iteration 743, loss = 0.53413748\n",
      "Iteration 744, loss = 0.52918558\n",
      "Iteration 745, loss = 0.52428158\n",
      "Iteration 746, loss = 0.51942515\n",
      "Iteration 747, loss = 0.51461595\n",
      "Iteration 748, loss = 0.50985368\n",
      "Iteration 749, loss = 0.50513799\n",
      "Iteration 750, loss = 0.50046858\n",
      "Iteration 751, loss = 0.49584511\n",
      "Iteration 752, loss = 0.49126727\n",
      "Iteration 753, loss = 0.48673473\n",
      "Iteration 754, loss = 0.48224716\n",
      "Iteration 755, loss = 0.47780426\n",
      "Iteration 756, loss = 0.47340570\n",
      "Iteration 757, loss = 0.46905115\n",
      "Iteration 758, loss = 0.46474031\n",
      "Iteration 759, loss = 0.46047284\n",
      "Iteration 760, loss = 0.45624845\n",
      "Iteration 761, loss = 0.45206680\n",
      "Iteration 762, loss = 0.44792758\n",
      "Iteration 763, loss = 0.44383048\n",
      "Iteration 764, loss = 0.43977518\n",
      "Iteration 765, loss = 0.43576138\n",
      "Iteration 766, loss = 0.43178875\n",
      "Iteration 767, loss = 0.42785698\n",
      "Iteration 768, loss = 0.42396577\n",
      "Iteration 769, loss = 0.42011481\n",
      "Iteration 770, loss = 0.41630377\n",
      "Iteration 771, loss = 0.41253237\n",
      "Iteration 772, loss = 0.40880028\n",
      "Iteration 773, loss = 0.40510721\n",
      "Iteration 774, loss = 0.40145284\n",
      "Iteration 775, loss = 0.39783688\n",
      "Iteration 776, loss = 0.39425902\n",
      "Iteration 777, loss = 0.39071895\n",
      "Iteration 778, loss = 0.38721638\n",
      "Iteration 779, loss = 0.38375101\n",
      "Iteration 780, loss = 0.38032253\n",
      "Iteration 781, loss = 0.37693065\n",
      "Iteration 782, loss = 0.37357507\n",
      "Iteration 783, loss = 0.37025550\n",
      "Iteration 784, loss = 0.36697163\n",
      "Iteration 785, loss = 0.36372319\n",
      "Iteration 786, loss = 0.36050986\n",
      "Iteration 787, loss = 0.35733137\n",
      "Iteration 788, loss = 0.35418742\n",
      "Iteration 789, loss = 0.35107773\n",
      "Iteration 790, loss = 0.34800199\n",
      "Iteration 791, loss = 0.34495994\n",
      "Iteration 792, loss = 0.34195128\n",
      "Iteration 793, loss = 0.33897573\n",
      "Iteration 794, loss = 0.33603300\n",
      "Iteration 795, loss = 0.33312282\n",
      "Iteration 796, loss = 0.33024489\n",
      "Iteration 797, loss = 0.32739896\n",
      "Iteration 798, loss = 0.32458472\n",
      "Iteration 799, loss = 0.32180192\n",
      "Iteration 800, loss = 0.31905027\n",
      "Iteration 801, loss = 0.31632950\n",
      "Iteration 802, loss = 0.31363934\n",
      "Iteration 803, loss = 0.31097951\n",
      "Iteration 804, loss = 0.30834974\n",
      "Iteration 805, loss = 0.30574978\n",
      "Iteration 806, loss = 0.30317934\n",
      "Iteration 807, loss = 0.30063816\n",
      "Iteration 808, loss = 0.29812598\n",
      "Iteration 809, loss = 0.29564253\n",
      "Iteration 810, loss = 0.29318756\n",
      "Iteration 811, loss = 0.29076080\n",
      "Iteration 812, loss = 0.28836199\n",
      "Iteration 813, loss = 0.28599088\n",
      "Iteration 814, loss = 0.28364720\n",
      "Iteration 815, loss = 0.28133071\n",
      "Iteration 816, loss = 0.27904115\n",
      "Iteration 817, loss = 0.27677826\n",
      "Iteration 818, loss = 0.27454181\n",
      "Iteration 819, loss = 0.27233153\n",
      "Iteration 820, loss = 0.27014718\n",
      "Iteration 821, loss = 0.26798851\n",
      "Iteration 822, loss = 0.26585528\n",
      "Iteration 823, loss = 0.26374725\n",
      "Iteration 824, loss = 0.26166417\n",
      "Iteration 825, loss = 0.25960581\n",
      "Iteration 826, loss = 0.25757191\n",
      "Iteration 827, loss = 0.25556226\n",
      "Iteration 828, loss = 0.25357660\n",
      "Iteration 829, loss = 0.25161470\n",
      "Iteration 830, loss = 0.24967634\n",
      "Iteration 831, loss = 0.24776128\n",
      "Iteration 832, loss = 0.24586929\n",
      "Iteration 833, loss = 0.24400014\n",
      "Iteration 834, loss = 0.24215361\n",
      "Iteration 835, loss = 0.24032946\n",
      "Iteration 836, loss = 0.23852747\n",
      "Iteration 837, loss = 0.23674743\n",
      "Iteration 838, loss = 0.23498911\n",
      "Iteration 839, loss = 0.23325228\n",
      "Iteration 840, loss = 0.23153674\n",
      "Iteration 841, loss = 0.22984226\n",
      "Iteration 842, loss = 0.22816863\n",
      "Iteration 843, loss = 0.22651564\n",
      "Iteration 844, loss = 0.22488307\n",
      "Iteration 845, loss = 0.22327071\n",
      "Iteration 846, loss = 0.22167836\n",
      "Iteration 847, loss = 0.22010580\n",
      "Iteration 848, loss = 0.21855283\n",
      "Iteration 849, loss = 0.21701925\n",
      "Iteration 850, loss = 0.21550484\n",
      "Iteration 851, loss = 0.21400942\n",
      "Iteration 852, loss = 0.21253277\n",
      "Iteration 853, loss = 0.21107470\n",
      "Iteration 854, loss = 0.20963502\n",
      "Iteration 855, loss = 0.20821352\n",
      "Iteration 856, loss = 0.20681002\n",
      "Iteration 857, loss = 0.20542431\n",
      "Iteration 858, loss = 0.20405621\n",
      "Iteration 859, loss = 0.20270553\n",
      "Iteration 860, loss = 0.20137207\n",
      "Iteration 861, loss = 0.20005566\n",
      "Iteration 862, loss = 0.19875611\n",
      "Iteration 863, loss = 0.19747323\n",
      "Iteration 864, loss = 0.19620684\n",
      "Iteration 865, loss = 0.19495676\n",
      "Iteration 866, loss = 0.19372280\n",
      "Iteration 867, loss = 0.19250480\n",
      "Iteration 868, loss = 0.19130257\n",
      "Iteration 869, loss = 0.19011594\n",
      "Iteration 870, loss = 0.18894474\n",
      "Iteration 871, loss = 0.18778879\n",
      "Iteration 872, loss = 0.18664792\n",
      "Iteration 873, loss = 0.18552196\n",
      "Iteration 874, loss = 0.18441074\n",
      "Iteration 875, loss = 0.18331410\n",
      "Iteration 876, loss = 0.18223188\n",
      "Iteration 877, loss = 0.18116390\n",
      "Iteration 878, loss = 0.18011001\n",
      "Iteration 879, loss = 0.17907004\n",
      "Iteration 880, loss = 0.17804383\n",
      "Iteration 881, loss = 0.17703123\n",
      "Iteration 882, loss = 0.17603208\n",
      "Iteration 883, loss = 0.17504623\n",
      "Iteration 884, loss = 0.17407351\n",
      "Iteration 885, loss = 0.17311378\n",
      "Iteration 886, loss = 0.17216689\n",
      "Iteration 887, loss = 0.17123268\n",
      "Iteration 888, loss = 0.17031101\n",
      "Iteration 889, loss = 0.16940173\n",
      "Iteration 890, loss = 0.16850469\n",
      "Iteration 891, loss = 0.16761975\n",
      "Iteration 892, loss = 0.16674676\n",
      "Iteration 893, loss = 0.16588559\n",
      "Iteration 894, loss = 0.16503609\n",
      "Iteration 895, loss = 0.16419813\n",
      "Iteration 896, loss = 0.16337156\n",
      "Iteration 897, loss = 0.16255625\n",
      "Iteration 898, loss = 0.16175206\n",
      "Iteration 899, loss = 0.16095886\n",
      "Iteration 900, loss = 0.16017651\n",
      "Iteration 901, loss = 0.15940489\n",
      "Iteration 902, loss = 0.15864387\n",
      "Iteration 903, loss = 0.15789331\n",
      "Iteration 904, loss = 0.15715308\n",
      "Iteration 905, loss = 0.15642307\n",
      "Iteration 906, loss = 0.15570314\n",
      "Iteration 907, loss = 0.15499317\n",
      "Iteration 908, loss = 0.15429304\n",
      "Iteration 909, loss = 0.15360263\n",
      "Iteration 910, loss = 0.15292181\n",
      "Iteration 911, loss = 0.15225047\n",
      "Iteration 912, loss = 0.15158849\n",
      "Iteration 913, loss = 0.15093575\n",
      "Iteration 914, loss = 0.15029214\n",
      "Iteration 915, loss = 0.14965754\n",
      "Iteration 916, loss = 0.14903183\n",
      "Iteration 917, loss = 0.14841491\n",
      "Iteration 918, loss = 0.14780667\n",
      "Iteration 919, loss = 0.14720699\n",
      "Iteration 920, loss = 0.14661577\n",
      "Iteration 921, loss = 0.14603290\n",
      "Iteration 922, loss = 0.14545827\n",
      "Iteration 923, loss = 0.14489177\n",
      "Iteration 924, loss = 0.14433331\n",
      "Iteration 925, loss = 0.14378278\n",
      "Iteration 926, loss = 0.14324008\n",
      "Iteration 927, loss = 0.14270510\n",
      "Iteration 928, loss = 0.14217775\n",
      "Iteration 929, loss = 0.14165792\n",
      "Iteration 930, loss = 0.14114553\n",
      "Iteration 931, loss = 0.14064047\n",
      "Iteration 932, loss = 0.14014264\n",
      "Iteration 933, loss = 0.13965196\n",
      "Iteration 934, loss = 0.13916833\n",
      "Iteration 935, loss = 0.13869166\n",
      "Iteration 936, loss = 0.13822185\n",
      "Iteration 937, loss = 0.13775881\n",
      "Iteration 938, loss = 0.13730246\n",
      "Iteration 939, loss = 0.13685270\n",
      "Iteration 940, loss = 0.13640946\n",
      "Iteration 941, loss = 0.13597264\n",
      "Iteration 942, loss = 0.13554215\n",
      "Iteration 943, loss = 0.13511792\n",
      "Iteration 944, loss = 0.13469985\n",
      "Iteration 945, loss = 0.13428787\n",
      "Iteration 946, loss = 0.13388189\n",
      "Iteration 947, loss = 0.13348184\n",
      "Iteration 948, loss = 0.13308762\n",
      "Iteration 949, loss = 0.13269918\n",
      "Iteration 950, loss = 0.13231641\n",
      "Iteration 951, loss = 0.13193926\n",
      "Iteration 952, loss = 0.13156764\n",
      "Iteration 953, loss = 0.13120147\n",
      "Iteration 954, loss = 0.13084069\n",
      "Iteration 955, loss = 0.13048521\n",
      "Iteration 956, loss = 0.13013497\n",
      "Iteration 957, loss = 0.12978989\n",
      "Iteration 958, loss = 0.12944991\n",
      "Iteration 959, loss = 0.12911494\n",
      "Iteration 960, loss = 0.12878493\n",
      "Iteration 961, loss = 0.12845981\n",
      "Iteration 962, loss = 0.12813949\n",
      "Iteration 963, loss = 0.12782393\n",
      "Iteration 964, loss = 0.12751305\n",
      "Iteration 965, loss = 0.12720679\n",
      "Iteration 966, loss = 0.12690508\n",
      "Iteration 967, loss = 0.12660786\n",
      "Iteration 968, loss = 0.12631506\n",
      "Iteration 969, loss = 0.12602663\n",
      "Iteration 970, loss = 0.12574250\n",
      "Iteration 971, loss = 0.12546261\n",
      "Iteration 972, loss = 0.12518691\n",
      "Iteration 973, loss = 0.12491532\n",
      "Iteration 974, loss = 0.12464780\n",
      "Iteration 975, loss = 0.12438428\n",
      "Iteration 976, loss = 0.12412471\n",
      "Iteration 977, loss = 0.12386903\n",
      "Iteration 978, loss = 0.12361719\n",
      "Iteration 979, loss = 0.12336913\n",
      "Iteration 980, loss = 0.12312480\n",
      "Iteration 981, loss = 0.12288414\n",
      "Iteration 982, loss = 0.12264710\n",
      "Iteration 983, loss = 0.12241362\n",
      "Iteration 984, loss = 0.12218367\n",
      "Iteration 985, loss = 0.12195718\n",
      "Iteration 986, loss = 0.12173410\n",
      "Iteration 987, loss = 0.12151439\n",
      "Iteration 988, loss = 0.12129799\n",
      "Iteration 989, loss = 0.12108486\n",
      "Iteration 990, loss = 0.12087495\n",
      "Iteration 991, loss = 0.12066822\n",
      "Iteration 992, loss = 0.12046461\n",
      "Iteration 993, loss = 0.12026408\n",
      "Iteration 994, loss = 0.12006658\n",
      "Iteration 995, loss = 0.11987208\n",
      "Iteration 996, loss = 0.11968052\n",
      "Iteration 997, loss = 0.11949186\n",
      "Iteration 998, loss = 0.11930606\n",
      "Iteration 999, loss = 0.11912308\n",
      "Iteration 1000, loss = 0.11894287\n",
      "Iteration 1001, loss = 0.11876540\n",
      "Iteration 1002, loss = 0.11859062\n",
      "Iteration 1003, loss = 0.11841849\n",
      "Iteration 1004, loss = 0.11824897\n",
      "Iteration 1005, loss = 0.11808202\n",
      "Iteration 1006, loss = 0.11791761\n",
      "Iteration 1007, loss = 0.11775570\n",
      "Iteration 1008, loss = 0.11759624\n",
      "Iteration 1009, loss = 0.11743921\n",
      "Iteration 1010, loss = 0.11728456\n",
      "Iteration 1011, loss = 0.11713226\n",
      "Iteration 1012, loss = 0.11698228\n",
      "Iteration 1013, loss = 0.11683457\n",
      "Iteration 1014, loss = 0.11668910\n",
      "Iteration 1015, loss = 0.11654584\n",
      "Iteration 1016, loss = 0.11640476\n",
      "Iteration 1017, loss = 0.11626582\n",
      "Iteration 1018, loss = 0.11612899\n",
      "Iteration 1019, loss = 0.11599423\n",
      "Iteration 1020, loss = 0.11586152\n",
      "Iteration 1021, loss = 0.11573083\n",
      "Iteration 1022, loss = 0.11560211\n",
      "Iteration 1023, loss = 0.11547535\n",
      "Iteration 1024, loss = 0.11535051\n",
      "Iteration 1025, loss = 0.11522755\n",
      "Iteration 1026, loss = 0.11510647\n",
      "Iteration 1027, loss = 0.11498721\n",
      "Iteration 1028, loss = 0.11486976\n",
      "Iteration 1029, loss = 0.11475408\n",
      "Iteration 1030, loss = 0.11464016\n",
      "Iteration 1031, loss = 0.11452795\n",
      "Iteration 1032, loss = 0.11441744\n",
      "Iteration 1033, loss = 0.11430860\n",
      "Iteration 1034, loss = 0.11420140\n",
      "Iteration 1035, loss = 0.11409581\n",
      "Iteration 1036, loss = 0.11399182\n",
      "Iteration 1037, loss = 0.11388939\n",
      "Iteration 1038, loss = 0.11378850\n",
      "Iteration 1039, loss = 0.11368913\n",
      "Iteration 1040, loss = 0.11359125\n",
      "Iteration 1041, loss = 0.11349483\n",
      "Iteration 1042, loss = 0.11339986\n",
      "Iteration 1043, loss = 0.11330632\n",
      "Iteration 1044, loss = 0.11321417\n",
      "Iteration 1045, loss = 0.11312340\n",
      "Iteration 1046, loss = 0.11303398\n",
      "Iteration 1047, loss = 0.11294590\n",
      "Iteration 1048, loss = 0.11285913\n",
      "Iteration 1049, loss = 0.11277365\n",
      "Iteration 1050, loss = 0.11268944\n",
      "Iteration 1051, loss = 0.11260648\n",
      "Iteration 1052, loss = 0.11252474\n",
      "Iteration 1053, loss = 0.11244422\n",
      "Iteration 1054, loss = 0.11236489\n",
      "Iteration 1055, loss = 0.11228672\n",
      "Iteration 1056, loss = 0.11220971\n",
      "Iteration 1057, loss = 0.11213383\n",
      "Iteration 1058, loss = 0.11205907\n",
      "Iteration 1059, loss = 0.11198540\n",
      "Iteration 1060, loss = 0.11191281\n",
      "Iteration 1061, loss = 0.11184128\n",
      "Iteration 1062, loss = 0.11177080\n",
      "Iteration 1063, loss = 0.11170134\n",
      "Iteration 1064, loss = 0.11163289\n",
      "Iteration 1065, loss = 0.11156543\n",
      "Iteration 1066, loss = 0.11149895\n",
      "Iteration 1067, loss = 0.11143343\n",
      "Iteration 1068, loss = 0.11136885\n",
      "Iteration 1069, loss = 0.11130521\n",
      "Iteration 1070, loss = 0.11124247\n",
      "Iteration 1071, loss = 0.11118064\n",
      "Iteration 1072, loss = 0.11111969\n",
      "Iteration 1073, loss = 0.11105960\n",
      "Iteration 1074, loss = 0.11100037\n",
      "Iteration 1075, loss = 0.11094199\n",
      "Iteration 1076, loss = 0.11088442\n",
      "Iteration 1077, loss = 0.11082767\n",
      "Iteration 1078, loss = 0.11077172\n",
      "Iteration 1079, loss = 0.11071656\n",
      "Iteration 1080, loss = 0.11066216\n",
      "Iteration 1081, loss = 0.11060853\n",
      "Iteration 1082, loss = 0.11055564\n",
      "Iteration 1083, loss = 0.11050348\n",
      "Iteration 1084, loss = 0.11045205\n",
      "Iteration 1085, loss = 0.11040133\n",
      "Iteration 1086, loss = 0.11035130\n",
      "Iteration 1087, loss = 0.11030196\n",
      "Iteration 1088, loss = 0.11025329\n",
      "Iteration 1089, loss = 0.11020529\n",
      "Iteration 1090, loss = 0.11015793\n",
      "Iteration 1091, loss = 0.11011122\n",
      "Iteration 1092, loss = 0.11006514\n",
      "Iteration 1093, loss = 0.11001967\n",
      "Iteration 1094, loss = 0.10997481\n",
      "Iteration 1095, loss = 0.10993055\n",
      "Iteration 1096, loss = 0.10988688\n",
      "Iteration 1097, loss = 0.10984379\n",
      "Iteration 1098, loss = 0.10980126\n",
      "Iteration 1099, loss = 0.10975929\n",
      "Iteration 1100, loss = 0.10971787\n",
      "Iteration 1101, loss = 0.10967698\n",
      "Iteration 1102, loss = 0.10963663\n",
      "Iteration 1103, loss = 0.10959679\n",
      "Iteration 1104, loss = 0.10955747\n",
      "Iteration 1105, loss = 0.10951865\n",
      "Iteration 1106, loss = 0.10948032\n",
      "Iteration 1107, loss = 0.10944248\n",
      "Iteration 1108, loss = 0.10940512\n",
      "Iteration 1109, loss = 0.10936822\n",
      "Iteration 1110, loss = 0.10933178\n",
      "Iteration 1111, loss = 0.10929580\n",
      "Iteration 1112, loss = 0.10926026\n",
      "Iteration 1113, loss = 0.10922515\n",
      "Iteration 1114, loss = 0.10919048\n",
      "Iteration 1115, loss = 0.10915622\n",
      "Iteration 1116, loss = 0.10912238\n",
      "Iteration 1117, loss = 0.10908895\n",
      "Iteration 1118, loss = 0.10905592\n",
      "Iteration 1119, loss = 0.10902328\n",
      "Iteration 1120, loss = 0.10899102\n",
      "Iteration 1121, loss = 0.10895914\n",
      "Iteration 1122, loss = 0.10892764\n",
      "Iteration 1123, loss = 0.10889650\n",
      "Iteration 1124, loss = 0.10886572\n",
      "Iteration 1125, loss = 0.10883529\n",
      "Iteration 1126, loss = 0.10880521\n",
      "Iteration 1127, loss = 0.10877547\n",
      "Iteration 1128, loss = 0.10874607\n",
      "Iteration 1129, loss = 0.10871699\n",
      "Iteration 1130, loss = 0.10868823\n",
      "Iteration 1131, loss = 0.10865979\n",
      "Iteration 1132, loss = 0.10863167\n",
      "Iteration 1133, loss = 0.10860384\n",
      "Iteration 1134, loss = 0.10857632\n",
      "Iteration 1135, loss = 0.10854909\n",
      "Iteration 1136, loss = 0.10852216\n",
      "Iteration 1137, loss = 0.10849550\n",
      "Iteration 1138, loss = 0.10846913\n",
      "Iteration 1139, loss = 0.10844302\n",
      "Iteration 1140, loss = 0.10841719\n",
      "Iteration 1141, loss = 0.10839162\n",
      "Iteration 1142, loss = 0.10836631\n",
      "Iteration 1143, loss = 0.10834126\n",
      "Iteration 1144, loss = 0.10831646\n",
      "Iteration 1145, loss = 0.10829190\n",
      "Iteration 1146, loss = 0.10826758\n",
      "Iteration 1147, loss = 0.10824350\n",
      "Iteration 1148, loss = 0.10821965\n",
      "Iteration 1149, loss = 0.10819603\n",
      "Iteration 1150, loss = 0.10817264\n",
      "Iteration 1151, loss = 0.10814946\n",
      "Iteration 1152, loss = 0.10812650\n",
      "Iteration 1153, loss = 0.10810375\n",
      "Iteration 1154, loss = 0.10808122\n",
      "Iteration 1155, loss = 0.10805888\n",
      "Iteration 1156, loss = 0.10803675\n",
      "Iteration 1157, loss = 0.10801481\n",
      "Iteration 1158, loss = 0.10799306\n",
      "Iteration 1159, loss = 0.10797151\n",
      "Iteration 1160, loss = 0.10795014\n",
      "Iteration 1161, loss = 0.10792895\n",
      "Iteration 1162, loss = 0.10790795\n",
      "Iteration 1163, loss = 0.10788712\n",
      "Iteration 1164, loss = 0.10786646\n",
      "Iteration 1165, loss = 0.10784597\n",
      "Iteration 1166, loss = 0.10782565\n",
      "Iteration 1167, loss = 0.10780549\n",
      "Iteration 1168, loss = 0.10778549\n",
      "Iteration 1169, loss = 0.10776565\n",
      "Iteration 1170, loss = 0.10774597\n",
      "Iteration 1171, loss = 0.10772643\n",
      "Iteration 1172, loss = 0.10770705\n",
      "Iteration 1173, loss = 0.10768781\n",
      "Iteration 1174, loss = 0.10766871\n",
      "Iteration 1175, loss = 0.10764976\n",
      "Iteration 1176, loss = 0.10763094\n",
      "Iteration 1177, loss = 0.10761226\n",
      "Iteration 1178, loss = 0.10759371\n",
      "Iteration 1179, loss = 0.10757529\n",
      "Iteration 1180, loss = 0.10755700\n",
      "Iteration 1181, loss = 0.10753883\n",
      "Iteration 1182, loss = 0.10752079\n",
      "Iteration 1183, loss = 0.10750287\n",
      "Iteration 1184, loss = 0.10748507\n",
      "Iteration 1185, loss = 0.10746738\n",
      "Iteration 1186, loss = 0.10744981\n",
      "Iteration 1187, loss = 0.10743235\n",
      "Iteration 1188, loss = 0.10741500\n",
      "Iteration 1189, loss = 0.10739775\n",
      "Iteration 1190, loss = 0.10738062\n",
      "Iteration 1191, loss = 0.10736358\n",
      "Iteration 1192, loss = 0.10734665\n",
      "Iteration 1193, loss = 0.10732982\n",
      "Iteration 1194, loss = 0.10731309\n",
      "Iteration 1195, loss = 0.10729645\n",
      "Iteration 1196, loss = 0.10727990\n",
      "Iteration 1197, loss = 0.10726345\n",
      "Iteration 1198, loss = 0.10724709\n",
      "Iteration 1199, loss = 0.10723082\n",
      "Iteration 1200, loss = 0.10721463\n",
      "Iteration 1201, loss = 0.10719854\n",
      "Iteration 1202, loss = 0.10718252\n",
      "Iteration 1203, loss = 0.10716659\n",
      "Iteration 1204, loss = 0.10715074\n",
      "Iteration 1205, loss = 0.10713496\n",
      "Iteration 1206, loss = 0.10711927\n",
      "Iteration 1207, loss = 0.10710365\n",
      "Iteration 1208, loss = 0.10708810\n",
      "Iteration 1209, loss = 0.10707263\n",
      "Iteration 1210, loss = 0.10705723\n",
      "Iteration 1211, loss = 0.10704190\n",
      "Iteration 1212, loss = 0.10702665\n",
      "Iteration 1213, loss = 0.10701145\n",
      "Iteration 1214, loss = 0.10699633\n",
      "Iteration 1215, loss = 0.10698127\n",
      "Iteration 1216, loss = 0.10696627\n",
      "Iteration 1217, loss = 0.10695134\n",
      "Iteration 1218, loss = 0.10693647\n",
      "Iteration 1219, loss = 0.10692165\n",
      "Iteration 1220, loss = 0.10690690\n",
      "Iteration 1221, loss = 0.10689221\n",
      "Iteration 1222, loss = 0.10687757\n",
      "Iteration 1223, loss = 0.10686299\n",
      "Iteration 1224, loss = 0.10684846\n",
      "Iteration 1225, loss = 0.10683399\n",
      "Iteration 1226, loss = 0.10681956\n",
      "Iteration 1227, loss = 0.10680519\n",
      "Iteration 1228, loss = 0.10679087\n",
      "Iteration 1229, loss = 0.10677661\n",
      "Iteration 1230, loss = 0.10676238\n",
      "Iteration 1231, loss = 0.10674821\n",
      "Iteration 1232, loss = 0.10673408\n",
      "Iteration 1233, loss = 0.10672000\n",
      "Iteration 1234, loss = 0.10670597\n",
      "Iteration 1235, loss = 0.10669197\n",
      "Iteration 1236, loss = 0.10667802\n",
      "Iteration 1237, loss = 0.10666412\n",
      "Iteration 1238, loss = 0.10665025\n",
      "Iteration 1239, loss = 0.10663643\n",
      "Iteration 1240, loss = 0.10662264\n",
      "Iteration 1241, loss = 0.10660890\n",
      "Iteration 1242, loss = 0.10659519\n",
      "Iteration 1243, loss = 0.10658152\n",
      "Iteration 1244, loss = 0.10656789\n",
      "Iteration 1245, loss = 0.10655429\n",
      "Iteration 1246, loss = 0.10654073\n",
      "Iteration 1247, loss = 0.10652720\n",
      "Iteration 1248, loss = 0.10651371\n",
      "Iteration 1249, loss = 0.10650025\n",
      "Iteration 1250, loss = 0.10648683\n",
      "Iteration 1251, loss = 0.10647343\n",
      "Iteration 1252, loss = 0.10646007\n",
      "Iteration 1253, loss = 0.10644674\n",
      "Iteration 1254, loss = 0.10643343\n",
      "Iteration 1255, loss = 0.10642016\n",
      "Iteration 1256, loss = 0.10640692\n",
      "Iteration 1257, loss = 0.10639370\n",
      "Iteration 1258, loss = 0.10638052\n",
      "Iteration 1259, loss = 0.10636736\n",
      "Iteration 1260, loss = 0.10635422\n",
      "Iteration 1261, loss = 0.10634112\n",
      "Iteration 1262, loss = 0.10632804\n",
      "Iteration 1263, loss = 0.10631498\n",
      "Iteration 1264, loss = 0.10630195\n",
      "Iteration 1265, loss = 0.10628894\n",
      "Iteration 1266, loss = 0.10627596\n",
      "Iteration 1267, loss = 0.10626300\n",
      "Iteration 1268, loss = 0.10625007\n",
      "Iteration 1269, loss = 0.10623715\n",
      "Iteration 1270, loss = 0.10622426\n",
      "Iteration 1271, loss = 0.10621139\n",
      "Iteration 1272, loss = 0.10619854\n",
      "Iteration 1273, loss = 0.10618571\n",
      "Iteration 1274, loss = 0.10617290\n",
      "Iteration 1275, loss = 0.10616012\n",
      "Iteration 1276, loss = 0.10614735\n",
      "Iteration 1277, loss = 0.10613460\n",
      "Iteration 1278, loss = 0.10612187\n",
      "Iteration 1279, loss = 0.10610916\n",
      "Iteration 1280, loss = 0.10609646\n",
      "Iteration 1281, loss = 0.10608379\n",
      "Iteration 1282, loss = 0.10607113\n",
      "Iteration 1283, loss = 0.10605848\n",
      "Iteration 1284, loss = 0.10604586\n",
      "Iteration 1285, loss = 0.10603325\n",
      "Iteration 1286, loss = 0.10602066\n",
      "Iteration 1287, loss = 0.10600808\n",
      "Iteration 1288, loss = 0.10599552\n",
      "Iteration 1289, loss = 0.10598297\n",
      "Iteration 1290, loss = 0.10597044\n",
      "Iteration 1291, loss = 0.10595792\n",
      "Iteration 1292, loss = 0.10594542\n",
      "Iteration 1293, loss = 0.10593293\n",
      "Iteration 1294, loss = 0.10592046\n",
      "Iteration 1295, loss = 0.10590800\n",
      "Iteration 1296, loss = 0.10589555\n",
      "Iteration 1297, loss = 0.10588311\n",
      "Iteration 1298, loss = 0.10587069\n",
      "Iteration 1299, loss = 0.10585828\n",
      "Iteration 1300, loss = 0.10584588\n",
      "Iteration 1301, loss = 0.10583350\n",
      "Iteration 1302, loss = 0.10582113\n",
      "Iteration 1303, loss = 0.10580876\n",
      "Iteration 1304, loss = 0.10579641\n",
      "Iteration 1305, loss = 0.10578407\n",
      "Iteration 1306, loss = 0.10577174\n",
      "Iteration 1307, loss = 0.10575943\n",
      "Iteration 1308, loss = 0.10574712\n",
      "Iteration 1309, loss = 0.10573482\n",
      "Iteration 1310, loss = 0.10572254\n",
      "Iteration 1311, loss = 0.10571026\n",
      "Iteration 1312, loss = 0.10569799\n",
      "Iteration 1313, loss = 0.10568573\n",
      "Iteration 1314, loss = 0.10567349\n",
      "Iteration 1315, loss = 0.10566125\n",
      "Iteration 1316, loss = 0.10564902\n",
      "Iteration 1317, loss = 0.10563680\n",
      "Iteration 1318, loss = 0.10562458\n",
      "Iteration 1319, loss = 0.10561238\n",
      "Iteration 1320, loss = 0.10560019\n",
      "Iteration 1321, loss = 0.10558800\n",
      "Iteration 1322, loss = 0.10557582\n",
      "Iteration 1323, loss = 0.10556365\n",
      "Iteration 1324, loss = 0.10555149\n",
      "Iteration 1325, loss = 0.10553933\n",
      "Iteration 1326, loss = 0.10552719\n",
      "Iteration 1327, loss = 0.10551505\n",
      "Iteration 1328, loss = 0.10550292\n",
      "Iteration 1329, loss = 0.10549079\n",
      "Iteration 1330, loss = 0.10547867\n",
      "Iteration 1331, loss = 0.10546656\n",
      "Iteration 1332, loss = 0.10545446\n",
      "Iteration 1333, loss = 0.10544236\n",
      "Iteration 1334, loss = 0.10543027\n",
      "Iteration 1335, loss = 0.10541819\n",
      "Iteration 1336, loss = 0.10540611\n",
      "Iteration 1337, loss = 0.10539404\n",
      "Iteration 1338, loss = 0.10538197\n",
      "Iteration 1339, loss = 0.10536991\n",
      "Iteration 1340, loss = 0.10535786\n",
      "Iteration 1341, loss = 0.10534581\n",
      "Iteration 1342, loss = 0.10533377\n",
      "Iteration 1343, loss = 0.10532174\n",
      "Iteration 1344, loss = 0.10530971\n",
      "Iteration 1345, loss = 0.10529768\n",
      "Iteration 1346, loss = 0.10528567\n",
      "Iteration 1347, loss = 0.10527365\n",
      "Iteration 1348, loss = 0.10526164\n",
      "Iteration 1349, loss = 0.10524964\n",
      "Iteration 1350, loss = 0.10523764\n",
      "Iteration 1351, loss = 0.10522565\n",
      "Iteration 1352, loss = 0.10521366\n",
      "Iteration 1353, loss = 0.10520168\n",
      "Iteration 1354, loss = 0.10518970\n",
      "Iteration 1355, loss = 0.10517773\n",
      "Iteration 1356, loss = 0.10516576\n",
      "Iteration 1357, loss = 0.10515380\n",
      "Iteration 1358, loss = 0.10514184\n",
      "Iteration 1359, loss = 0.10512989\n",
      "Iteration 1360, loss = 0.10511794\n",
      "Iteration 1361, loss = 0.10510599\n",
      "Iteration 1362, loss = 0.10509405\n",
      "Iteration 1363, loss = 0.10508211\n",
      "Iteration 1364, loss = 0.10507018\n",
      "Iteration 1365, loss = 0.10505825\n",
      "Iteration 1366, loss = 0.10504633\n",
      "Iteration 1367, loss = 0.10503441\n",
      "Iteration 1368, loss = 0.10502249\n",
      "Iteration 1369, loss = 0.10501058\n",
      "Iteration 1370, loss = 0.10499867\n",
      "Iteration 1371, loss = 0.10498677\n",
      "Iteration 1372, loss = 0.10497487\n",
      "Iteration 1373, loss = 0.10496297\n",
      "Iteration 1374, loss = 0.10495108\n",
      "Iteration 1375, loss = 0.10493919\n",
      "Iteration 1376, loss = 0.10492731\n",
      "Iteration 1377, loss = 0.10491543\n",
      "Iteration 1378, loss = 0.10490355\n",
      "Iteration 1379, loss = 0.10489167\n",
      "Iteration 1380, loss = 0.10487980\n",
      "Iteration 1381, loss = 0.10486794\n",
      "Iteration 1382, loss = 0.10485607\n",
      "Iteration 1383, loss = 0.10484421\n",
      "Iteration 1384, loss = 0.10483236\n",
      "Iteration 1385, loss = 0.10482050\n",
      "Iteration 1386, loss = 0.10480865\n",
      "Iteration 1387, loss = 0.10479681\n",
      "Iteration 1388, loss = 0.10478496\n",
      "Iteration 1389, loss = 0.10477312\n",
      "Iteration 1390, loss = 0.10476129\n",
      "Iteration 1391, loss = 0.10474945\n",
      "Iteration 1392, loss = 0.10473762\n",
      "Iteration 1393, loss = 0.10472579\n",
      "Iteration 1394, loss = 0.10471397\n",
      "Iteration 1395, loss = 0.10470215\n",
      "Iteration 1396, loss = 0.10469033\n",
      "Iteration 1397, loss = 0.10467852\n",
      "Iteration 1398, loss = 0.10466670\n",
      "Iteration 1399, loss = 0.10465490\n",
      "Iteration 1400, loss = 0.10464309\n",
      "Iteration 1401, loss = 0.10463129\n",
      "Iteration 1402, loss = 0.10461949\n",
      "Iteration 1403, loss = 0.10460769\n",
      "Iteration 1404, loss = 0.10459590\n",
      "Iteration 1405, loss = 0.10458411\n",
      "Iteration 1406, loss = 0.10457232\n",
      "Iteration 1407, loss = 0.10456053\n",
      "Iteration 1408, loss = 0.10454875\n",
      "Iteration 1409, loss = 0.10453697\n",
      "Iteration 1410, loss = 0.10452519\n",
      "Iteration 1411, loss = 0.10451342\n",
      "Iteration 1412, loss = 0.10450165\n",
      "Iteration 1413, loss = 0.10448988\n",
      "Iteration 1414, loss = 0.10447811\n",
      "Iteration 1415, loss = 0.10446635\n",
      "Iteration 1416, loss = 0.10445459\n",
      "Iteration 1417, loss = 0.10444284\n",
      "Iteration 1418, loss = 0.10443108\n",
      "Iteration 1419, loss = 0.10441933\n",
      "Iteration 1420, loss = 0.10440758\n",
      "Iteration 1421, loss = 0.10439583\n",
      "Iteration 1422, loss = 0.10438409\n",
      "Iteration 1423, loss = 0.10437235\n",
      "Iteration 1424, loss = 0.10436061\n",
      "Iteration 1425, loss = 0.10434888\n",
      "Iteration 1426, loss = 0.10433714\n",
      "Iteration 1427, loss = 0.10432541\n",
      "Iteration 1428, loss = 0.10431369\n",
      "Iteration 1429, loss = 0.10430196\n",
      "Iteration 1430, loss = 0.10429024\n",
      "Iteration 1431, loss = 0.10427852\n",
      "Iteration 1432, loss = 0.10426680\n",
      "Iteration 1433, loss = 0.10425509\n",
      "Iteration 1434, loss = 0.10424338\n",
      "Iteration 1435, loss = 0.10423167\n",
      "Iteration 1436, loss = 0.10421996\n",
      "Iteration 1437, loss = 0.10420826\n",
      "Iteration 1438, loss = 0.10419656\n",
      "Iteration 1439, loss = 0.10418486\n",
      "Iteration 1440, loss = 0.10417316\n",
      "Iteration 1441, loss = 0.10416147\n",
      "Iteration 1442, loss = 0.10414978\n",
      "Iteration 1443, loss = 0.10413809\n",
      "Iteration 1444, loss = 0.10412640\n",
      "Iteration 1445, loss = 0.10411472\n",
      "Iteration 1446, loss = 0.10410304\n",
      "Iteration 1447, loss = 0.10409136\n",
      "Iteration 1448, loss = 0.10407969\n",
      "Iteration 1449, loss = 0.10406801\n",
      "Iteration 1450, loss = 0.10405634\n",
      "Iteration 1451, loss = 0.10404468\n",
      "Iteration 1452, loss = 0.10403301\n",
      "Iteration 1453, loss = 0.10402135\n",
      "Iteration 1454, loss = 0.10400969\n",
      "Iteration 1455, loss = 0.10399803\n",
      "Iteration 1456, loss = 0.10398638\n",
      "Iteration 1457, loss = 0.10397473\n",
      "Iteration 1458, loss = 0.10396308\n",
      "Iteration 1459, loss = 0.10395143\n",
      "Iteration 1460, loss = 0.10393979\n",
      "Iteration 1461, loss = 0.10392814\n",
      "Iteration 1462, loss = 0.10391650\n",
      "Iteration 1463, loss = 0.10390487\n",
      "Iteration 1464, loss = 0.10389323\n",
      "Iteration 1465, loss = 0.10388160\n",
      "Iteration 1466, loss = 0.10386997\n",
      "Iteration 1467, loss = 0.10385835\n",
      "Iteration 1468, loss = 0.10384672\n",
      "Iteration 1469, loss = 0.10383510\n",
      "Iteration 1470, loss = 0.10382349\n",
      "Iteration 1471, loss = 0.10381187\n",
      "Iteration 1472, loss = 0.10380026\n",
      "Iteration 1473, loss = 0.10378865\n",
      "Iteration 1474, loss = 0.10377704\n",
      "Iteration 1475, loss = 0.10376543\n",
      "Iteration 1476, loss = 0.10375383\n",
      "Iteration 1477, loss = 0.10374223\n",
      "Iteration 1478, loss = 0.10373063\n",
      "Iteration 1479, loss = 0.10371904\n",
      "Iteration 1480, loss = 0.10370745\n",
      "Iteration 1481, loss = 0.10369586\n",
      "Iteration 1482, loss = 0.10368427\n",
      "Iteration 1483, loss = 0.10367269\n",
      "Iteration 1484, loss = 0.10366111\n",
      "Iteration 1485, loss = 0.10364953\n",
      "Iteration 1486, loss = 0.10363795\n",
      "Iteration 1487, loss = 0.10362638\n",
      "Iteration 1488, loss = 0.10361481\n",
      "Iteration 1489, loss = 0.10360324\n",
      "Iteration 1490, loss = 0.10359167\n",
      "Iteration 1491, loss = 0.10358011\n",
      "Iteration 1492, loss = 0.10356855\n",
      "Iteration 1493, loss = 0.10355699\n",
      "Iteration 1494, loss = 0.10354544\n",
      "Iteration 1495, loss = 0.10353389\n",
      "Iteration 1496, loss = 0.10352234\n",
      "Iteration 1497, loss = 0.10351079\n",
      "Iteration 1498, loss = 0.10349925\n",
      "Iteration 1499, loss = 0.10348771\n",
      "Iteration 1500, loss = 0.10347617\n",
      "Iteration 1501, loss = 0.10346463\n",
      "Iteration 1502, loss = 0.10345310\n",
      "Iteration 1503, loss = 0.10344157\n",
      "Iteration 1504, loss = 0.10343004\n",
      "Iteration 1505, loss = 0.10341852\n",
      "Iteration 1506, loss = 0.10340700\n",
      "Iteration 1507, loss = 0.10339548\n",
      "Iteration 1508, loss = 0.10338396\n",
      "Iteration 1509, loss = 0.10337245\n",
      "Iteration 1510, loss = 0.10336094\n",
      "Iteration 1511, loss = 0.10334943\n",
      "Iteration 1512, loss = 0.10333792\n",
      "Iteration 1513, loss = 0.10332642\n",
      "Iteration 1514, loss = 0.10331492\n",
      "Iteration 1515, loss = 0.10330342\n",
      "Iteration 1516, loss = 0.10329193\n",
      "Iteration 1517, loss = 0.10328044\n",
      "Iteration 1518, loss = 0.10326895\n",
      "Iteration 1519, loss = 0.10325747\n",
      "Iteration 1520, loss = 0.10324598\n",
      "Iteration 1521, loss = 0.10323450\n",
      "Iteration 1522, loss = 0.10322303\n",
      "Iteration 1523, loss = 0.10321155\n",
      "Iteration 1524, loss = 0.10320008\n",
      "Iteration 1525, loss = 0.10318861\n",
      "Iteration 1526, loss = 0.10317715\n",
      "Iteration 1527, loss = 0.10316569\n",
      "Iteration 1528, loss = 0.10315423\n",
      "Iteration 1529, loss = 0.10314277\n",
      "Iteration 1530, loss = 0.10313132\n",
      "Iteration 1531, loss = 0.10311987\n",
      "Iteration 1532, loss = 0.10310842\n",
      "Iteration 1533, loss = 0.10309697\n",
      "Iteration 1534, loss = 0.10308553\n",
      "Iteration 1535, loss = 0.10307409\n",
      "Iteration 1536, loss = 0.10306266\n",
      "Iteration 1537, loss = 0.10305122\n",
      "Iteration 1538, loss = 0.10303979\n",
      "Iteration 1539, loss = 0.10302837\n",
      "Iteration 1540, loss = 0.10301694\n",
      "Iteration 1541, loss = 0.10300552\n",
      "Iteration 1542, loss = 0.10299410\n",
      "Iteration 1543, loss = 0.10298269\n",
      "Iteration 1544, loss = 0.10297128\n",
      "Iteration 1545, loss = 0.10295987\n",
      "Iteration 1546, loss = 0.10294846\n",
      "Iteration 1547, loss = 0.10293706\n",
      "Iteration 1548, loss = 0.10292566\n",
      "Iteration 1549, loss = 0.10291426\n",
      "Iteration 1550, loss = 0.10290287\n",
      "Iteration 1551, loss = 0.10289148\n",
      "Iteration 1552, loss = 0.10288009\n",
      "Iteration 1553, loss = 0.10286870\n",
      "Iteration 1554, loss = 0.10285732\n",
      "Iteration 1555, loss = 0.10284594\n",
      "Iteration 1556, loss = 0.10283457\n",
      "Iteration 1557, loss = 0.10282319\n",
      "Iteration 1558, loss = 0.10281183\n",
      "Iteration 1559, loss = 0.10280046\n",
      "Iteration 1560, loss = 0.10278910\n",
      "Iteration 1561, loss = 0.10277774\n",
      "Iteration 1562, loss = 0.10276638\n",
      "Iteration 1563, loss = 0.10275503\n",
      "Iteration 1564, loss = 0.10274367\n",
      "Iteration 1565, loss = 0.10273233\n",
      "Iteration 1566, loss = 0.10272098\n",
      "Iteration 1567, loss = 0.10270964\n",
      "Iteration 1568, loss = 0.10269830\n",
      "Iteration 1569, loss = 0.10268697\n",
      "Iteration 1570, loss = 0.10267564\n",
      "Iteration 1571, loss = 0.10266431\n",
      "Iteration 1572, loss = 0.10265298\n",
      "Iteration 1573, loss = 0.10264166\n",
      "Iteration 1574, loss = 0.10263034\n",
      "Iteration 1575, loss = 0.10261903\n",
      "Iteration 1576, loss = 0.10260772\n",
      "Iteration 1577, loss = 0.10259641\n",
      "Iteration 1578, loss = 0.10258510\n",
      "Iteration 1579, loss = 0.10257380\n",
      "Iteration 1580, loss = 0.10256250\n",
      "Iteration 1581, loss = 0.10255120\n",
      "Iteration 1582, loss = 0.10253991\n",
      "Iteration 1583, loss = 0.10252862\n",
      "Iteration 1584, loss = 0.10251734\n",
      "Iteration 1585, loss = 0.10250605\n",
      "Iteration 1586, loss = 0.10249477\n",
      "Iteration 1587, loss = 0.10248350\n",
      "Iteration 1588, loss = 0.10247222\n",
      "Iteration 1589, loss = 0.10246095\n",
      "Iteration 1590, loss = 0.10244969\n",
      "Iteration 1591, loss = 0.10243842\n",
      "Iteration 1592, loss = 0.10242717\n",
      "Iteration 1593, loss = 0.10241591\n",
      "Iteration 1594, loss = 0.10240466\n",
      "Iteration 1595, loss = 0.10239341\n",
      "Iteration 1596, loss = 0.10238216\n",
      "Iteration 1597, loss = 0.10237092\n",
      "Iteration 1598, loss = 0.10235968\n",
      "Iteration 1599, loss = 0.10234844\n",
      "Iteration 1600, loss = 0.10233721\n",
      "Iteration 1601, loss = 0.10232598\n",
      "Iteration 1602, loss = 0.10231476\n",
      "Iteration 1603, loss = 0.10230353\n",
      "Iteration 1604, loss = 0.10229231\n",
      "Iteration 1605, loss = 0.10228110\n",
      "Iteration 1606, loss = 0.10226989\n",
      "Iteration 1607, loss = 0.10225868\n",
      "Iteration 1608, loss = 0.10224747\n",
      "Iteration 1609, loss = 0.10223627\n",
      "Iteration 1610, loss = 0.10222507\n",
      "Iteration 1611, loss = 0.10221388\n",
      "Iteration 1612, loss = 0.10220269\n",
      "Iteration 1613, loss = 0.10219150\n",
      "Iteration 1614, loss = 0.10218032\n",
      "Iteration 1615, loss = 0.10216914\n",
      "Iteration 1616, loss = 0.10215796\n",
      "Iteration 1617, loss = 0.10214678\n",
      "Iteration 1618, loss = 0.10213561\n",
      "Iteration 1619, loss = 0.10212445\n",
      "Iteration 1620, loss = 0.10211329\n",
      "Iteration 1621, loss = 0.10210213\n",
      "Iteration 1622, loss = 0.10209097\n",
      "Iteration 1623, loss = 0.10207982\n",
      "Iteration 1624, loss = 0.10206867\n",
      "Iteration 1625, loss = 0.10205752\n",
      "Iteration 1626, loss = 0.10204638\n",
      "Iteration 1627, loss = 0.10203524\n",
      "Iteration 1628, loss = 0.10202411\n",
      "Iteration 1629, loss = 0.10201298\n",
      "Iteration 1630, loss = 0.10200185\n",
      "Iteration 1631, loss = 0.10199073\n",
      "Iteration 1632, loss = 0.10197961\n",
      "Iteration 1633, loss = 0.10196849\n",
      "Iteration 1634, loss = 0.10195738\n",
      "Iteration 1635, loss = 0.10194627\n",
      "Iteration 1636, loss = 0.10193517\n",
      "Iteration 1637, loss = 0.10192406\n",
      "Iteration 1638, loss = 0.10191297\n",
      "Iteration 1639, loss = 0.10190187\n",
      "Iteration 1640, loss = 0.10189078\n",
      "Iteration 1641, loss = 0.10187970\n",
      "Iteration 1642, loss = 0.10186861\n",
      "Iteration 1643, loss = 0.10185753\n",
      "Iteration 1644, loss = 0.10184646\n",
      "Iteration 1645, loss = 0.10183539\n",
      "Iteration 1646, loss = 0.10182432\n",
      "Iteration 1647, loss = 0.10181325\n",
      "Iteration 1648, loss = 0.10180219\n",
      "Iteration 1649, loss = 0.10179113\n",
      "Iteration 1650, loss = 0.10178008\n",
      "Iteration 1651, loss = 0.10176903\n",
      "Iteration 1652, loss = 0.10175799\n",
      "Iteration 1653, loss = 0.10174694\n",
      "Iteration 1654, loss = 0.10173591\n",
      "Iteration 1655, loss = 0.10172487\n",
      "Iteration 1656, loss = 0.10171384\n",
      "Iteration 1657, loss = 0.10170281\n",
      "Iteration 1658, loss = 0.10169179\n",
      "Iteration 1659, loss = 0.10168077\n",
      "Iteration 1660, loss = 0.10166976\n",
      "Iteration 1661, loss = 0.10165875\n",
      "Iteration 1662, loss = 0.10164774\n",
      "Iteration 1663, loss = 0.10163673\n",
      "Iteration 1664, loss = 0.10162573\n",
      "Iteration 1665, loss = 0.10161474\n",
      "Iteration 1666, loss = 0.10160375\n",
      "Iteration 1667, loss = 0.10159276\n",
      "Iteration 1668, loss = 0.10158177\n",
      "Iteration 1669, loss = 0.10157079\n",
      "Iteration 1670, loss = 0.10155982\n",
      "Iteration 1671, loss = 0.10154884\n",
      "Iteration 1672, loss = 0.10153787\n",
      "Iteration 1673, loss = 0.10152691\n",
      "Iteration 1674, loss = 0.10151595\n",
      "Iteration 1675, loss = 0.10150499\n",
      "Iteration 1676, loss = 0.10149404\n",
      "Iteration 1677, loss = 0.10148309\n",
      "Iteration 1678, loss = 0.10147214\n",
      "Iteration 1679, loss = 0.10146120\n",
      "Iteration 1680, loss = 0.10145027\n",
      "Iteration 1681, loss = 0.10143933\n",
      "Iteration 1682, loss = 0.10142840\n",
      "Iteration 1683, loss = 0.10141748\n",
      "Iteration 1684, loss = 0.10140656\n",
      "Iteration 1685, loss = 0.10139564\n",
      "Iteration 1686, loss = 0.10138472\n",
      "Iteration 1687, loss = 0.10137382\n",
      "Iteration 1688, loss = 0.10136291\n",
      "Iteration 1689, loss = 0.10135201\n",
      "Iteration 1690, loss = 0.10134111\n",
      "Iteration 1691, loss = 0.10133022\n",
      "Iteration 1692, loss = 0.10131933\n",
      "Iteration 1693, loss = 0.10130844\n",
      "Iteration 1694, loss = 0.10129756\n",
      "Iteration 1695, loss = 0.10128669\n",
      "Iteration 1696, loss = 0.10127581\n",
      "Iteration 1697, loss = 0.10126494\n",
      "Iteration 1698, loss = 0.10125408\n",
      "Iteration 1699, loss = 0.10124322\n",
      "Iteration 1700, loss = 0.10123236\n",
      "Iteration 1701, loss = 0.10122151\n",
      "Iteration 1702, loss = 0.10121066\n",
      "Iteration 1703, loss = 0.10119982\n",
      "Iteration 1704, loss = 0.10118898\n",
      "Iteration 1705, loss = 0.10117814\n",
      "Iteration 1706, loss = 0.10116731\n",
      "Iteration 1707, loss = 0.10115648\n",
      "Iteration 1708, loss = 0.10114566\n",
      "Iteration 1709, loss = 0.10113484\n",
      "Iteration 1710, loss = 0.10112402\n",
      "Iteration 1711, loss = 0.10111321\n",
      "Iteration 1712, loss = 0.10110240\n",
      "Iteration 1713, loss = 0.10109160\n",
      "Iteration 1714, loss = 0.10108080\n",
      "Iteration 1715, loss = 0.10107001\n",
      "Iteration 1716, loss = 0.10105922\n",
      "Iteration 1717, loss = 0.10104843\n",
      "Iteration 1718, loss = 0.10103765\n",
      "Iteration 1719, loss = 0.10102687\n",
      "Iteration 1720, loss = 0.10101610\n",
      "Iteration 1721, loss = 0.10100533\n",
      "Iteration 1722, loss = 0.10099456\n",
      "Iteration 1723, loss = 0.10098380\n",
      "Iteration 1724, loss = 0.10097305\n",
      "Iteration 1725, loss = 0.10096230\n",
      "Iteration 1726, loss = 0.10095155\n",
      "Iteration 1727, loss = 0.10094080\n",
      "Iteration 1728, loss = 0.10093006\n",
      "Iteration 1729, loss = 0.10091933\n",
      "Iteration 1730, loss = 0.10090860\n",
      "Iteration 1731, loss = 0.10089787\n",
      "Iteration 1732, loss = 0.10088715\n",
      "Iteration 1733, loss = 0.10087643\n",
      "Iteration 1734, loss = 0.10086572\n",
      "Iteration 1735, loss = 0.10085501\n",
      "Iteration 1736, loss = 0.10084431\n",
      "Iteration 1737, loss = 0.10083360\n",
      "Iteration 1738, loss = 0.10082291\n",
      "Iteration 1739, loss = 0.10081222\n",
      "Iteration 1740, loss = 0.10080153\n",
      "Iteration 1741, loss = 0.10079085\n",
      "Iteration 1742, loss = 0.10078017\n",
      "Iteration 1743, loss = 0.10076949\n",
      "Iteration 1744, loss = 0.10075882\n",
      "Iteration 1745, loss = 0.10074816\n",
      "Iteration 1746, loss = 0.10073750\n",
      "Iteration 1747, loss = 0.10072684\n",
      "Iteration 1748, loss = 0.10071619\n",
      "Iteration 1749, loss = 0.10070554\n",
      "Iteration 1750, loss = 0.10069490\n",
      "Iteration 1751, loss = 0.10068426\n",
      "Iteration 1752, loss = 0.10067362\n",
      "Iteration 1753, loss = 0.10066299\n",
      "Iteration 1754, loss = 0.10065237\n",
      "Iteration 1755, loss = 0.10064174\n",
      "Iteration 1756, loss = 0.10063113\n",
      "Iteration 1757, loss = 0.10062052\n",
      "Iteration 1758, loss = 0.10060991\n",
      "Iteration 1759, loss = 0.10059930\n",
      "Iteration 1760, loss = 0.10058870\n",
      "Iteration 1761, loss = 0.10057811\n",
      "Iteration 1762, loss = 0.10056752\n",
      "Iteration 1763, loss = 0.10055693\n",
      "Iteration 1764, loss = 0.10054635\n",
      "Iteration 1765, loss = 0.10053578\n",
      "Iteration 1766, loss = 0.10052520\n",
      "Iteration 1767, loss = 0.10051464\n",
      "Iteration 1768, loss = 0.10050407\n",
      "Iteration 1769, loss = 0.10049352\n",
      "Iteration 1770, loss = 0.10048296\n",
      "Iteration 1771, loss = 0.10047241\n",
      "Iteration 1772, loss = 0.10046187\n",
      "Iteration 1773, loss = 0.10045133\n",
      "Iteration 1774, loss = 0.10044079\n",
      "Iteration 1775, loss = 0.10043026\n",
      "Iteration 1776, loss = 0.10041973\n",
      "Iteration 1777, loss = 0.10040921\n",
      "Iteration 1778, loss = 0.10039869\n",
      "Iteration 1779, loss = 0.10038818\n",
      "Iteration 1780, loss = 0.10037767\n",
      "Iteration 1781, loss = 0.10036717\n",
      "Iteration 1782, loss = 0.10035667\n",
      "Iteration 1783, loss = 0.10034618\n",
      "Iteration 1784, loss = 0.10033569\n",
      "Iteration 1785, loss = 0.10032520\n",
      "Iteration 1786, loss = 0.10031472\n",
      "Iteration 1787, loss = 0.10030424\n",
      "Iteration 1788, loss = 0.10029377\n",
      "Iteration 1789, loss = 0.10028331\n",
      "Iteration 1790, loss = 0.10027285\n",
      "Iteration 1791, loss = 0.10026239\n",
      "Iteration 1792, loss = 0.10025194\n",
      "Iteration 1793, loss = 0.10024149\n",
      "Iteration 1794, loss = 0.10023104\n",
      "Iteration 1795, loss = 0.10022061\n",
      "Iteration 1796, loss = 0.10021017\n",
      "Iteration 1797, loss = 0.10019974\n",
      "Iteration 1798, loss = 0.10018932\n",
      "Iteration 1799, loss = 0.10017890\n",
      "Iteration 1800, loss = 0.10016848\n",
      "Iteration 1801, loss = 0.10015807\n",
      "Iteration 1802, loss = 0.10014767\n",
      "Iteration 1803, loss = 0.10013727\n",
      "Iteration 1804, loss = 0.10012687\n",
      "Iteration 1805, loss = 0.10011648\n",
      "Iteration 1806, loss = 0.10010609\n",
      "Iteration 1807, loss = 0.10009571\n",
      "Iteration 1808, loss = 0.10008533\n",
      "Iteration 1809, loss = 0.10007496\n",
      "Iteration 1810, loss = 0.10006459\n",
      "Iteration 1811, loss = 0.10005423\n",
      "Iteration 1812, loss = 0.10004387\n",
      "Iteration 1813, loss = 0.10003352\n",
      "Iteration 1814, loss = 0.10002317\n",
      "Iteration 1815, loss = 0.10001283\n",
      "Iteration 1816, loss = 0.10000249\n",
      "Iteration 1817, loss = 0.09999216\n",
      "Iteration 1818, loss = 0.09998183\n",
      "Iteration 1819, loss = 0.09997150\n",
      "Iteration 1820, loss = 0.09996118\n",
      "Iteration 1821, loss = 0.09995087\n",
      "Iteration 1822, loss = 0.09994056\n",
      "Iteration 1823, loss = 0.09993025\n",
      "Iteration 1824, loss = 0.09991995\n",
      "Iteration 1825, loss = 0.09990966\n",
      "Iteration 1826, loss = 0.09989937\n",
      "Iteration 1827, loss = 0.09988908\n",
      "Iteration 1828, loss = 0.09987880\n",
      "Iteration 1829, loss = 0.09986853\n",
      "Iteration 1830, loss = 0.09985826\n",
      "Iteration 1831, loss = 0.09984799\n",
      "Iteration 1832, loss = 0.09983773\n",
      "Iteration 1833, loss = 0.09982747\n",
      "Iteration 1834, loss = 0.09981722\n",
      "Iteration 1835, loss = 0.09980698\n",
      "Iteration 1836, loss = 0.09979673\n",
      "Iteration 1837, loss = 0.09978650\n",
      "Iteration 1838, loss = 0.09977627\n",
      "Iteration 1839, loss = 0.09976604\n",
      "Iteration 1840, loss = 0.09975582\n",
      "Iteration 1841, loss = 0.09974560\n",
      "Iteration 1842, loss = 0.09973539\n",
      "Iteration 1843, loss = 0.09972518\n",
      "Iteration 1844, loss = 0.09971498\n",
      "Iteration 1845, loss = 0.09970478\n",
      "Iteration 1846, loss = 0.09969459\n",
      "Iteration 1847, loss = 0.09968440\n",
      "Iteration 1848, loss = 0.09967422\n",
      "Iteration 1849, loss = 0.09966405\n",
      "Iteration 1850, loss = 0.09965387\n",
      "Iteration 1851, loss = 0.09964371\n",
      "Iteration 1852, loss = 0.09963354\n",
      "Iteration 1853, loss = 0.09962339\n",
      "Iteration 1854, loss = 0.09961324\n",
      "Iteration 1855, loss = 0.09960309\n",
      "Iteration 1856, loss = 0.09959295\n",
      "Iteration 1857, loss = 0.09958281\n",
      "Iteration 1858, loss = 0.09957268\n",
      "Iteration 1859, loss = 0.09956255\n",
      "Iteration 1860, loss = 0.09955243\n",
      "Iteration 1861, loss = 0.09954231\n",
      "Iteration 1862, loss = 0.09953220\n",
      "Iteration 1863, loss = 0.09952210\n",
      "Iteration 1864, loss = 0.09951199\n",
      "Iteration 1865, loss = 0.09950190\n",
      "Iteration 1866, loss = 0.09949181\n",
      "Iteration 1867, loss = 0.09948172\n",
      "Iteration 1868, loss = 0.09947164\n",
      "Iteration 1869, loss = 0.09946156\n",
      "Iteration 1870, loss = 0.09945149\n",
      "Iteration 1871, loss = 0.09944143\n",
      "Iteration 1872, loss = 0.09943136\n",
      "Iteration 1873, loss = 0.09942131\n",
      "Iteration 1874, loss = 0.09941126\n",
      "Iteration 1875, loss = 0.09940121\n",
      "Iteration 1876, loss = 0.09939117\n",
      "Iteration 1877, loss = 0.09938114\n",
      "Iteration 1878, loss = 0.09937111\n",
      "Iteration 1879, loss = 0.09936108\n",
      "Iteration 1880, loss = 0.09935106\n",
      "Iteration 1881, loss = 0.09934105\n",
      "Iteration 1882, loss = 0.09933104\n",
      "Iteration 1883, loss = 0.09932104\n",
      "Iteration 1884, loss = 0.09931104\n",
      "Iteration 1885, loss = 0.09930104\n",
      "Iteration 1886, loss = 0.09929105\n",
      "Iteration 1887, loss = 0.09928107\n",
      "Iteration 1888, loss = 0.09927109\n",
      "Iteration 1889, loss = 0.09926112\n",
      "Iteration 1890, loss = 0.09925115\n",
      "Iteration 1891, loss = 0.09924119\n",
      "Iteration 1892, loss = 0.09923123\n",
      "Iteration 1893, loss = 0.09922128\n",
      "Iteration 1894, loss = 0.09921133\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "mlp.score=0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "M_scaled = scaler.transform(M)\n",
    "\n",
    "mlp.fit(X_scaled, y)\n",
    "y_pred_mlp = mlp.predict(M_scaled)\n",
    "\n",
    "print(f\"mlp.score={mlp.score(X_scaled, y):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Outliers and the Min-max Scaler vs. the Standard Scaler\n",
    "\n",
    "Explain the fundamental problem with a min-max scaler and outliers. \n",
    "\n",
    "Will a `sklearn.preprocessing.StandardScaler` do better here, in the case of abnormal feature values/outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: research the problem here..\n",
    "assert False, \"TODO: investigate outlier problems and try a StandardScaler..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Modify the MLP Hyperparameters\n",
    "\n",
    "Finally, try out some of the hyperparameters associated with the MLP.\n",
    "\n",
    "Specifically, test how few neurons the MLP can do with---still producing a sensible output, i.e. high $R^2$. \n",
    "\n",
    "Also try-out some other activation functions, ala sigmoid, and solvers, like `sgd`.\n",
    "\n",
    "Notice, that the Scikit-learn MLP does not have as many adjustable parameters, as a Keras MLP, for example, the Scikit-learn MLP misses neurons initialization parameters (p. 333-334 [HOML]) and the ELU activation function (p. 336 [HOML]).\n",
    "\n",
    "OPTIONAL$_1$: use a Keras MLP regressor instead of the Scikit-learn MLP (You need to install the  Keras if its not installed as default).\n",
    "\n",
    "OPTIONAL$_2$: try out the `early_stopping` hyperparameter on the `MLPRegressor`. \n",
    "\n",
    "OPTIONAL$_3$: try putting all score-calculations into K-fold cross-validation  methods readily available in Scikit-learn using\n",
    "\n",
    "* `sklearn.model_selection.cross_val_predict`\n",
    "* `sklearn.model_selection.cross_val_score` \n",
    "\n",
    "or similar (this is, in theory, the correct method, but can be hard to use due to the  extremely small number of data points, `n=29`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add your code here..\n",
    "assert False, \"TODO: test out various hyperparameters for the MLP..\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2020-10-15| CEF, initial. \n",
    "2020-10-21| CEF, added Standard Scaler Q.\n",
    "2020-11-17| CEF, removed orhpant text in Qa (moded to Qc).\n",
    "2021-02-10| CEF, updated for ITMAL F21.\n",
    "2021-11-08| CEF, updated print info.\n",
    "2021-02.10| CEF, updated for SWMAL F22.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
