{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Pipelines\n",
    "\n",
    "We now try building af ML pipeline. The data for this exercise is the same as in L01, meaning that the OECD data from the 'intro.ipynb' have been save into a Python 'pickle' file. \n",
    "\n",
    "The pickle library is a nifty data preservation method in Python, and from L01 the tuple `(X, y)` have been stored to the pickle file `tmal_l01_data.pkl', try reloading it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=(29, 1),  y.shape=(29,)\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def LoadDataFromL01():\n",
    "    import pickle\n",
    "    filename = \"./itmal_l01_data.pkl\"\n",
    "    with open(f\"{filename}\", \"rb\") as f:\n",
    "        (X, y) = pickle.load(f)\n",
    "        return X, y\n",
    "\n",
    "X, y = LoadDataFromL01()\n",
    "\n",
    "print(f\"X.shape={X.shape},  y.shape={y.shape}\")\n",
    "\n",
    "assert X.shape[0] == y.shape[0]\n",
    "assert X.ndim == 2\n",
    "assert y.ndim == 1  # did a y.ravel() before saving to picke file\n",
    "assert X.shape[0] == 29\n",
    "\n",
    "# re-create plot data (not stored in the Pickel file)\n",
    "m = np.linspace(0, 60000, 1000)\n",
    "M = np.empty([m.shape[0], 1])\n",
    "M[:, 0] = m\n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Revisiting the problem with the MLP\n",
    "\n",
    "Using the MLP for the QECD data in Qd) from `intro.ipynb` produced a negative $R^2$, meaning that it was unable to fit the data, and the MPL model was actually _worse_ than the naive $\\hat y$ (mean value of y).\n",
    "\n",
    "Let's just revisit this fact. When running the next cell you should now see an OK $~R^2_{lin.reg}~$ score and a negative $~R^2_{mlp}~$ score.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\n",
      "\n",
      "lin.reg.score(X, y)=0.73\n",
      "MLP    .score(X, y)=-8.85\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setup the MLP and lin. regression again..\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def PlotModels(model1, model2, X, y, name_model1=\"lin.reg\", name_model2=\"MLP\"):\n",
    "\n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(model1, model2, X, y):\n",
    "        y_pred_model1 = model1.predict(X)\n",
    "        y_pred_model2 = model2.predict(X)\n",
    "\n",
    "        # call r2\n",
    "        score_model1 = r2_score(y, y_pred_model1)\n",
    "        score_model2 = r2_score(y, y_pred_model2)\n",
    "\n",
    "        return y_pred_model1, y_pred_model2, score_model1, score_model2\n",
    "    \n",
    "    def Fill(s, n):\n",
    "        while(len(s)<n):\n",
    "            s += \" \"\n",
    "        return s\n",
    "\n",
    "    y_pred_model1, y_pred_model2, score_model1, score_model2 = CalcPredAndScore(\n",
    "        model1, model2, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.plot(X, y_pred_model2, \"kx-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([name_model1, name_model2, \"X OECD data\"])\n",
    "\n",
    "    l = max(len(name_model1), len(name_model2))\n",
    "    \n",
    "    print(f\"{Fill(name_model1,l)}.score(X, y)={score_model1:0.2f}\")\n",
    "    print(f\"{Fill(name_model2,l)}.score(X, y)={score_model2:0.2f}\")\n",
    "\n",
    "\n",
    "# lets make a linear and MLP regressor and redo the plots\n",
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)\n",
    "linreg = LinearRegression()\n",
    "\n",
    "mlp.fit(X, y)\n",
    "linreg.fit(X, y)\n",
    "\n",
    "print(\"The MLP may mis-fit the data, seen in the, sometimes, bad R^2 score..\\n\")\n",
    "PlotModels(linreg, mlp, X, y)\n",
    "print(\"\\nOK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qa) Create a Min/max scaler for the MLP\n",
    "\n",
    "Now, the neurons in neural networks normally expect input data in the range `[0;1]` or sometimes in the range `[-1;1]`, meaning that for value outside this range then the neuron will saturate to its min or max value (also typical `0` or `1`). \n",
    "\n",
    "A concrete value of `X` is, say 22.000 USD, that is far away from what the MLP expects. Af fix to the problem in Qd), from `intro.ipynb`, is to preprocess data by scaling it down to something more sensible.\n",
    "\n",
    "Try to manually scale X to a range of `[0;1]`, re-train the MLP, re-plot and find the new score from the rescaled input. Any better?\n",
    "\n",
    "(If you already made exercise \"Qe) Neural Network with pre-scaling\" in L01, then reuse Your work here!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 18.28253901\n",
      "Iteration 2, loss = 18.26565519\n",
      "Iteration 3, loss = 18.24867758\n",
      "Iteration 4, loss = 18.23196407\n",
      "Iteration 5, loss = 18.21519929\n",
      "Iteration 6, loss = 18.19834801\n",
      "Iteration 7, loss = 18.18146241\n",
      "Iteration 8, loss = 18.16446101\n",
      "Iteration 9, loss = 18.14744640\n",
      "Iteration 10, loss = 18.13041941\n",
      "Iteration 11, loss = 18.11338060\n",
      "Iteration 12, loss = 18.09633038\n",
      "Iteration 13, loss = 18.07926690\n",
      "Iteration 14, loss = 18.06206856\n",
      "Iteration 15, loss = 18.04480362\n",
      "Iteration 16, loss = 18.02742877\n",
      "Iteration 17, loss = 18.01003203\n",
      "Iteration 18, loss = 17.99261494\n",
      "Iteration 19, loss = 17.97517874\n",
      "Iteration 20, loss = 17.95760769\n",
      "Iteration 21, loss = 17.93998657\n",
      "Iteration 22, loss = 17.92234063\n",
      "Iteration 23, loss = 17.90467142\n",
      "Iteration 24, loss = 17.88698018\n",
      "Iteration 25, loss = 17.86926789\n",
      "Iteration 26, loss = 17.85153536\n",
      "Iteration 27, loss = 17.83378321\n",
      "Iteration 28, loss = 17.81590961\n",
      "Iteration 29, loss = 17.79800074\n",
      "Iteration 30, loss = 17.78006634\n",
      "Iteration 31, loss = 17.76210748\n",
      "Iteration 32, loss = 17.74412504\n",
      "Iteration 33, loss = 17.72611972\n",
      "Iteration 34, loss = 17.70832432\n",
      "Iteration 35, loss = 17.69054229\n",
      "Iteration 36, loss = 17.67286828\n",
      "Iteration 37, loss = 17.65531073\n",
      "Iteration 38, loss = 17.63776985\n",
      "Iteration 39, loss = 17.62026952\n",
      "Iteration 40, loss = 17.60284774\n",
      "Iteration 41, loss = 17.58541745\n",
      "Iteration 42, loss = 17.56797643\n",
      "Iteration 43, loss = 17.55052136\n",
      "Iteration 44, loss = 17.53293408\n",
      "Iteration 45, loss = 17.51532246\n",
      "Iteration 46, loss = 17.49768583\n",
      "Iteration 47, loss = 17.48002358\n",
      "Iteration 48, loss = 17.46224477\n",
      "Iteration 49, loss = 17.44439259\n",
      "Iteration 50, loss = 17.42650350\n",
      "Iteration 51, loss = 17.40857809\n",
      "Iteration 52, loss = 17.39061684\n",
      "Iteration 53, loss = 17.37268473\n",
      "Iteration 54, loss = 17.35477302\n",
      "Iteration 55, loss = 17.33683277\n",
      "Iteration 56, loss = 17.31886338\n",
      "Iteration 57, loss = 17.30086427\n",
      "Iteration 58, loss = 17.28283491\n",
      "Iteration 59, loss = 17.26477478\n",
      "Iteration 60, loss = 17.24668342\n",
      "Iteration 61, loss = 17.22856037\n",
      "Iteration 62, loss = 17.21040523\n",
      "Iteration 63, loss = 17.19221761\n",
      "Iteration 64, loss = 17.17402045\n",
      "Iteration 65, loss = 17.15586701\n",
      "Iteration 66, loss = 17.13758971\n",
      "Iteration 67, loss = 17.11925938\n",
      "Iteration 68, loss = 17.10089047\n",
      "Iteration 69, loss = 17.08248312\n",
      "Iteration 70, loss = 17.06403740\n",
      "Iteration 71, loss = 17.04555339\n",
      "Iteration 72, loss = 17.02696083\n",
      "Iteration 73, loss = 17.00829345\n",
      "Iteration 74, loss = 16.98957986\n",
      "Iteration 75, loss = 16.97082091\n",
      "Iteration 76, loss = 16.95203425\n",
      "Iteration 77, loss = 16.93317301\n",
      "Iteration 78, loss = 16.91426576\n",
      "Iteration 79, loss = 16.89526689\n",
      "Iteration 80, loss = 16.87614889\n",
      "Iteration 81, loss = 16.85697744\n",
      "Iteration 82, loss = 16.83775406\n",
      "Iteration 83, loss = 16.81848008\n",
      "Iteration 84, loss = 16.79914364\n",
      "Iteration 85, loss = 16.77963709\n",
      "Iteration 86, loss = 16.76007260\n",
      "Iteration 87, loss = 16.74045216\n",
      "Iteration 88, loss = 16.72077750\n",
      "Iteration 89, loss = 16.70105017\n",
      "Iteration 90, loss = 16.68127155\n",
      "Iteration 91, loss = 16.66144285\n",
      "Iteration 92, loss = 16.64156515\n",
      "Iteration 93, loss = 16.62163940\n",
      "Iteration 94, loss = 16.60166646\n",
      "Iteration 95, loss = 16.58164707\n",
      "Iteration 96, loss = 16.56158191\n",
      "Iteration 97, loss = 16.54147158\n",
      "Iteration 98, loss = 16.52131660\n",
      "Iteration 99, loss = 16.50111744\n",
      "Iteration 100, loss = 16.48075782\n",
      "Iteration 101, loss = 16.46026182\n",
      "Iteration 102, loss = 16.43968460\n",
      "Iteration 103, loss = 16.41904737\n",
      "Iteration 104, loss = 16.39835229\n",
      "Iteration 105, loss = 16.37760125\n",
      "Iteration 106, loss = 16.35679598\n",
      "Iteration 107, loss = 16.33593799\n",
      "Iteration 108, loss = 16.31502864\n",
      "Iteration 109, loss = 16.29406915\n",
      "Iteration 110, loss = 16.27306060\n",
      "Iteration 111, loss = 16.25200397\n",
      "Iteration 112, loss = 16.23092119\n",
      "Iteration 113, loss = 16.20983434\n",
      "Iteration 114, loss = 16.18865433\n",
      "Iteration 115, loss = 16.16735740\n",
      "Iteration 116, loss = 16.14594304\n",
      "Iteration 117, loss = 16.12440691\n",
      "Iteration 118, loss = 16.10281078\n",
      "Iteration 119, loss = 16.08115676\n",
      "Iteration 120, loss = 16.05944673\n",
      "Iteration 121, loss = 16.03768239\n",
      "Iteration 122, loss = 16.01586522\n",
      "Iteration 123, loss = 15.99405048\n",
      "Iteration 124, loss = 15.97219625\n",
      "Iteration 125, loss = 15.95029658\n",
      "Iteration 126, loss = 15.92835196\n",
      "Iteration 127, loss = 15.90636280\n",
      "Iteration 128, loss = 15.88432950\n",
      "Iteration 129, loss = 15.86225241\n",
      "Iteration 130, loss = 15.84013184\n",
      "Iteration 131, loss = 15.81796808\n",
      "Iteration 132, loss = 15.79576138\n",
      "Iteration 133, loss = 15.77351199\n",
      "Iteration 134, loss = 15.75122011\n",
      "Iteration 135, loss = 15.72888597\n",
      "Iteration 136, loss = 15.70650973\n",
      "Iteration 137, loss = 15.68409158\n",
      "Iteration 138, loss = 15.66163167\n",
      "Iteration 139, loss = 15.63913018\n",
      "Iteration 140, loss = 15.61658725\n",
      "Iteration 141, loss = 15.59400301\n",
      "Iteration 142, loss = 15.57137762\n",
      "Iteration 143, loss = 15.54871120\n",
      "Iteration 144, loss = 15.52600389\n",
      "Iteration 145, loss = 15.50325581\n",
      "Iteration 146, loss = 15.48046709\n",
      "Iteration 147, loss = 15.45763787\n",
      "Iteration 148, loss = 15.43476825\n",
      "Iteration 149, loss = 15.41185837\n",
      "Iteration 150, loss = 15.38890836\n",
      "Iteration 151, loss = 15.36591832\n",
      "Iteration 152, loss = 15.34288840\n",
      "Iteration 153, loss = 15.31981871\n",
      "Iteration 154, loss = 15.29670939\n",
      "Iteration 155, loss = 15.27356055\n",
      "Iteration 156, loss = 15.25040620\n",
      "Iteration 157, loss = 15.22722861\n",
      "Iteration 158, loss = 15.20401493\n",
      "Iteration 159, loss = 15.18076489\n",
      "Iteration 160, loss = 15.15747829\n",
      "Iteration 161, loss = 15.13415494\n",
      "Iteration 162, loss = 15.11079468\n",
      "Iteration 163, loss = 15.08739741\n",
      "Iteration 164, loss = 15.06396303\n",
      "Iteration 165, loss = 15.04049147\n",
      "Iteration 166, loss = 15.01698268\n",
      "Iteration 167, loss = 14.99343666\n",
      "Iteration 168, loss = 14.96985338\n",
      "Iteration 169, loss = 14.94623286\n",
      "Iteration 170, loss = 14.92257512\n",
      "Iteration 171, loss = 14.89888021\n",
      "Iteration 172, loss = 14.87514818\n",
      "Iteration 173, loss = 14.85137910\n",
      "Iteration 174, loss = 14.82757303\n",
      "Iteration 175, loss = 14.80373008\n",
      "Iteration 176, loss = 14.77985032\n",
      "Iteration 177, loss = 14.75593387\n",
      "Iteration 178, loss = 14.73198083\n",
      "Iteration 179, loss = 14.70799133\n",
      "Iteration 180, loss = 14.68396548\n",
      "Iteration 181, loss = 14.65990342\n",
      "Iteration 182, loss = 14.63580528\n",
      "Iteration 183, loss = 14.61167119\n",
      "Iteration 184, loss = 14.58750132\n",
      "Iteration 185, loss = 14.56329579\n",
      "Iteration 186, loss = 14.53905478\n",
      "Iteration 187, loss = 14.51477843\n",
      "Iteration 188, loss = 14.49046690\n",
      "Iteration 189, loss = 14.46612036\n",
      "Iteration 190, loss = 14.44173898\n",
      "Iteration 191, loss = 14.41732292\n",
      "Iteration 192, loss = 14.39287235\n",
      "Iteration 193, loss = 14.36838746\n",
      "Iteration 194, loss = 14.34386841\n",
      "Iteration 195, loss = 14.31931539\n",
      "Iteration 196, loss = 14.29472857\n",
      "Iteration 197, loss = 14.27010814\n",
      "Iteration 198, loss = 14.24545429\n",
      "Iteration 199, loss = 14.22076720\n",
      "Iteration 200, loss = 14.19604706\n",
      "Iteration 201, loss = 14.17129406\n",
      "Iteration 202, loss = 14.14650839\n",
      "Iteration 203, loss = 14.12169024\n",
      "Iteration 204, loss = 14.09683981\n",
      "Iteration 205, loss = 14.07195730\n",
      "Iteration 206, loss = 14.04704290\n",
      "Iteration 207, loss = 14.02209681\n",
      "Iteration 208, loss = 13.99711924\n",
      "Iteration 209, loss = 13.97211037\n",
      "Iteration 210, loss = 13.94707042\n",
      "Iteration 211, loss = 13.92199959\n",
      "Iteration 212, loss = 13.89689808\n",
      "Iteration 213, loss = 13.87176610\n",
      "Iteration 214, loss = 13.84660385\n",
      "Iteration 215, loss = 13.82141155\n",
      "Iteration 216, loss = 13.79618939\n",
      "Iteration 217, loss = 13.77093760\n",
      "Iteration 218, loss = 13.74565638\n",
      "Iteration 219, loss = 13.72034594\n",
      "Iteration 220, loss = 13.69500649\n",
      "Iteration 221, loss = 13.66963826\n",
      "Iteration 222, loss = 13.64424145\n",
      "Iteration 223, loss = 13.61881628\n",
      "Iteration 224, loss = 13.59336296\n",
      "Iteration 225, loss = 13.56788171\n",
      "Iteration 226, loss = 13.54237275\n",
      "Iteration 227, loss = 13.51684719\n",
      "Iteration 228, loss = 13.49130651\n",
      "Iteration 229, loss = 13.46574019\n",
      "Iteration 230, loss = 13.44014826\n",
      "Iteration 231, loss = 13.41453077\n",
      "Iteration 232, loss = 13.38888779\n",
      "Iteration 233, loss = 13.36321940\n",
      "Iteration 234, loss = 13.33752570\n",
      "Iteration 235, loss = 13.31180681\n",
      "Iteration 236, loss = 13.28606284\n",
      "Iteration 237, loss = 13.26029394\n",
      "Iteration 238, loss = 13.23450024\n",
      "Iteration 239, loss = 13.20868191\n",
      "Iteration 240, loss = 13.18283911\n",
      "Iteration 241, loss = 13.15697200\n",
      "Iteration 242, loss = 13.13108076\n",
      "Iteration 243, loss = 13.10516558\n",
      "Iteration 244, loss = 13.07922665\n",
      "Iteration 245, loss = 13.05326415\n",
      "Iteration 246, loss = 13.02727830\n",
      "Iteration 247, loss = 13.00126928\n",
      "Iteration 248, loss = 12.97523731\n",
      "Iteration 249, loss = 12.94918260\n",
      "Iteration 250, loss = 12.92310536\n",
      "Iteration 251, loss = 12.89700581\n",
      "Iteration 252, loss = 12.87088416\n",
      "Iteration 253, loss = 12.84474065\n",
      "Iteration 254, loss = 12.81857549\n",
      "Iteration 255, loss = 12.79238890\n",
      "Iteration 256, loss = 12.76618113\n",
      "Iteration 257, loss = 12.73995240\n",
      "Iteration 258, loss = 12.71370294\n",
      "Iteration 259, loss = 12.68743298\n",
      "Iteration 260, loss = 12.66114277\n",
      "Iteration 261, loss = 12.63483253\n",
      "Iteration 262, loss = 12.60850250\n",
      "Iteration 263, loss = 12.58215293\n",
      "Iteration 264, loss = 12.55578405\n",
      "Iteration 265, loss = 12.52939611\n",
      "Iteration 266, loss = 12.50298934\n",
      "Iteration 267, loss = 12.47656399\n",
      "Iteration 268, loss = 12.45012031\n",
      "Iteration 269, loss = 12.42365853\n",
      "Iteration 270, loss = 12.39717891\n",
      "Iteration 271, loss = 12.37068169\n",
      "Iteration 272, loss = 12.34416712\n",
      "Iteration 273, loss = 12.31763544\n",
      "Iteration 274, loss = 12.29108690\n",
      "Iteration 275, loss = 12.26452176\n",
      "Iteration 276, loss = 12.23794026\n",
      "Iteration 277, loss = 12.21134265\n",
      "Iteration 278, loss = 12.18472918\n",
      "Iteration 279, loss = 12.15810011\n",
      "Iteration 280, loss = 12.13145569\n",
      "Iteration 281, loss = 12.10479616\n",
      "Iteration 282, loss = 12.07812178\n",
      "Iteration 283, loss = 12.05143281\n",
      "Iteration 284, loss = 12.02472949\n",
      "Iteration 285, loss = 11.99801208\n",
      "Iteration 286, loss = 11.97128084\n",
      "Iteration 287, loss = 11.94453602\n",
      "Iteration 288, loss = 11.91777788\n",
      "Iteration 289, loss = 11.89100667\n",
      "Iteration 290, loss = 11.86422264\n",
      "Iteration 291, loss = 11.83742606\n",
      "Iteration 292, loss = 11.81061718\n",
      "Iteration 293, loss = 11.78379625\n",
      "Iteration 294, loss = 11.75696354\n",
      "Iteration 295, loss = 11.73011930\n",
      "Iteration 296, loss = 11.70326378\n",
      "Iteration 297, loss = 11.67639726\n",
      "Iteration 298, loss = 11.64951998\n",
      "Iteration 299, loss = 11.62263221\n",
      "Iteration 300, loss = 11.59573420\n",
      "Iteration 301, loss = 11.56882621\n",
      "Iteration 302, loss = 11.54190851\n",
      "Iteration 303, loss = 11.51498134\n",
      "Iteration 304, loss = 11.48804498\n",
      "Iteration 305, loss = 11.46109968\n",
      "Iteration 306, loss = 11.43414571\n",
      "Iteration 307, loss = 11.40718331\n",
      "Iteration 308, loss = 11.38021276\n",
      "Iteration 309, loss = 11.35323432\n",
      "Iteration 310, loss = 11.32624824\n",
      "Iteration 311, loss = 11.29925479\n",
      "Iteration 312, loss = 11.27225423\n",
      "Iteration 313, loss = 11.24524682\n",
      "Iteration 314, loss = 11.21823282\n",
      "Iteration 315, loss = 11.19121793\n",
      "Iteration 316, loss = 11.16420346\n",
      "Iteration 317, loss = 11.13718389\n",
      "Iteration 318, loss = 11.11015938\n",
      "Iteration 319, loss = 11.08313011\n",
      "Iteration 320, loss = 11.05609624\n",
      "Iteration 321, loss = 11.02905798\n",
      "Iteration 322, loss = 11.00201552\n",
      "Iteration 323, loss = 10.97496906\n",
      "Iteration 324, loss = 10.94791882\n",
      "Iteration 325, loss = 10.92086501\n",
      "Iteration 326, loss = 10.89380786\n",
      "Iteration 327, loss = 10.86674758\n",
      "Iteration 328, loss = 10.83968441\n",
      "Iteration 329, loss = 10.81261857\n",
      "Iteration 330, loss = 10.78555032\n",
      "Iteration 331, loss = 10.75847989\n",
      "Iteration 332, loss = 10.73140751\n",
      "Iteration 333, loss = 10.70433344\n",
      "Iteration 334, loss = 10.67725792\n",
      "Iteration 335, loss = 10.65018119\n",
      "Iteration 336, loss = 10.62310352\n",
      "Iteration 337, loss = 10.59602515\n",
      "Iteration 338, loss = 10.56894634\n",
      "Iteration 339, loss = 10.54186733\n",
      "Iteration 340, loss = 10.51478840\n",
      "Iteration 341, loss = 10.48770978\n",
      "Iteration 342, loss = 10.46063175\n",
      "Iteration 343, loss = 10.43355456\n",
      "Iteration 344, loss = 10.40647847\n",
      "Iteration 345, loss = 10.37940374\n",
      "Iteration 346, loss = 10.35233063\n",
      "Iteration 347, loss = 10.32525940\n",
      "Iteration 348, loss = 10.29819031\n",
      "Iteration 349, loss = 10.27112363\n",
      "Iteration 350, loss = 10.24405962\n",
      "Iteration 351, loss = 10.21699853\n",
      "Iteration 352, loss = 10.18994064\n",
      "Iteration 353, loss = 10.16288620\n",
      "Iteration 354, loss = 10.13583549\n",
      "Iteration 355, loss = 10.10878875\n",
      "Iteration 356, loss = 10.08174626\n",
      "Iteration 357, loss = 10.05470829\n",
      "Iteration 358, loss = 10.02767508\n",
      "Iteration 359, loss = 10.00064691\n",
      "Iteration 360, loss = 9.97362404\n",
      "Iteration 361, loss = 9.94660673\n",
      "Iteration 362, loss = 9.91959525\n",
      "Iteration 363, loss = 9.89258987\n",
      "Iteration 364, loss = 9.86559083\n",
      "Iteration 365, loss = 9.83859841\n",
      "Iteration 366, loss = 9.81161288\n",
      "Iteration 367, loss = 9.78463448\n",
      "Iteration 368, loss = 9.75766350\n",
      "Iteration 369, loss = 9.73070018\n",
      "Iteration 370, loss = 9.70374480\n",
      "Iteration 371, loss = 9.67679761\n",
      "Iteration 372, loss = 9.64985888\n",
      "Iteration 373, loss = 9.62292887\n",
      "Iteration 374, loss = 9.59600784\n",
      "Iteration 375, loss = 9.56909605\n",
      "Iteration 376, loss = 9.54219376\n",
      "Iteration 377, loss = 9.51530125\n",
      "Iteration 378, loss = 9.48841876\n",
      "Iteration 379, loss = 9.46154656\n",
      "Iteration 380, loss = 9.43468491\n",
      "Iteration 381, loss = 9.40783406\n",
      "Iteration 382, loss = 9.38099429\n",
      "Iteration 383, loss = 9.35416585\n",
      "Iteration 384, loss = 9.32734899\n",
      "Iteration 385, loss = 9.30054399\n",
      "Iteration 386, loss = 9.27375109\n",
      "Iteration 387, loss = 9.24697055\n",
      "Iteration 388, loss = 9.22020264\n",
      "Iteration 389, loss = 9.19344762\n",
      "Iteration 390, loss = 9.16670573\n",
      "Iteration 391, loss = 9.13997724\n",
      "Iteration 392, loss = 9.11326241\n",
      "Iteration 393, loss = 9.08656149\n",
      "Iteration 394, loss = 9.05987474\n",
      "Iteration 395, loss = 9.03320241\n",
      "Iteration 396, loss = 9.00654476\n",
      "Iteration 397, loss = 8.97990205\n",
      "Iteration 398, loss = 8.95327453\n",
      "Iteration 399, loss = 8.92666245\n",
      "Iteration 400, loss = 8.90006607\n",
      "Iteration 401, loss = 8.87348565\n",
      "Iteration 402, loss = 8.84692143\n",
      "Iteration 403, loss = 8.82037367\n",
      "Iteration 404, loss = 8.79384263\n",
      "Iteration 405, loss = 8.76732855\n",
      "Iteration 406, loss = 8.74083169\n",
      "Iteration 407, loss = 8.71435229\n",
      "Iteration 408, loss = 8.68789062\n",
      "Iteration 409, loss = 8.66144691\n",
      "Iteration 410, loss = 8.63502143\n",
      "Iteration 411, loss = 8.60861441\n",
      "Iteration 412, loss = 8.58222612\n",
      "Iteration 413, loss = 8.55585679\n",
      "Iteration 414, loss = 8.52950668\n",
      "Iteration 415, loss = 8.50317603\n",
      "Iteration 416, loss = 8.47686509\n",
      "Iteration 417, loss = 8.45057411\n",
      "Iteration 418, loss = 8.42430334\n",
      "Iteration 419, loss = 8.39805301\n",
      "Iteration 420, loss = 8.37182339\n",
      "Iteration 421, loss = 8.34561470\n",
      "Iteration 422, loss = 8.31942720\n",
      "Iteration 423, loss = 8.29326112\n",
      "Iteration 424, loss = 8.26711672\n",
      "Iteration 425, loss = 8.24099424\n",
      "Iteration 426, loss = 8.21489391\n",
      "Iteration 427, loss = 8.18881598\n",
      "Iteration 428, loss = 8.16276069\n",
      "Iteration 429, loss = 8.13672828\n",
      "Iteration 430, loss = 8.11071900\n",
      "Iteration 431, loss = 8.08473307\n",
      "Iteration 432, loss = 8.05877074\n",
      "Iteration 433, loss = 8.03283226\n",
      "Iteration 434, loss = 8.00691784\n",
      "Iteration 435, loss = 7.98102774\n",
      "Iteration 436, loss = 7.95516219\n",
      "Iteration 437, loss = 7.92932143\n",
      "Iteration 438, loss = 7.90350569\n",
      "Iteration 439, loss = 7.87771520\n",
      "Iteration 440, loss = 7.85195021\n",
      "Iteration 441, loss = 7.82621094\n",
      "Iteration 442, loss = 7.80049762\n",
      "Iteration 443, loss = 7.77481050\n",
      "Iteration 444, loss = 7.74914979\n",
      "Iteration 445, loss = 7.72351574\n",
      "Iteration 446, loss = 7.69790857\n",
      "Iteration 447, loss = 7.67232852\n",
      "Iteration 448, loss = 7.64677580\n",
      "Iteration 449, loss = 7.62125066\n",
      "Iteration 450, loss = 7.59575331\n",
      "Iteration 451, loss = 7.57028399\n",
      "Iteration 452, loss = 7.54484292\n",
      "Iteration 453, loss = 7.51943033\n",
      "Iteration 454, loss = 7.49404644\n",
      "Iteration 455, loss = 7.46869148\n",
      "Iteration 456, loss = 7.44336567\n",
      "Iteration 457, loss = 7.41806923\n",
      "Iteration 458, loss = 7.39280239\n",
      "Iteration 459, loss = 7.36756537\n",
      "Iteration 460, loss = 7.34235839\n",
      "Iteration 461, loss = 7.31718166\n",
      "Iteration 462, loss = 7.29203762\n",
      "Iteration 463, loss = 7.26692476\n",
      "Iteration 464, loss = 7.24184296\n",
      "Iteration 465, loss = 7.21679240\n",
      "Iteration 466, loss = 7.19177328\n",
      "Iteration 467, loss = 7.16678580\n",
      "Iteration 468, loss = 7.14183016\n",
      "Iteration 469, loss = 7.11690655\n",
      "Iteration 470, loss = 7.09201518\n",
      "Iteration 471, loss = 7.06715624\n",
      "Iteration 472, loss = 7.04232994\n",
      "Iteration 473, loss = 7.01753648\n",
      "Iteration 474, loss = 6.99277606\n",
      "Iteration 475, loss = 6.96804887\n",
      "Iteration 476, loss = 6.94335514\n",
      "Iteration 477, loss = 6.91869504\n",
      "Iteration 478, loss = 6.89406880\n",
      "Iteration 479, loss = 6.86947660\n",
      "Iteration 480, loss = 6.84491865\n",
      "Iteration 481, loss = 6.82039515\n",
      "Iteration 482, loss = 6.79590630\n",
      "Iteration 483, loss = 6.77145231\n",
      "Iteration 484, loss = 6.74703337\n",
      "Iteration 485, loss = 6.72264968\n",
      "Iteration 486, loss = 6.69830144\n",
      "Iteration 487, loss = 6.67398884\n",
      "Iteration 488, loss = 6.64971210\n",
      "Iteration 489, loss = 6.62547140\n",
      "Iteration 490, loss = 6.60126693\n",
      "Iteration 491, loss = 6.57709890\n",
      "Iteration 492, loss = 6.55296751\n",
      "Iteration 493, loss = 6.52887293\n",
      "Iteration 494, loss = 6.50481538\n",
      "Iteration 495, loss = 6.48079503\n",
      "Iteration 496, loss = 6.45681209\n",
      "Iteration 497, loss = 6.43286674\n",
      "Iteration 498, loss = 6.40895917\n",
      "Iteration 499, loss = 6.38508958\n",
      "Iteration 500, loss = 6.36125815\n",
      "Iteration 501, loss = 6.33746507\n",
      "Iteration 502, loss = 6.31371053\n",
      "Iteration 503, loss = 6.28999471\n",
      "Iteration 504, loss = 6.26631780\n",
      "Iteration 505, loss = 6.24267998\n",
      "Iteration 506, loss = 6.21908144\n",
      "Iteration 507, loss = 6.19552236\n",
      "Iteration 508, loss = 6.17200291\n",
      "Iteration 509, loss = 6.14852329\n",
      "Iteration 510, loss = 6.12508367\n",
      "Iteration 511, loss = 6.10168423\n",
      "Iteration 512, loss = 6.07832514\n",
      "Iteration 513, loss = 6.05500659\n",
      "Iteration 514, loss = 6.03172875\n",
      "Iteration 515, loss = 6.00849179\n",
      "Iteration 516, loss = 5.98529590\n",
      "Iteration 517, loss = 5.96214124\n",
      "Iteration 518, loss = 5.93902798\n",
      "Iteration 519, loss = 5.91595630\n",
      "Iteration 520, loss = 5.89292636\n",
      "Iteration 521, loss = 5.86993834\n",
      "Iteration 522, loss = 5.84699240\n",
      "Iteration 523, loss = 5.82408872\n",
      "Iteration 524, loss = 5.80122745\n",
      "Iteration 525, loss = 5.77840876\n",
      "Iteration 526, loss = 5.75563282\n",
      "Iteration 527, loss = 5.73289979\n",
      "Iteration 528, loss = 5.71020983\n",
      "Iteration 529, loss = 5.68756311\n",
      "Iteration 530, loss = 5.66495977\n",
      "Iteration 531, loss = 5.64239999\n",
      "Iteration 532, loss = 5.61988392\n",
      "Iteration 533, loss = 5.59741172\n",
      "Iteration 534, loss = 5.57498355\n",
      "Iteration 535, loss = 5.55259955\n",
      "Iteration 536, loss = 5.53025988\n",
      "Iteration 537, loss = 5.50796470\n",
      "Iteration 538, loss = 5.48571415\n",
      "Iteration 539, loss = 5.46350840\n",
      "Iteration 540, loss = 5.44134758\n",
      "Iteration 541, loss = 5.41923185\n",
      "Iteration 542, loss = 5.39716135\n",
      "Iteration 543, loss = 5.37513624\n",
      "Iteration 544, loss = 5.35315665\n",
      "Iteration 545, loss = 5.33122273\n",
      "Iteration 546, loss = 5.30933463\n",
      "Iteration 547, loss = 5.28749248\n",
      "Iteration 548, loss = 5.26569643\n",
      "Iteration 549, loss = 5.24394662\n",
      "Iteration 550, loss = 5.22224318\n",
      "Iteration 551, loss = 5.20058626\n",
      "Iteration 552, loss = 5.17897599\n",
      "Iteration 553, loss = 5.15741251\n",
      "Iteration 554, loss = 5.13589595\n",
      "Iteration 555, loss = 5.11442644\n",
      "Iteration 556, loss = 5.09300413\n",
      "Iteration 557, loss = 5.07162913\n",
      "Iteration 558, loss = 5.05030158\n",
      "Iteration 559, loss = 5.02902162\n",
      "Iteration 560, loss = 5.00778936\n",
      "Iteration 561, loss = 4.98660493\n",
      "Iteration 562, loss = 4.96546846\n",
      "Iteration 563, loss = 4.94438008\n",
      "Iteration 564, loss = 4.92333990\n",
      "Iteration 565, loss = 4.90234806\n",
      "Iteration 566, loss = 4.88140466\n",
      "Iteration 567, loss = 4.86050984\n",
      "Iteration 568, loss = 4.83966371\n",
      "Iteration 569, loss = 4.81886639\n",
      "Iteration 570, loss = 4.79811799\n",
      "Iteration 571, loss = 4.77741863\n",
      "Iteration 572, loss = 4.75676843\n",
      "Iteration 573, loss = 4.73616750\n",
      "Iteration 574, loss = 4.71561596\n",
      "Iteration 575, loss = 4.69511390\n",
      "Iteration 576, loss = 4.67466144\n",
      "Iteration 577, loss = 4.65425870\n",
      "Iteration 578, loss = 4.63390578\n",
      "Iteration 579, loss = 4.61360278\n",
      "Iteration 580, loss = 4.59334981\n",
      "Iteration 581, loss = 4.57314697\n",
      "Iteration 582, loss = 4.55299438\n",
      "Iteration 583, loss = 4.53289212\n",
      "Iteration 584, loss = 4.51284031\n",
      "Iteration 585, loss = 4.49283903\n",
      "Iteration 586, loss = 4.47288839\n",
      "Iteration 587, loss = 4.45298849\n",
      "Iteration 588, loss = 4.43313942\n",
      "Iteration 589, loss = 4.41334128\n",
      "Iteration 590, loss = 4.39359416\n",
      "Iteration 591, loss = 4.37389815\n",
      "Iteration 592, loss = 4.35425334\n",
      "Iteration 593, loss = 4.33465983\n",
      "Iteration 594, loss = 4.31511770\n",
      "Iteration 595, loss = 4.29562705\n",
      "Iteration 596, loss = 4.27618795\n",
      "Iteration 597, loss = 4.25680049\n",
      "Iteration 598, loss = 4.23746476\n",
      "Iteration 599, loss = 4.21818085\n",
      "Iteration 600, loss = 4.19894882\n",
      "Iteration 601, loss = 4.17976878\n",
      "Iteration 602, loss = 4.16064078\n",
      "Iteration 603, loss = 4.14156491\n",
      "Iteration 604, loss = 4.12254126\n",
      "Iteration 605, loss = 4.10356989\n",
      "Iteration 606, loss = 4.08465088\n",
      "Iteration 607, loss = 4.06578430\n",
      "Iteration 608, loss = 4.04697023\n",
      "Iteration 609, loss = 4.02820874\n",
      "Iteration 610, loss = 4.00949989\n",
      "Iteration 611, loss = 3.99084377\n",
      "Iteration 612, loss = 3.97224042\n",
      "Iteration 613, loss = 3.95368993\n",
      "Iteration 614, loss = 3.93519235\n",
      "Iteration 615, loss = 3.91674775\n",
      "Iteration 616, loss = 3.89835620\n",
      "Iteration 617, loss = 3.88001775\n",
      "Iteration 618, loss = 3.86173247\n",
      "Iteration 619, loss = 3.84350041\n",
      "Iteration 620, loss = 3.82532164\n",
      "Iteration 621, loss = 3.80719621\n",
      "Iteration 622, loss = 3.78912417\n",
      "Iteration 623, loss = 3.77110558\n",
      "Iteration 624, loss = 3.75314050\n",
      "Iteration 625, loss = 3.73522898\n",
      "Iteration 626, loss = 3.71737106\n",
      "Iteration 627, loss = 3.69956680\n",
      "Iteration 628, loss = 3.68181625\n",
      "Iteration 629, loss = 3.66411945\n",
      "Iteration 630, loss = 3.64647646\n",
      "Iteration 631, loss = 3.62888730\n",
      "Iteration 632, loss = 3.61135204\n",
      "Iteration 633, loss = 3.59387071\n",
      "Iteration 634, loss = 3.57644335\n",
      "Iteration 635, loss = 3.55907001\n",
      "Iteration 636, loss = 3.54175072\n",
      "Iteration 637, loss = 3.52448552\n",
      "Iteration 638, loss = 3.50727445\n",
      "Iteration 639, loss = 3.49011754\n",
      "Iteration 640, loss = 3.47301483\n",
      "Iteration 641, loss = 3.45596635\n",
      "Iteration 642, loss = 3.43897214\n",
      "Iteration 643, loss = 3.42203222\n",
      "Iteration 644, loss = 3.40514662\n",
      "Iteration 645, loss = 3.38831538\n",
      "Iteration 646, loss = 3.37153852\n",
      "Iteration 647, loss = 3.35481606\n",
      "Iteration 648, loss = 3.33814803\n",
      "Iteration 649, loss = 3.32153446\n",
      "Iteration 650, loss = 3.30497537\n",
      "Iteration 651, loss = 3.28847077\n",
      "Iteration 652, loss = 3.27202070\n",
      "Iteration 653, loss = 3.25562516\n",
      "Iteration 654, loss = 3.23928425\n",
      "Iteration 655, loss = 3.22299793\n",
      "Iteration 656, loss = 3.20676621\n",
      "Iteration 657, loss = 3.19058909\n",
      "Iteration 658, loss = 3.17446659\n",
      "Iteration 659, loss = 3.15839871\n",
      "Iteration 660, loss = 3.14238547\n",
      "Iteration 661, loss = 3.12642687\n",
      "Iteration 662, loss = 3.11052293\n",
      "Iteration 663, loss = 3.09467365\n",
      "Iteration 664, loss = 3.07887903\n",
      "Iteration 665, loss = 3.06313909\n",
      "Iteration 666, loss = 3.04745382\n",
      "Iteration 667, loss = 3.03182323\n",
      "Iteration 668, loss = 3.01624732\n",
      "Iteration 669, loss = 3.00072608\n",
      "Iteration 670, loss = 2.98525953\n",
      "Iteration 671, loss = 2.96984765\n",
      "Iteration 672, loss = 2.95449045\n",
      "Iteration 673, loss = 2.93918792\n",
      "Iteration 674, loss = 2.92394005\n",
      "Iteration 675, loss = 2.90874685\n",
      "Iteration 676, loss = 2.89360829\n",
      "Iteration 677, loss = 2.87852437\n",
      "Iteration 678, loss = 2.86349509\n",
      "Iteration 679, loss = 2.84852043\n",
      "Iteration 680, loss = 2.83360039\n",
      "Iteration 681, loss = 2.81873494\n",
      "Iteration 682, loss = 2.80392407\n",
      "Iteration 683, loss = 2.78916777\n",
      "Iteration 684, loss = 2.77446602\n",
      "Iteration 685, loss = 2.75981881\n",
      "Iteration 686, loss = 2.74522612\n",
      "Iteration 687, loss = 2.73068792\n",
      "Iteration 688, loss = 2.71620419\n",
      "Iteration 689, loss = 2.70177492\n",
      "Iteration 690, loss = 2.68740008\n",
      "Iteration 691, loss = 2.67307965\n",
      "Iteration 692, loss = 2.65881360\n",
      "Iteration 693, loss = 2.64460191\n",
      "Iteration 694, loss = 2.63044454\n",
      "Iteration 695, loss = 2.61634148\n",
      "Iteration 696, loss = 2.60229268\n",
      "Iteration 697, loss = 2.58829813\n",
      "Iteration 698, loss = 2.57435778\n",
      "Iteration 699, loss = 2.56047161\n",
      "Iteration 700, loss = 2.54663959\n",
      "Iteration 701, loss = 2.53286167\n",
      "Iteration 702, loss = 2.51913782\n",
      "Iteration 703, loss = 2.50546800\n",
      "Iteration 704, loss = 2.49185218\n",
      "Iteration 705, loss = 2.47829032\n",
      "Iteration 706, loss = 2.46478238\n",
      "Iteration 707, loss = 2.45132831\n",
      "Iteration 708, loss = 2.43792808\n",
      "Iteration 709, loss = 2.42458164\n",
      "Iteration 710, loss = 2.41128894\n",
      "Iteration 711, loss = 2.39804995\n",
      "Iteration 712, loss = 2.38486461\n",
      "Iteration 713, loss = 2.37173288\n",
      "Iteration 714, loss = 2.35865470\n",
      "Iteration 715, loss = 2.34563004\n",
      "Iteration 716, loss = 2.33265884\n",
      "Iteration 717, loss = 2.31974104\n",
      "Iteration 718, loss = 2.30687660\n",
      "Iteration 719, loss = 2.29406547\n",
      "Iteration 720, loss = 2.28130758\n",
      "Iteration 721, loss = 2.26860288\n",
      "Iteration 722, loss = 2.25595132\n",
      "Iteration 723, loss = 2.24335284\n",
      "Iteration 724, loss = 2.23080737\n",
      "Iteration 725, loss = 2.21831487\n",
      "Iteration 726, loss = 2.20587527\n",
      "Iteration 727, loss = 2.19348851\n",
      "Iteration 728, loss = 2.18115452\n",
      "Iteration 729, loss = 2.16887325\n",
      "Iteration 730, loss = 2.15664462\n",
      "Iteration 731, loss = 2.14446858\n",
      "Iteration 732, loss = 2.13234506\n",
      "Iteration 733, loss = 2.12027399\n",
      "Iteration 734, loss = 2.10825531\n",
      "Iteration 735, loss = 2.09628893\n",
      "Iteration 736, loss = 2.08437481\n",
      "Iteration 737, loss = 2.07251285\n",
      "Iteration 738, loss = 2.06070300\n",
      "Iteration 739, loss = 2.04894517\n",
      "Iteration 740, loss = 2.03723930\n",
      "Iteration 741, loss = 2.02558532\n",
      "Iteration 742, loss = 2.01398314\n",
      "Iteration 743, loss = 2.00243268\n",
      "Iteration 744, loss = 1.99093388\n",
      "Iteration 745, loss = 1.97948666\n",
      "Iteration 746, loss = 1.96809093\n",
      "Iteration 747, loss = 1.95674661\n",
      "Iteration 748, loss = 1.94545363\n",
      "Iteration 749, loss = 1.93421191\n",
      "Iteration 750, loss = 1.92302135\n",
      "Iteration 751, loss = 1.91188189\n",
      "Iteration 752, loss = 1.90079342\n",
      "Iteration 753, loss = 1.88975588\n",
      "Iteration 754, loss = 1.87876917\n",
      "Iteration 755, loss = 1.86783320\n",
      "Iteration 756, loss = 1.85694789\n",
      "Iteration 757, loss = 1.84611316\n",
      "Iteration 758, loss = 1.83532890\n",
      "Iteration 759, loss = 1.82459504\n",
      "Iteration 760, loss = 1.81391147\n",
      "Iteration 761, loss = 1.80327812\n",
      "Iteration 762, loss = 1.79269488\n",
      "Iteration 763, loss = 1.78216166\n",
      "Iteration 764, loss = 1.77167838\n",
      "Iteration 765, loss = 1.76124493\n",
      "Iteration 766, loss = 1.75086121\n",
      "Iteration 767, loss = 1.74052715\n",
      "Iteration 768, loss = 1.73024262\n",
      "Iteration 769, loss = 1.72000754\n",
      "Iteration 770, loss = 1.70982182\n",
      "Iteration 771, loss = 1.69968534\n",
      "Iteration 772, loss = 1.68959801\n",
      "Iteration 773, loss = 1.67955973\n",
      "Iteration 774, loss = 1.66957040\n",
      "Iteration 775, loss = 1.65962991\n",
      "Iteration 776, loss = 1.64973817\n",
      "Iteration 777, loss = 1.63989506\n",
      "Iteration 778, loss = 1.63010049\n",
      "Iteration 779, loss = 1.62035434\n",
      "Iteration 780, loss = 1.61065652\n",
      "Iteration 781, loss = 1.60100691\n",
      "Iteration 782, loss = 1.59140541\n",
      "Iteration 783, loss = 1.58185191\n",
      "Iteration 784, loss = 1.57234631\n",
      "Iteration 785, loss = 1.56288848\n",
      "Iteration 786, loss = 1.55347833\n",
      "Iteration 787, loss = 1.54411574\n",
      "Iteration 788, loss = 1.53480060\n",
      "Iteration 789, loss = 1.52553280\n",
      "Iteration 790, loss = 1.51631222\n",
      "Iteration 791, loss = 1.50713876\n",
      "Iteration 792, loss = 1.49801230\n",
      "Iteration 793, loss = 1.48893272\n",
      "Iteration 794, loss = 1.47989991\n",
      "Iteration 795, loss = 1.47091375\n",
      "Iteration 796, loss = 1.46197413\n",
      "Iteration 797, loss = 1.45308093\n",
      "Iteration 798, loss = 1.44423403\n",
      "Iteration 799, loss = 1.43543331\n",
      "Iteration 800, loss = 1.42667867\n",
      "Iteration 801, loss = 1.41796996\n",
      "Iteration 802, loss = 1.40930709\n",
      "Iteration 803, loss = 1.40068992\n",
      "Iteration 804, loss = 1.39211833\n",
      "Iteration 805, loss = 1.38359221\n",
      "Iteration 806, loss = 1.37511143\n",
      "Iteration 807, loss = 1.36667586\n",
      "Iteration 808, loss = 1.35828540\n",
      "Iteration 809, loss = 1.34993990\n",
      "Iteration 810, loss = 1.34163925\n",
      "Iteration 811, loss = 1.33338332\n",
      "Iteration 812, loss = 1.32517200\n",
      "Iteration 813, loss = 1.31700508\n",
      "Iteration 814, loss = 1.30888250\n",
      "Iteration 815, loss = 1.30080412\n",
      "Iteration 816, loss = 1.29276982\n",
      "Iteration 817, loss = 1.28477948\n",
      "Iteration 818, loss = 1.27683297\n",
      "Iteration 819, loss = 1.26893015\n",
      "Iteration 820, loss = 1.26107091\n",
      "Iteration 821, loss = 1.25325510\n",
      "Iteration 822, loss = 1.24548261\n",
      "Iteration 823, loss = 1.23775330\n",
      "Iteration 824, loss = 1.23006703\n",
      "Iteration 825, loss = 1.22242369\n",
      "Iteration 826, loss = 1.21482313\n",
      "Iteration 827, loss = 1.20726523\n",
      "Iteration 828, loss = 1.19974985\n",
      "Iteration 829, loss = 1.19227686\n",
      "Iteration 830, loss = 1.18484613\n",
      "Iteration 831, loss = 1.17745752\n",
      "Iteration 832, loss = 1.17011090\n",
      "Iteration 833, loss = 1.16280614\n",
      "Iteration 834, loss = 1.15554309\n",
      "Iteration 835, loss = 1.14832163\n",
      "Iteration 836, loss = 1.14114162\n",
      "Iteration 837, loss = 1.13400292\n",
      "Iteration 838, loss = 1.12690539\n",
      "Iteration 839, loss = 1.11984891\n",
      "Iteration 840, loss = 1.11283333\n",
      "Iteration 841, loss = 1.10585852\n",
      "Iteration 842, loss = 1.09892433\n",
      "Iteration 843, loss = 1.09203064\n",
      "Iteration 844, loss = 1.08517730\n",
      "Iteration 845, loss = 1.07836417\n",
      "Iteration 846, loss = 1.07159111\n",
      "Iteration 847, loss = 1.06485799\n",
      "Iteration 848, loss = 1.05816467\n",
      "Iteration 849, loss = 1.05151100\n",
      "Iteration 850, loss = 1.04489685\n",
      "Iteration 851, loss = 1.03832208\n",
      "Iteration 852, loss = 1.03178654\n",
      "Iteration 853, loss = 1.02529010\n",
      "Iteration 854, loss = 1.01883261\n",
      "Iteration 855, loss = 1.01241394\n",
      "Iteration 856, loss = 1.00603393\n",
      "Iteration 857, loss = 0.99969246\n",
      "Iteration 858, loss = 0.99338937\n",
      "Iteration 859, loss = 0.98712453\n",
      "Iteration 860, loss = 0.98089779\n",
      "Iteration 861, loss = 0.97470902\n",
      "Iteration 862, loss = 0.96855806\n",
      "Iteration 863, loss = 0.96244477\n",
      "Iteration 864, loss = 0.95636902\n",
      "Iteration 865, loss = 0.95033066\n",
      "Iteration 866, loss = 0.94432954\n",
      "Iteration 867, loss = 0.93836553\n",
      "Iteration 868, loss = 0.93243847\n",
      "Iteration 869, loss = 0.92654823\n",
      "Iteration 870, loss = 0.92069465\n",
      "Iteration 871, loss = 0.91487760\n",
      "Iteration 872, loss = 0.90909694\n",
      "Iteration 873, loss = 0.90335251\n",
      "Iteration 874, loss = 0.89764417\n",
      "Iteration 875, loss = 0.89197178\n",
      "Iteration 876, loss = 0.88633519\n",
      "Iteration 877, loss = 0.88073427\n",
      "Iteration 878, loss = 0.87516885\n",
      "Iteration 879, loss = 0.86963880\n",
      "Iteration 880, loss = 0.86414397\n",
      "Iteration 881, loss = 0.85868423\n",
      "Iteration 882, loss = 0.85325941\n",
      "Iteration 883, loss = 0.84786938\n",
      "Iteration 884, loss = 0.84251399\n",
      "Iteration 885, loss = 0.83719309\n",
      "Iteration 886, loss = 0.83190655\n",
      "Iteration 887, loss = 0.82665421\n",
      "Iteration 888, loss = 0.82143593\n",
      "Iteration 889, loss = 0.81625156\n",
      "Iteration 890, loss = 0.81110095\n",
      "Iteration 891, loss = 0.80598397\n",
      "Iteration 892, loss = 0.80090047\n",
      "Iteration 893, loss = 0.79585029\n",
      "Iteration 894, loss = 0.79083330\n",
      "Iteration 895, loss = 0.78584934\n",
      "Iteration 896, loss = 0.78089828\n",
      "Iteration 897, loss = 0.77597996\n",
      "Iteration 898, loss = 0.77109425\n",
      "Iteration 899, loss = 0.76624099\n",
      "Iteration 900, loss = 0.76142004\n",
      "Iteration 901, loss = 0.75663125\n",
      "Iteration 902, loss = 0.75187448\n",
      "Iteration 903, loss = 0.74714958\n",
      "Iteration 904, loss = 0.74245641\n",
      "Iteration 905, loss = 0.73779482\n",
      "Iteration 906, loss = 0.73316466\n",
      "Iteration 907, loss = 0.72856580\n",
      "Iteration 908, loss = 0.72399808\n",
      "Iteration 909, loss = 0.71946136\n",
      "Iteration 910, loss = 0.71495549\n",
      "Iteration 911, loss = 0.71048034\n",
      "Iteration 912, loss = 0.70603575\n",
      "Iteration 913, loss = 0.70162158\n",
      "Iteration 914, loss = 0.69723768\n",
      "Iteration 915, loss = 0.69288391\n",
      "Iteration 916, loss = 0.68856013\n",
      "Iteration 917, loss = 0.68426619\n",
      "Iteration 918, loss = 0.68000195\n",
      "Iteration 919, loss = 0.67576726\n",
      "Iteration 920, loss = 0.67156198\n",
      "Iteration 921, loss = 0.66738597\n",
      "Iteration 922, loss = 0.66323907\n",
      "Iteration 923, loss = 0.65912116\n",
      "Iteration 924, loss = 0.65503207\n",
      "Iteration 925, loss = 0.65097168\n",
      "Iteration 926, loss = 0.64693984\n",
      "Iteration 927, loss = 0.64293640\n",
      "Iteration 928, loss = 0.63896122\n",
      "Iteration 929, loss = 0.63501416\n",
      "Iteration 930, loss = 0.63109508\n",
      "Iteration 931, loss = 0.62720383\n",
      "Iteration 932, loss = 0.62334027\n",
      "Iteration 933, loss = 0.61950427\n",
      "Iteration 934, loss = 0.61569567\n",
      "Iteration 935, loss = 0.61191434\n",
      "Iteration 936, loss = 0.60816014\n",
      "Iteration 937, loss = 0.60443292\n",
      "Iteration 938, loss = 0.60073254\n",
      "Iteration 939, loss = 0.59705887\n",
      "Iteration 940, loss = 0.59341176\n",
      "Iteration 941, loss = 0.58979108\n",
      "Iteration 942, loss = 0.58619668\n",
      "Iteration 943, loss = 0.58262842\n",
      "Iteration 944, loss = 0.57908616\n",
      "Iteration 945, loss = 0.57556977\n",
      "Iteration 946, loss = 0.57207911\n",
      "Iteration 947, loss = 0.56861403\n",
      "Iteration 948, loss = 0.56517440\n",
      "Iteration 949, loss = 0.56176008\n",
      "Iteration 950, loss = 0.55837093\n",
      "Iteration 951, loss = 0.55500681\n",
      "Iteration 952, loss = 0.55166760\n",
      "Iteration 953, loss = 0.54835314\n",
      "Iteration 954, loss = 0.54506331\n",
      "Iteration 955, loss = 0.54179796\n",
      "Iteration 956, loss = 0.53855697\n",
      "Iteration 957, loss = 0.53534019\n",
      "Iteration 958, loss = 0.53214748\n",
      "Iteration 959, loss = 0.52897872\n",
      "Iteration 960, loss = 0.52583377\n",
      "Iteration 961, loss = 0.52271250\n",
      "Iteration 962, loss = 0.51961476\n",
      "Iteration 963, loss = 0.51654043\n",
      "Iteration 964, loss = 0.51348937\n",
      "Iteration 965, loss = 0.51046144\n",
      "Iteration 966, loss = 0.50745652\n",
      "Iteration 967, loss = 0.50447448\n",
      "Iteration 968, loss = 0.50151517\n",
      "Iteration 969, loss = 0.49857847\n",
      "Iteration 970, loss = 0.49566424\n",
      "Iteration 971, loss = 0.49277236\n",
      "Iteration 972, loss = 0.48990269\n",
      "Iteration 973, loss = 0.48705511\n",
      "Iteration 974, loss = 0.48422947\n",
      "Iteration 975, loss = 0.48142566\n",
      "Iteration 976, loss = 0.47864353\n",
      "Iteration 977, loss = 0.47588297\n",
      "Iteration 978, loss = 0.47314385\n",
      "Iteration 979, loss = 0.47042602\n",
      "Iteration 980, loss = 0.46772938\n",
      "Iteration 981, loss = 0.46505378\n",
      "Iteration 982, loss = 0.46239910\n",
      "Iteration 983, loss = 0.45976522\n",
      "Iteration 984, loss = 0.45715200\n",
      "Iteration 985, loss = 0.45455932\n",
      "Iteration 986, loss = 0.45198706\n",
      "Iteration 987, loss = 0.44943509\n",
      "Iteration 988, loss = 0.44690328\n",
      "Iteration 989, loss = 0.44439151\n",
      "Iteration 990, loss = 0.44189965\n",
      "Iteration 991, loss = 0.43942758\n",
      "Iteration 992, loss = 0.43697518\n",
      "Iteration 993, loss = 0.43454232\n",
      "Iteration 994, loss = 0.43212889\n",
      "Iteration 995, loss = 0.42973475\n",
      "Iteration 996, loss = 0.42735978\n",
      "Iteration 997, loss = 0.42500387\n",
      "Iteration 998, loss = 0.42266689\n",
      "Iteration 999, loss = 0.42034873\n",
      "Iteration 1000, loss = 0.41804926\n",
      "Iteration 1001, loss = 0.41576835\n",
      "Iteration 1002, loss = 0.41350590\n",
      "Iteration 1003, loss = 0.41126179\n",
      "Iteration 1004, loss = 0.40903588\n",
      "Iteration 1005, loss = 0.40682808\n",
      "Iteration 1006, loss = 0.40463825\n",
      "Iteration 1007, loss = 0.40246628\n",
      "Iteration 1008, loss = 0.40031206\n",
      "Iteration 1009, loss = 0.39817546\n",
      "Iteration 1010, loss = 0.39605637\n",
      "Iteration 1011, loss = 0.39395468\n",
      "Iteration 1012, loss = 0.39187027\n",
      "Iteration 1013, loss = 0.38980302\n",
      "Iteration 1014, loss = 0.38775282\n",
      "Iteration 1015, loss = 0.38571957\n",
      "Iteration 1016, loss = 0.38370313\n",
      "Iteration 1017, loss = 0.38170340\n",
      "Iteration 1018, loss = 0.37972028\n",
      "Iteration 1019, loss = 0.37775363\n",
      "Iteration 1020, loss = 0.37580336\n",
      "Iteration 1021, loss = 0.37386936\n",
      "Iteration 1022, loss = 0.37195150\n",
      "Iteration 1023, loss = 0.37004969\n",
      "Iteration 1024, loss = 0.36816381\n",
      "Iteration 1025, loss = 0.36629375\n",
      "Iteration 1026, loss = 0.36443940\n",
      "Iteration 1027, loss = 0.36260066\n",
      "Iteration 1028, loss = 0.36077741\n",
      "Iteration 1029, loss = 0.35896955\n",
      "Iteration 1030, loss = 0.35717698\n",
      "Iteration 1031, loss = 0.35539957\n",
      "Iteration 1032, loss = 0.35363724\n",
      "Iteration 1033, loss = 0.35188987\n",
      "Iteration 1034, loss = 0.35015736\n",
      "Iteration 1035, loss = 0.34843960\n",
      "Iteration 1036, loss = 0.34673649\n",
      "Iteration 1037, loss = 0.34504793\n",
      "Iteration 1038, loss = 0.34337380\n",
      "Iteration 1039, loss = 0.34171402\n",
      "Iteration 1040, loss = 0.34006846\n",
      "Iteration 1041, loss = 0.33843705\n",
      "Iteration 1042, loss = 0.33681966\n",
      "Iteration 1043, loss = 0.33521621\n",
      "Iteration 1044, loss = 0.33362659\n",
      "Iteration 1045, loss = 0.33205069\n",
      "Iteration 1046, loss = 0.33048843\n",
      "Iteration 1047, loss = 0.32893970\n",
      "Iteration 1048, loss = 0.32740440\n",
      "Iteration 1049, loss = 0.32588244\n",
      "Iteration 1050, loss = 0.32437371\n",
      "Iteration 1051, loss = 0.32287812\n",
      "Iteration 1052, loss = 0.32139557\n",
      "Iteration 1053, loss = 0.31992598\n",
      "Iteration 1054, loss = 0.31846923\n",
      "Iteration 1055, loss = 0.31702523\n",
      "Iteration 1056, loss = 0.31559390\n",
      "Iteration 1057, loss = 0.31417513\n",
      "Iteration 1058, loss = 0.31276883\n",
      "Iteration 1059, loss = 0.31137491\n",
      "Iteration 1060, loss = 0.30999328\n",
      "Iteration 1061, loss = 0.30862384\n",
      "Iteration 1062, loss = 0.30726649\n",
      "Iteration 1063, loss = 0.30592116\n",
      "Iteration 1064, loss = 0.30458775\n",
      "Iteration 1065, loss = 0.30326616\n",
      "Iteration 1066, loss = 0.30195630\n",
      "Iteration 1067, loss = 0.30065810\n",
      "Iteration 1068, loss = 0.29937145\n",
      "Iteration 1069, loss = 0.29809627\n",
      "Iteration 1070, loss = 0.29683248\n",
      "Iteration 1071, loss = 0.29557997\n",
      "Iteration 1072, loss = 0.29433867\n",
      "Iteration 1073, loss = 0.29310849\n",
      "Iteration 1074, loss = 0.29188935\n",
      "Iteration 1075, loss = 0.29068114\n",
      "Iteration 1076, loss = 0.28948380\n",
      "Iteration 1077, loss = 0.28829724\n",
      "Iteration 1078, loss = 0.28712136\n",
      "Iteration 1079, loss = 0.28595610\n",
      "Iteration 1080, loss = 0.28480136\n",
      "Iteration 1081, loss = 0.28365705\n",
      "Iteration 1082, loss = 0.28252311\n",
      "Iteration 1083, loss = 0.28139944\n",
      "Iteration 1084, loss = 0.28028597\n",
      "Iteration 1085, loss = 0.27918261\n",
      "Iteration 1086, loss = 0.27808928\n",
      "Iteration 1087, loss = 0.27700590\n",
      "Iteration 1088, loss = 0.27593240\n",
      "Iteration 1089, loss = 0.27486868\n",
      "Iteration 1090, loss = 0.27381469\n",
      "Iteration 1091, loss = 0.27277032\n",
      "Iteration 1092, loss = 0.27173552\n",
      "Iteration 1093, loss = 0.27071019\n",
      "Iteration 1094, loss = 0.26969427\n",
      "Iteration 1095, loss = 0.26868767\n",
      "Iteration 1096, loss = 0.26769032\n",
      "Iteration 1097, loss = 0.26670214\n",
      "Iteration 1098, loss = 0.26572307\n",
      "Iteration 1099, loss = 0.26475302\n",
      "Iteration 1100, loss = 0.26379192\n",
      "Iteration 1101, loss = 0.26283969\n",
      "Iteration 1102, loss = 0.26189627\n",
      "Iteration 1103, loss = 0.26096158\n",
      "Iteration 1104, loss = 0.26003554\n",
      "Iteration 1105, loss = 0.25911809\n",
      "Iteration 1106, loss = 0.25820915\n",
      "Iteration 1107, loss = 0.25730865\n",
      "Iteration 1108, loss = 0.25641653\n",
      "Iteration 1109, loss = 0.25553271\n",
      "Iteration 1110, loss = 0.25465711\n",
      "Iteration 1111, loss = 0.25378968\n",
      "Iteration 1112, loss = 0.25293035\n",
      "Iteration 1113, loss = 0.25207903\n",
      "Iteration 1114, loss = 0.25123568\n",
      "Iteration 1115, loss = 0.25040021\n",
      "Iteration 1116, loss = 0.24957256\n",
      "Iteration 1117, loss = 0.24875267\n",
      "Iteration 1118, loss = 0.24794047\n",
      "Iteration 1119, loss = 0.24713589\n",
      "Iteration 1120, loss = 0.24633886\n",
      "Iteration 1121, loss = 0.24554933\n",
      "Iteration 1122, loss = 0.24476722\n",
      "Iteration 1123, loss = 0.24399248\n",
      "Iteration 1124, loss = 0.24322504\n",
      "Iteration 1125, loss = 0.24246483\n",
      "Iteration 1126, loss = 0.24171180\n",
      "Iteration 1127, loss = 0.24096588\n",
      "Iteration 1128, loss = 0.24022700\n",
      "Iteration 1129, loss = 0.23949512\n",
      "Iteration 1130, loss = 0.23877015\n",
      "Iteration 1131, loss = 0.23805206\n",
      "Iteration 1132, loss = 0.23734077\n",
      "Iteration 1133, loss = 0.23663622\n",
      "Iteration 1134, loss = 0.23593836\n",
      "Iteration 1135, loss = 0.23524712\n",
      "Iteration 1136, loss = 0.23456245\n",
      "Iteration 1137, loss = 0.23388429\n",
      "Iteration 1138, loss = 0.23321254\n",
      "Iteration 1139, loss = 0.23254715\n",
      "Iteration 1140, loss = 0.23188810\n",
      "Iteration 1141, loss = 0.23123533\n",
      "Iteration 1142, loss = 0.23058877\n",
      "Iteration 1143, loss = 0.22994838\n",
      "Iteration 1144, loss = 0.22931410\n",
      "Iteration 1145, loss = 0.22868587\n",
      "Iteration 1146, loss = 0.22806364\n",
      "Iteration 1147, loss = 0.22744736\n",
      "Iteration 1148, loss = 0.22683697\n",
      "Iteration 1149, loss = 0.22623243\n",
      "Iteration 1150, loss = 0.22563366\n",
      "Iteration 1151, loss = 0.22504063\n",
      "Iteration 1152, loss = 0.22445329\n",
      "Iteration 1153, loss = 0.22387157\n",
      "Iteration 1154, loss = 0.22329543\n",
      "Iteration 1155, loss = 0.22272481\n",
      "Iteration 1156, loss = 0.22215968\n",
      "Iteration 1157, loss = 0.22159997\n",
      "Iteration 1158, loss = 0.22104563\n",
      "Iteration 1159, loss = 0.22049662\n",
      "Iteration 1160, loss = 0.21995289\n",
      "Iteration 1161, loss = 0.21941438\n",
      "Iteration 1162, loss = 0.21888106\n",
      "Iteration 1163, loss = 0.21835286\n",
      "Iteration 1164, loss = 0.21782975\n",
      "Iteration 1165, loss = 0.21731168\n",
      "Iteration 1166, loss = 0.21679859\n",
      "Iteration 1167, loss = 0.21629045\n",
      "Iteration 1168, loss = 0.21578720\n",
      "Iteration 1169, loss = 0.21528879\n",
      "Iteration 1170, loss = 0.21479520\n",
      "Iteration 1171, loss = 0.21430636\n",
      "Iteration 1172, loss = 0.21382223\n",
      "Iteration 1173, loss = 0.21334277\n",
      "Iteration 1174, loss = 0.21286794\n",
      "Iteration 1175, loss = 0.21239768\n",
      "Iteration 1176, loss = 0.21193196\n",
      "Iteration 1177, loss = 0.21147073\n",
      "Iteration 1178, loss = 0.21101396\n",
      "Iteration 1179, loss = 0.21056159\n",
      "Iteration 1180, loss = 0.21011358\n",
      "Iteration 1181, loss = 0.20966990\n",
      "Iteration 1182, loss = 0.20923050\n",
      "Iteration 1183, loss = 0.20879534\n",
      "Iteration 1184, loss = 0.20836437\n",
      "Iteration 1185, loss = 0.20793757\n",
      "Iteration 1186, loss = 0.20751488\n",
      "Iteration 1187, loss = 0.20709627\n",
      "Iteration 1188, loss = 0.20668170\n",
      "Iteration 1189, loss = 0.20627113\n",
      "Iteration 1190, loss = 0.20586451\n",
      "Iteration 1191, loss = 0.20546182\n",
      "Iteration 1192, loss = 0.20506301\n",
      "Iteration 1193, loss = 0.20466804\n",
      "Iteration 1194, loss = 0.20427688\n",
      "Iteration 1195, loss = 0.20388949\n",
      "Iteration 1196, loss = 0.20350583\n",
      "Iteration 1197, loss = 0.20312586\n",
      "Iteration 1198, loss = 0.20274956\n",
      "Iteration 1199, loss = 0.20237687\n",
      "Iteration 1200, loss = 0.20200777\n",
      "Iteration 1201, loss = 0.20164222\n",
      "Iteration 1202, loss = 0.20128019\n",
      "Iteration 1203, loss = 0.20092163\n",
      "Iteration 1204, loss = 0.20056652\n",
      "Iteration 1205, loss = 0.20021482\n",
      "Iteration 1206, loss = 0.19986650\n",
      "Iteration 1207, loss = 0.19952152\n",
      "Iteration 1208, loss = 0.19917984\n",
      "Iteration 1209, loss = 0.19884145\n",
      "Iteration 1210, loss = 0.19850629\n",
      "Iteration 1211, loss = 0.19817434\n",
      "Iteration 1212, loss = 0.19784557\n",
      "Iteration 1213, loss = 0.19751995\n",
      "Iteration 1214, loss = 0.19719744\n",
      "Iteration 1215, loss = 0.19687801\n",
      "Iteration 1216, loss = 0.19656162\n",
      "Iteration 1217, loss = 0.19624826\n",
      "Iteration 1218, loss = 0.19593788\n",
      "Iteration 1219, loss = 0.19563047\n",
      "Iteration 1220, loss = 0.19532597\n",
      "Iteration 1221, loss = 0.19502438\n",
      "Iteration 1222, loss = 0.19472565\n",
      "Iteration 1223, loss = 0.19442976\n",
      "Iteration 1224, loss = 0.19413667\n",
      "Iteration 1225, loss = 0.19384637\n",
      "Iteration 1226, loss = 0.19355881\n",
      "Iteration 1227, loss = 0.19327398\n",
      "Iteration 1228, loss = 0.19299184\n",
      "Iteration 1229, loss = 0.19271236\n",
      "Iteration 1230, loss = 0.19243553\n",
      "Iteration 1231, loss = 0.19216130\n",
      "Iteration 1232, loss = 0.19188966\n",
      "Iteration 1233, loss = 0.19162057\n",
      "Iteration 1234, loss = 0.19135401\n",
      "Iteration 1235, loss = 0.19108995\n",
      "Iteration 1236, loss = 0.19082837\n",
      "Iteration 1237, loss = 0.19056924\n",
      "Iteration 1238, loss = 0.19031253\n",
      "Iteration 1239, loss = 0.19005822\n",
      "Iteration 1240, loss = 0.18980628\n",
      "Iteration 1241, loss = 0.18955670\n",
      "Iteration 1242, loss = 0.18930943\n",
      "Iteration 1243, loss = 0.18906447\n",
      "Iteration 1244, loss = 0.18882177\n",
      "Iteration 1245, loss = 0.18858133\n",
      "Iteration 1246, loss = 0.18834312\n",
      "Iteration 1247, loss = 0.18810710\n",
      "Iteration 1248, loss = 0.18787327\n",
      "Iteration 1249, loss = 0.18764158\n",
      "Iteration 1250, loss = 0.18741204\n",
      "Iteration 1251, loss = 0.18718460\n",
      "Iteration 1252, loss = 0.18695924\n",
      "Iteration 1253, loss = 0.18673595\n",
      "Iteration 1254, loss = 0.18651471\n",
      "Iteration 1255, loss = 0.18629548\n",
      "Iteration 1256, loss = 0.18607825\n",
      "Iteration 1257, loss = 0.18586299\n",
      "Iteration 1258, loss = 0.18564969\n",
      "Iteration 1259, loss = 0.18543832\n",
      "Iteration 1260, loss = 0.18522887\n",
      "Iteration 1261, loss = 0.18502131\n",
      "Iteration 1262, loss = 0.18481561\n",
      "Iteration 1263, loss = 0.18461177\n",
      "Iteration 1264, loss = 0.18440976\n",
      "Iteration 1265, loss = 0.18420956\n",
      "Iteration 1266, loss = 0.18401114\n",
      "Iteration 1267, loss = 0.18381450\n",
      "Iteration 1268, loss = 0.18361961\n",
      "Iteration 1269, loss = 0.18342645\n",
      "Iteration 1270, loss = 0.18323500\n",
      "Iteration 1271, loss = 0.18304525\n",
      "Iteration 1272, loss = 0.18285717\n",
      "Iteration 1273, loss = 0.18267075\n",
      "Iteration 1274, loss = 0.18248596\n",
      "Iteration 1275, loss = 0.18230280\n",
      "Iteration 1276, loss = 0.18212123\n",
      "Iteration 1277, loss = 0.18194125\n",
      "Iteration 1278, loss = 0.18176284\n",
      "Iteration 1279, loss = 0.18158597\n",
      "Iteration 1280, loss = 0.18141064\n",
      "Iteration 1281, loss = 0.18123682\n",
      "Iteration 1282, loss = 0.18106450\n",
      "Iteration 1283, loss = 0.18089365\n",
      "Iteration 1284, loss = 0.18072427\n",
      "Iteration 1285, loss = 0.18055634\n",
      "Iteration 1286, loss = 0.18038984\n",
      "Iteration 1287, loss = 0.18022475\n",
      "Iteration 1288, loss = 0.18006106\n",
      "Iteration 1289, loss = 0.17989876\n",
      "Iteration 1290, loss = 0.17973782\n",
      "Iteration 1291, loss = 0.17957823\n",
      "Iteration 1292, loss = 0.17941998\n",
      "Iteration 1293, loss = 0.17926305\n",
      "Iteration 1294, loss = 0.17910742\n",
      "Iteration 1295, loss = 0.17895309\n",
      "Iteration 1296, loss = 0.17880003\n",
      "Iteration 1297, loss = 0.17864823\n",
      "Iteration 1298, loss = 0.17849768\n",
      "Iteration 1299, loss = 0.17834836\n",
      "Iteration 1300, loss = 0.17820027\n",
      "Iteration 1301, loss = 0.17805337\n",
      "Iteration 1302, loss = 0.17790767\n",
      "Iteration 1303, loss = 0.17776314\n",
      "Iteration 1304, loss = 0.17761978\n",
      "Iteration 1305, loss = 0.17747756\n",
      "Iteration 1306, loss = 0.17733648\n",
      "Iteration 1307, loss = 0.17719653\n",
      "Iteration 1308, loss = 0.17705768\n",
      "Iteration 1309, loss = 0.17691994\n",
      "Iteration 1310, loss = 0.17678327\n",
      "Iteration 1311, loss = 0.17664768\n",
      "Iteration 1312, loss = 0.17651315\n",
      "Iteration 1313, loss = 0.17637967\n",
      "Iteration 1314, loss = 0.17624722\n",
      "Iteration 1315, loss = 0.17611579\n",
      "Iteration 1316, loss = 0.17598537\n",
      "Iteration 1317, loss = 0.17585595\n",
      "Iteration 1318, loss = 0.17572752\n",
      "Iteration 1319, loss = 0.17560006\n",
      "Iteration 1320, loss = 0.17547357\n",
      "Iteration 1321, loss = 0.17534803\n",
      "Iteration 1322, loss = 0.17522343\n",
      "Iteration 1323, loss = 0.17509976\n",
      "Iteration 1324, loss = 0.17497701\n",
      "Iteration 1325, loss = 0.17485516\n",
      "Iteration 1326, loss = 0.17473422\n",
      "Iteration 1327, loss = 0.17461416\n",
      "Iteration 1328, loss = 0.17449497\n",
      "Iteration 1329, loss = 0.17437666\n",
      "Iteration 1330, loss = 0.17425919\n",
      "Iteration 1331, loss = 0.17414257\n",
      "Iteration 1332, loss = 0.17402679\n",
      "Iteration 1333, loss = 0.17391183\n",
      "Iteration 1334, loss = 0.17379769\n",
      "Iteration 1335, loss = 0.17368435\n",
      "Iteration 1336, loss = 0.17357180\n",
      "Iteration 1337, loss = 0.17346004\n",
      "Iteration 1338, loss = 0.17334906\n",
      "Iteration 1339, loss = 0.17323884\n",
      "Iteration 1340, loss = 0.17312939\n",
      "Iteration 1341, loss = 0.17302068\n",
      "Iteration 1342, loss = 0.17291271\n",
      "Iteration 1343, loss = 0.17280547\n",
      "Iteration 1344, loss = 0.17269895\n",
      "Iteration 1345, loss = 0.17259314\n",
      "Iteration 1346, loss = 0.17248804\n",
      "Iteration 1347, loss = 0.17238363\n",
      "Iteration 1348, loss = 0.17227991\n",
      "Iteration 1349, loss = 0.17217687\n",
      "Iteration 1350, loss = 0.17207450\n",
      "Iteration 1351, loss = 0.17197279\n",
      "Iteration 1352, loss = 0.17187173\n",
      "Iteration 1353, loss = 0.17177132\n",
      "Iteration 1354, loss = 0.17167154\n",
      "Iteration 1355, loss = 0.17157240\n",
      "Iteration 1356, loss = 0.17147388\n",
      "Iteration 1357, loss = 0.17137597\n",
      "Iteration 1358, loss = 0.17127867\n",
      "Iteration 1359, loss = 0.17118197\n",
      "Iteration 1360, loss = 0.17108586\n",
      "Iteration 1361, loss = 0.17099034\n",
      "Iteration 1362, loss = 0.17089539\n",
      "Iteration 1363, loss = 0.17080101\n",
      "Iteration 1364, loss = 0.17070720\n",
      "Iteration 1365, loss = 0.17061394\n",
      "Iteration 1366, loss = 0.17052123\n",
      "Iteration 1367, loss = 0.17042906\n",
      "Iteration 1368, loss = 0.17033743\n",
      "Iteration 1369, loss = 0.17024633\n",
      "Iteration 1370, loss = 0.17015575\n",
      "Iteration 1371, loss = 0.17006568\n",
      "Iteration 1372, loss = 0.16997612\n",
      "Iteration 1373, loss = 0.16988707\n",
      "Iteration 1374, loss = 0.16979851\n",
      "Iteration 1375, loss = 0.16971044\n",
      "Iteration 1376, loss = 0.16962285\n",
      "Iteration 1377, loss = 0.16953574\n",
      "Iteration 1378, loss = 0.16944911\n",
      "Iteration 1379, loss = 0.16936293\n",
      "Iteration 1380, loss = 0.16927722\n",
      "Iteration 1381, loss = 0.16919196\n",
      "Iteration 1382, loss = 0.16910715\n",
      "Iteration 1383, loss = 0.16902278\n",
      "Iteration 1384, loss = 0.16893885\n",
      "Iteration 1385, loss = 0.16885535\n",
      "Iteration 1386, loss = 0.16877228\n",
      "Iteration 1387, loss = 0.16868962\n",
      "Iteration 1388, loss = 0.16860738\n",
      "Iteration 1389, loss = 0.16852555\n",
      "Iteration 1390, loss = 0.16844412\n",
      "Iteration 1391, loss = 0.16836309\n",
      "Iteration 1392, loss = 0.16828246\n",
      "Iteration 1393, loss = 0.16820221\n",
      "Iteration 1394, loss = 0.16812235\n",
      "Iteration 1395, loss = 0.16804286\n",
      "Iteration 1396, loss = 0.16796375\n",
      "Iteration 1397, loss = 0.16788501\n",
      "Iteration 1398, loss = 0.16780663\n",
      "Iteration 1399, loss = 0.16772861\n",
      "Iteration 1400, loss = 0.16765095\n",
      "Iteration 1401, loss = 0.16757363\n",
      "Iteration 1402, loss = 0.16749666\n",
      "Iteration 1403, loss = 0.16742004\n",
      "Iteration 1404, loss = 0.16734374\n",
      "Iteration 1405, loss = 0.16726778\n",
      "Iteration 1406, loss = 0.16719215\n",
      "Iteration 1407, loss = 0.16711684\n",
      "Iteration 1408, loss = 0.16704185\n",
      "Iteration 1409, loss = 0.16696718\n",
      "Iteration 1410, loss = 0.16689281\n",
      "Iteration 1411, loss = 0.16681876\n",
      "Iteration 1412, loss = 0.16674500\n",
      "Iteration 1413, loss = 0.16667155\n",
      "Iteration 1414, loss = 0.16659838\n",
      "Iteration 1415, loss = 0.16652551\n",
      "Iteration 1416, loss = 0.16645293\n",
      "Iteration 1417, loss = 0.16638063\n",
      "Iteration 1418, loss = 0.16630861\n",
      "Iteration 1419, loss = 0.16623686\n",
      "Iteration 1420, loss = 0.16616539\n",
      "Iteration 1421, loss = 0.16609418\n",
      "Iteration 1422, loss = 0.16602324\n",
      "Iteration 1423, loss = 0.16595256\n",
      "Iteration 1424, loss = 0.16588214\n",
      "Iteration 1425, loss = 0.16581197\n",
      "Iteration 1426, loss = 0.16574205\n",
      "Iteration 1427, loss = 0.16567238\n",
      "Iteration 1428, loss = 0.16560295\n",
      "Iteration 1429, loss = 0.16553377\n",
      "Iteration 1430, loss = 0.16546482\n",
      "Iteration 1431, loss = 0.16539611\n",
      "Iteration 1432, loss = 0.16532763\n",
      "Iteration 1433, loss = 0.16525937\n",
      "Iteration 1434, loss = 0.16519134\n",
      "Iteration 1435, loss = 0.16512353\n",
      "Iteration 1436, loss = 0.16505594\n",
      "Iteration 1437, loss = 0.16498857\n",
      "Iteration 1438, loss = 0.16492141\n",
      "Iteration 1439, loss = 0.16485446\n",
      "Iteration 1440, loss = 0.16478772\n",
      "Iteration 1441, loss = 0.16472118\n",
      "Iteration 1442, loss = 0.16465484\n",
      "Iteration 1443, loss = 0.16458870\n",
      "Iteration 1444, loss = 0.16452276\n",
      "Iteration 1445, loss = 0.16445701\n",
      "Iteration 1446, loss = 0.16439145\n",
      "Iteration 1447, loss = 0.16432608\n",
      "Iteration 1448, loss = 0.16426090\n",
      "Iteration 1449, loss = 0.16419589\n",
      "Iteration 1450, loss = 0.16413107\n",
      "Iteration 1451, loss = 0.16406643\n",
      "Iteration 1452, loss = 0.16400196\n",
      "Iteration 1453, loss = 0.16393766\n",
      "Iteration 1454, loss = 0.16387354\n",
      "Iteration 1455, loss = 0.16380958\n",
      "Iteration 1456, loss = 0.16374579\n",
      "Iteration 1457, loss = 0.16368216\n",
      "Iteration 1458, loss = 0.16361870\n",
      "Iteration 1459, loss = 0.16355539\n",
      "Iteration 1460, loss = 0.16349224\n",
      "Iteration 1461, loss = 0.16342924\n",
      "Iteration 1462, loss = 0.16336640\n",
      "Iteration 1463, loss = 0.16330370\n",
      "Iteration 1464, loss = 0.16324116\n",
      "Iteration 1465, loss = 0.16317876\n",
      "Iteration 1466, loss = 0.16311650\n",
      "Iteration 1467, loss = 0.16305439\n",
      "Iteration 1468, loss = 0.16299242\n",
      "Iteration 1469, loss = 0.16293058\n",
      "Iteration 1470, loss = 0.16286888\n",
      "Iteration 1471, loss = 0.16280731\n",
      "Iteration 1472, loss = 0.16274588\n",
      "Iteration 1473, loss = 0.16268458\n",
      "Iteration 1474, loss = 0.16262340\n",
      "Iteration 1475, loss = 0.16256236\n",
      "Iteration 1476, loss = 0.16250143\n",
      "Iteration 1477, loss = 0.16244063\n",
      "Iteration 1478, loss = 0.16237995\n",
      "Iteration 1479, loss = 0.16231939\n",
      "Iteration 1480, loss = 0.16225895\n",
      "Iteration 1481, loss = 0.16219863\n",
      "Iteration 1482, loss = 0.16213841\n",
      "Iteration 1483, loss = 0.16207832\n",
      "Iteration 1484, loss = 0.16201833\n",
      "Iteration 1485, loss = 0.16195845\n",
      "Iteration 1486, loss = 0.16189868\n",
      "Iteration 1487, loss = 0.16183902\n",
      "Iteration 1488, loss = 0.16177946\n",
      "Iteration 1489, loss = 0.16172001\n",
      "Iteration 1490, loss = 0.16166065\n",
      "Iteration 1491, loss = 0.16160140\n",
      "Iteration 1492, loss = 0.16154225\n",
      "Iteration 1493, loss = 0.16148319\n",
      "Iteration 1494, loss = 0.16142423\n",
      "Iteration 1495, loss = 0.16136536\n",
      "Iteration 1496, loss = 0.16130659\n",
      "Iteration 1497, loss = 0.16124791\n",
      "Iteration 1498, loss = 0.16118932\n",
      "Iteration 1499, loss = 0.16113082\n",
      "Iteration 1500, loss = 0.16107241\n",
      "Iteration 1501, loss = 0.16101408\n",
      "Iteration 1502, loss = 0.16095584\n",
      "Iteration 1503, loss = 0.16089768\n",
      "Iteration 1504, loss = 0.16083961\n",
      "Iteration 1505, loss = 0.16078162\n",
      "Iteration 1506, loss = 0.16072370\n",
      "Iteration 1507, loss = 0.16066587\n",
      "Iteration 1508, loss = 0.16060812\n",
      "Iteration 1509, loss = 0.16055044\n",
      "Iteration 1510, loss = 0.16049284\n",
      "Iteration 1511, loss = 0.16043531\n",
      "Iteration 1512, loss = 0.16037786\n",
      "Iteration 1513, loss = 0.16032047\n",
      "Iteration 1514, loss = 0.16026316\n",
      "Iteration 1515, loss = 0.16020592\n",
      "Iteration 1516, loss = 0.16014875\n",
      "Iteration 1517, loss = 0.16009165\n",
      "Iteration 1518, loss = 0.16003461\n",
      "Iteration 1519, loss = 0.15997764\n",
      "Iteration 1520, loss = 0.15992074\n",
      "Iteration 1521, loss = 0.15986390\n",
      "Iteration 1522, loss = 0.15980712\n",
      "Iteration 1523, loss = 0.15975041\n",
      "Iteration 1524, loss = 0.15969376\n",
      "Iteration 1525, loss = 0.15963716\n",
      "Iteration 1526, loss = 0.15958063\n",
      "Iteration 1527, loss = 0.15952415\n",
      "Iteration 1528, loss = 0.15946774\n",
      "Iteration 1529, loss = 0.15941138\n",
      "Iteration 1530, loss = 0.15935507\n",
      "Iteration 1531, loss = 0.15929882\n",
      "Iteration 1532, loss = 0.15924262\n",
      "Iteration 1533, loss = 0.15918648\n",
      "Iteration 1534, loss = 0.15913039\n",
      "Iteration 1535, loss = 0.15907436\n",
      "Iteration 1536, loss = 0.15901837\n",
      "Iteration 1537, loss = 0.15896243\n",
      "Iteration 1538, loss = 0.15890654\n",
      "Iteration 1539, loss = 0.15885071\n",
      "Iteration 1540, loss = 0.15879492\n",
      "Iteration 1541, loss = 0.15873917\n",
      "Iteration 1542, loss = 0.15868347\n",
      "Iteration 1543, loss = 0.15862782\n",
      "Iteration 1544, loss = 0.15857222\n",
      "Iteration 1545, loss = 0.15851665\n",
      "Iteration 1546, loss = 0.15846114\n",
      "Iteration 1547, loss = 0.15840566\n",
      "Iteration 1548, loss = 0.15835023\n",
      "Iteration 1549, loss = 0.15829484\n",
      "Iteration 1550, loss = 0.15823949\n",
      "Iteration 1551, loss = 0.15818417\n",
      "Iteration 1552, loss = 0.15812890\n",
      "Iteration 1553, loss = 0.15807367\n",
      "Iteration 1554, loss = 0.15801848\n",
      "Iteration 1555, loss = 0.15796332\n",
      "Iteration 1556, loss = 0.15790821\n",
      "Iteration 1557, loss = 0.15785313\n",
      "Iteration 1558, loss = 0.15779808\n",
      "Iteration 1559, loss = 0.15774307\n",
      "Iteration 1560, loss = 0.15768810\n",
      "Iteration 1561, loss = 0.15763316\n",
      "Iteration 1562, loss = 0.15757825\n",
      "Iteration 1563, loss = 0.15752338\n",
      "Iteration 1564, loss = 0.15746854\n",
      "Iteration 1565, loss = 0.15741373\n",
      "Iteration 1566, loss = 0.15735895\n",
      "Iteration 1567, loss = 0.15730421\n",
      "Iteration 1568, loss = 0.15724950\n",
      "Iteration 1569, loss = 0.15719481\n",
      "Iteration 1570, loss = 0.15714016\n",
      "Iteration 1571, loss = 0.15708553\n",
      "Iteration 1572, loss = 0.15703094\n",
      "Iteration 1573, loss = 0.15697637\n",
      "Iteration 1574, loss = 0.15692183\n",
      "Iteration 1575, loss = 0.15686737\n",
      "Iteration 1576, loss = 0.15681301\n",
      "Iteration 1577, loss = 0.15675868\n",
      "Iteration 1578, loss = 0.15670438\n",
      "Iteration 1579, loss = 0.15665012\n",
      "Iteration 1580, loss = 0.15659590\n",
      "Iteration 1581, loss = 0.15654170\n",
      "Iteration 1582, loss = 0.15648754\n",
      "Iteration 1583, loss = 0.15643340\n",
      "Iteration 1584, loss = 0.15637930\n",
      "Iteration 1585, loss = 0.15632522\n",
      "Iteration 1586, loss = 0.15627118\n",
      "Iteration 1587, loss = 0.15621715\n",
      "Iteration 1588, loss = 0.15616316\n",
      "Iteration 1589, loss = 0.15610919\n",
      "Iteration 1590, loss = 0.15605525\n",
      "Iteration 1591, loss = 0.15600133\n",
      "Iteration 1592, loss = 0.15594743\n",
      "Iteration 1593, loss = 0.15589356\n",
      "Iteration 1594, loss = 0.15583971\n",
      "Iteration 1595, loss = 0.15578588\n",
      "Iteration 1596, loss = 0.15573208\n",
      "Iteration 1597, loss = 0.15567829\n",
      "Iteration 1598, loss = 0.15562453\n",
      "Iteration 1599, loss = 0.15557079\n",
      "Iteration 1600, loss = 0.15551707\n",
      "Iteration 1601, loss = 0.15546337\n",
      "Iteration 1602, loss = 0.15540969\n",
      "Iteration 1603, loss = 0.15535603\n",
      "Iteration 1604, loss = 0.15530239\n",
      "Iteration 1605, loss = 0.15524876\n",
      "Iteration 1606, loss = 0.15519516\n",
      "Iteration 1607, loss = 0.15514157\n",
      "Iteration 1608, loss = 0.15508800\n",
      "Iteration 1609, loss = 0.15503445\n",
      "Iteration 1610, loss = 0.15498091\n",
      "Iteration 1611, loss = 0.15492739\n",
      "Iteration 1612, loss = 0.15487389\n",
      "Iteration 1613, loss = 0.15482040\n",
      "Iteration 1614, loss = 0.15476693\n",
      "Iteration 1615, loss = 0.15471348\n",
      "Iteration 1616, loss = 0.15466004\n",
      "Iteration 1617, loss = 0.15460662\n",
      "Iteration 1618, loss = 0.15455321\n",
      "Iteration 1619, loss = 0.15449982\n",
      "Iteration 1620, loss = 0.15444644\n",
      "Iteration 1621, loss = 0.15439308\n",
      "Iteration 1622, loss = 0.15433973\n",
      "Iteration 1623, loss = 0.15428639\n",
      "Iteration 1624, loss = 0.15423307\n",
      "Iteration 1625, loss = 0.15417976\n",
      "Iteration 1626, loss = 0.15412647\n",
      "Iteration 1627, loss = 0.15407319\n",
      "Iteration 1628, loss = 0.15401992\n",
      "Iteration 1629, loss = 0.15396666\n",
      "Iteration 1630, loss = 0.15391342\n",
      "Iteration 1631, loss = 0.15386019\n",
      "Iteration 1632, loss = 0.15380697\n",
      "Iteration 1633, loss = 0.15375377\n",
      "Iteration 1634, loss = 0.15370057\n",
      "Iteration 1635, loss = 0.15364739\n",
      "Iteration 1636, loss = 0.15359422\n",
      "Iteration 1637, loss = 0.15354106\n",
      "Iteration 1638, loss = 0.15348791\n",
      "Iteration 1639, loss = 0.15343478\n",
      "Iteration 1640, loss = 0.15338165\n",
      "Iteration 1641, loss = 0.15332853\n",
      "Iteration 1642, loss = 0.15327543\n",
      "Iteration 1643, loss = 0.15322234\n",
      "Iteration 1644, loss = 0.15316925\n",
      "Iteration 1645, loss = 0.15311618\n",
      "Iteration 1646, loss = 0.15306312\n",
      "Iteration 1647, loss = 0.15301007\n",
      "Iteration 1648, loss = 0.15295702\n",
      "Iteration 1649, loss = 0.15290399\n",
      "Iteration 1650, loss = 0.15285097\n",
      "Iteration 1651, loss = 0.15279795\n",
      "Iteration 1652, loss = 0.15274495\n",
      "Iteration 1653, loss = 0.15269195\n",
      "Iteration 1654, loss = 0.15263897\n",
      "Iteration 1655, loss = 0.15258599\n",
      "Iteration 1656, loss = 0.15253302\n",
      "Iteration 1657, loss = 0.15248006\n",
      "Iteration 1658, loss = 0.15242711\n",
      "Iteration 1659, loss = 0.15237417\n",
      "Iteration 1660, loss = 0.15232124\n",
      "Iteration 1661, loss = 0.15226831\n",
      "Iteration 1662, loss = 0.15221540\n",
      "Iteration 1663, loss = 0.15216249\n",
      "Iteration 1664, loss = 0.15210959\n",
      "Iteration 1665, loss = 0.15205670\n",
      "Iteration 1666, loss = 0.15200382\n",
      "Iteration 1667, loss = 0.15195094\n",
      "Iteration 1668, loss = 0.15189807\n",
      "Iteration 1669, loss = 0.15184521\n",
      "Iteration 1670, loss = 0.15179236\n",
      "Iteration 1671, loss = 0.15173952\n",
      "Iteration 1672, loss = 0.15168668\n",
      "Iteration 1673, loss = 0.15163385\n",
      "Iteration 1674, loss = 0.15158103\n",
      "Iteration 1675, loss = 0.15152821\n",
      "Iteration 1676, loss = 0.15147540\n",
      "Iteration 1677, loss = 0.15142260\n",
      "Iteration 1678, loss = 0.15136981\n",
      "Iteration 1679, loss = 0.15131702\n",
      "Iteration 1680, loss = 0.15126425\n",
      "Iteration 1681, loss = 0.15121147\n",
      "Iteration 1682, loss = 0.15115871\n",
      "Iteration 1683, loss = 0.15110595\n",
      "Iteration 1684, loss = 0.15105320\n",
      "Iteration 1685, loss = 0.15100045\n",
      "Iteration 1686, loss = 0.15094771\n",
      "Iteration 1687, loss = 0.15089498\n",
      "Iteration 1688, loss = 0.15084226\n",
      "Iteration 1689, loss = 0.15078954\n",
      "Iteration 1690, loss = 0.15073683\n",
      "Iteration 1691, loss = 0.15068412\n",
      "Iteration 1692, loss = 0.15063142\n",
      "Iteration 1693, loss = 0.15057873\n",
      "Iteration 1694, loss = 0.15052604\n",
      "Iteration 1695, loss = 0.15047336\n",
      "Iteration 1696, loss = 0.15042069\n",
      "Iteration 1697, loss = 0.15036802\n",
      "Iteration 1698, loss = 0.15031536\n",
      "Iteration 1699, loss = 0.15026270\n",
      "Iteration 1700, loss = 0.15021005\n",
      "Iteration 1701, loss = 0.15015741\n",
      "Iteration 1702, loss = 0.15010477\n",
      "Iteration 1703, loss = 0.15005214\n",
      "Iteration 1704, loss = 0.14999951\n",
      "Iteration 1705, loss = 0.14994689\n",
      "Iteration 1706, loss = 0.14989428\n",
      "Iteration 1707, loss = 0.14984167\n",
      "Iteration 1708, loss = 0.14978907\n",
      "Iteration 1709, loss = 0.14973647\n",
      "Iteration 1710, loss = 0.14968388\n",
      "Iteration 1711, loss = 0.14963130\n",
      "Iteration 1712, loss = 0.14957872\n",
      "Iteration 1713, loss = 0.14952615\n",
      "Iteration 1714, loss = 0.14947358\n",
      "Iteration 1715, loss = 0.14942102\n",
      "Iteration 1716, loss = 0.14936846\n",
      "Iteration 1717, loss = 0.14931591\n",
      "Iteration 1718, loss = 0.14926337\n",
      "Iteration 1719, loss = 0.14921083\n",
      "Iteration 1720, loss = 0.14915829\n",
      "Iteration 1721, loss = 0.14910577\n",
      "Iteration 1722, loss = 0.14905325\n",
      "Iteration 1723, loss = 0.14900073\n",
      "Iteration 1724, loss = 0.14894822\n",
      "Iteration 1725, loss = 0.14889571\n",
      "Iteration 1726, loss = 0.14884321\n",
      "Iteration 1727, loss = 0.14879072\n",
      "Iteration 1728, loss = 0.14873823\n",
      "Iteration 1729, loss = 0.14868575\n",
      "Iteration 1730, loss = 0.14863327\n",
      "Iteration 1731, loss = 0.14858080\n",
      "Iteration 1732, loss = 0.14852834\n",
      "Iteration 1733, loss = 0.14847588\n",
      "Iteration 1734, loss = 0.14842342\n",
      "Iteration 1735, loss = 0.14837097\n",
      "Iteration 1736, loss = 0.14831853\n",
      "Iteration 1737, loss = 0.14826609\n",
      "Iteration 1738, loss = 0.14821366\n",
      "Iteration 1739, loss = 0.14816123\n",
      "Iteration 1740, loss = 0.14810881\n",
      "Iteration 1741, loss = 0.14805640\n",
      "Iteration 1742, loss = 0.14800399\n",
      "Iteration 1743, loss = 0.14795158\n",
      "Iteration 1744, loss = 0.14789918\n",
      "Iteration 1745, loss = 0.14784679\n",
      "Iteration 1746, loss = 0.14779440\n",
      "Iteration 1747, loss = 0.14774202\n",
      "Iteration 1748, loss = 0.14768965\n",
      "Iteration 1749, loss = 0.14763727\n",
      "Iteration 1750, loss = 0.14758491\n",
      "Iteration 1751, loss = 0.14753255\n",
      "Iteration 1752, loss = 0.14748020\n",
      "Iteration 1753, loss = 0.14742785\n",
      "Iteration 1754, loss = 0.14737551\n",
      "Iteration 1755, loss = 0.14732317\n",
      "Iteration 1756, loss = 0.14727084\n",
      "Iteration 1757, loss = 0.14721852\n",
      "Iteration 1758, loss = 0.14716620\n",
      "Iteration 1759, loss = 0.14711389\n",
      "Iteration 1760, loss = 0.14706158\n",
      "Iteration 1761, loss = 0.14700928\n",
      "Iteration 1762, loss = 0.14695698\n",
      "Iteration 1763, loss = 0.14690469\n",
      "Iteration 1764, loss = 0.14685241\n",
      "Iteration 1765, loss = 0.14680013\n",
      "Iteration 1766, loss = 0.14674786\n",
      "Iteration 1767, loss = 0.14669559\n",
      "Iteration 1768, loss = 0.14664333\n",
      "Iteration 1769, loss = 0.14659108\n",
      "Iteration 1770, loss = 0.14653883\n",
      "Iteration 1771, loss = 0.14648659\n",
      "Iteration 1772, loss = 0.14643435\n",
      "Iteration 1773, loss = 0.14638212\n",
      "Iteration 1774, loss = 0.14632990\n",
      "Iteration 1775, loss = 0.14627768\n",
      "Iteration 1776, loss = 0.14622547\n",
      "Iteration 1777, loss = 0.14617326\n",
      "Iteration 1778, loss = 0.14612106\n",
      "Iteration 1779, loss = 0.14606887\n",
      "Iteration 1780, loss = 0.14601669\n",
      "Iteration 1781, loss = 0.14596450\n",
      "Iteration 1782, loss = 0.14591233\n",
      "Iteration 1783, loss = 0.14586016\n",
      "Iteration 1784, loss = 0.14580800\n",
      "Iteration 1785, loss = 0.14575585\n",
      "Iteration 1786, loss = 0.14570370\n",
      "Iteration 1787, loss = 0.14565155\n",
      "Iteration 1788, loss = 0.14559942\n",
      "Iteration 1789, loss = 0.14554729\n",
      "Iteration 1790, loss = 0.14549517\n",
      "Iteration 1791, loss = 0.14544305\n",
      "Iteration 1792, loss = 0.14539094\n",
      "Iteration 1793, loss = 0.14533884\n",
      "Iteration 1794, loss = 0.14528674\n",
      "Iteration 1795, loss = 0.14523465\n",
      "Iteration 1796, loss = 0.14518257\n",
      "Iteration 1797, loss = 0.14513049\n",
      "Iteration 1798, loss = 0.14507842\n",
      "Iteration 1799, loss = 0.14502636\n",
      "Iteration 1800, loss = 0.14497430\n",
      "Iteration 1801, loss = 0.14492225\n",
      "Iteration 1802, loss = 0.14487021\n",
      "Iteration 1803, loss = 0.14481818\n",
      "Iteration 1804, loss = 0.14476615\n",
      "Iteration 1805, loss = 0.14471413\n",
      "Iteration 1806, loss = 0.14466211\n",
      "Iteration 1807, loss = 0.14461010\n",
      "Iteration 1808, loss = 0.14455810\n",
      "Iteration 1809, loss = 0.14450611\n",
      "Iteration 1810, loss = 0.14445412\n",
      "Iteration 1811, loss = 0.14440215\n",
      "Iteration 1812, loss = 0.14435017\n",
      "Iteration 1813, loss = 0.14429821\n",
      "Iteration 1814, loss = 0.14424625\n",
      "Iteration 1815, loss = 0.14419430\n",
      "Iteration 1816, loss = 0.14414236\n",
      "Iteration 1817, loss = 0.14409043\n",
      "Iteration 1818, loss = 0.14403850\n",
      "Iteration 1819, loss = 0.14398658\n",
      "Iteration 1820, loss = 0.14393467\n",
      "Iteration 1821, loss = 0.14388276\n",
      "Iteration 1822, loss = 0.14383086\n",
      "Iteration 1823, loss = 0.14377897\n",
      "Iteration 1824, loss = 0.14372709\n",
      "Iteration 1825, loss = 0.14367522\n",
      "Iteration 1826, loss = 0.14362335\n",
      "Iteration 1827, loss = 0.14357149\n",
      "Iteration 1828, loss = 0.14351964\n",
      "Iteration 1829, loss = 0.14346780\n",
      "Iteration 1830, loss = 0.14341596\n",
      "Iteration 1831, loss = 0.14336414\n",
      "Iteration 1832, loss = 0.14331232\n",
      "Iteration 1833, loss = 0.14326051\n",
      "Iteration 1834, loss = 0.14320870\n",
      "Iteration 1835, loss = 0.14315691\n",
      "Iteration 1836, loss = 0.14310512\n",
      "Iteration 1837, loss = 0.14305334\n",
      "Iteration 1838, loss = 0.14300157\n",
      "Iteration 1839, loss = 0.14294981\n",
      "Iteration 1840, loss = 0.14289806\n",
      "Iteration 1841, loss = 0.14284631\n",
      "Iteration 1842, loss = 0.14279458\n",
      "Iteration 1843, loss = 0.14274285\n",
      "Iteration 1844, loss = 0.14269113\n",
      "Iteration 1845, loss = 0.14263942\n",
      "Iteration 1846, loss = 0.14258771\n",
      "Iteration 1847, loss = 0.14253602\n",
      "Iteration 1848, loss = 0.14248433\n",
      "Iteration 1849, loss = 0.14243266\n",
      "Iteration 1850, loss = 0.14238099\n",
      "Iteration 1851, loss = 0.14232933\n",
      "Iteration 1852, loss = 0.14227768\n",
      "Iteration 1853, loss = 0.14222604\n",
      "Iteration 1854, loss = 0.14217440\n",
      "Iteration 1855, loss = 0.14212278\n",
      "Iteration 1856, loss = 0.14207117\n",
      "Iteration 1857, loss = 0.14201956\n",
      "Iteration 1858, loss = 0.14196796\n",
      "Iteration 1859, loss = 0.14191638\n",
      "Iteration 1860, loss = 0.14186480\n",
      "Iteration 1861, loss = 0.14181323\n",
      "Iteration 1862, loss = 0.14176167\n",
      "Iteration 1863, loss = 0.14171012\n",
      "Iteration 1864, loss = 0.14165858\n",
      "Iteration 1865, loss = 0.14160705\n",
      "Iteration 1866, loss = 0.14155553\n",
      "Iteration 1867, loss = 0.14150401\n",
      "Iteration 1868, loss = 0.14145251\n",
      "Iteration 1869, loss = 0.14140102\n",
      "Iteration 1870, loss = 0.14134954\n",
      "Iteration 1871, loss = 0.14129806\n",
      "Iteration 1872, loss = 0.14124660\n",
      "Iteration 1873, loss = 0.14119514\n",
      "Iteration 1874, loss = 0.14114370\n",
      "Iteration 1875, loss = 0.14109226\n",
      "Iteration 1876, loss = 0.14104084\n",
      "Iteration 1877, loss = 0.14098943\n",
      "Iteration 1878, loss = 0.14093802\n",
      "Iteration 1879, loss = 0.14088663\n",
      "Iteration 1880, loss = 0.14083524\n",
      "Iteration 1881, loss = 0.14078387\n",
      "Iteration 1882, loss = 0.14073250\n",
      "Iteration 1883, loss = 0.14068115\n",
      "Iteration 1884, loss = 0.14062981\n",
      "Iteration 1885, loss = 0.14057847\n",
      "Iteration 1886, loss = 0.14052715\n",
      "Iteration 1887, loss = 0.14047584\n",
      "Iteration 1888, loss = 0.14042454\n",
      "Iteration 1889, loss = 0.14037325\n",
      "Iteration 1890, loss = 0.14032197\n",
      "Iteration 1891, loss = 0.14027070\n",
      "Iteration 1892, loss = 0.14021944\n",
      "Iteration 1893, loss = 0.14016819\n",
      "Iteration 1894, loss = 0.14011695\n",
      "Iteration 1895, loss = 0.14006573\n",
      "Iteration 1896, loss = 0.14001451\n",
      "Iteration 1897, loss = 0.13996389\n",
      "Iteration 1898, loss = 0.13991339\n",
      "Iteration 1899, loss = 0.13986295\n",
      "Iteration 1900, loss = 0.13981257\n",
      "Iteration 1901, loss = 0.13976224\n",
      "Iteration 1902, loss = 0.13971195\n",
      "Iteration 1903, loss = 0.13966171\n",
      "Iteration 1904, loss = 0.13961151\n",
      "Iteration 1905, loss = 0.13956135\n",
      "Iteration 1906, loss = 0.13951123\n",
      "Iteration 1907, loss = 0.13946114\n",
      "Iteration 1908, loss = 0.13941108\n",
      "Iteration 1909, loss = 0.13936105\n",
      "Iteration 1910, loss = 0.13931106\n",
      "Iteration 1911, loss = 0.13926108\n",
      "Iteration 1912, loss = 0.13921114\n",
      "Iteration 1913, loss = 0.13916122\n",
      "Iteration 1914, loss = 0.13911132\n",
      "Iteration 1915, loss = 0.13906145\n",
      "Iteration 1916, loss = 0.13901160\n",
      "Iteration 1917, loss = 0.13896177\n",
      "Iteration 1918, loss = 0.13891196\n",
      "Iteration 1919, loss = 0.13886218\n",
      "Iteration 1920, loss = 0.13881241\n",
      "Iteration 1921, loss = 0.13876266\n",
      "Iteration 1922, loss = 0.13871294\n",
      "Iteration 1923, loss = 0.13866323\n",
      "Iteration 1924, loss = 0.13861354\n",
      "Iteration 1925, loss = 0.13856386\n",
      "Iteration 1926, loss = 0.13851421\n",
      "Iteration 1927, loss = 0.13846457\n",
      "Iteration 1928, loss = 0.13841495\n",
      "Iteration 1929, loss = 0.13836535\n",
      "Iteration 1930, loss = 0.13831576\n",
      "Iteration 1931, loss = 0.13826620\n",
      "Iteration 1932, loss = 0.13821665\n",
      "Iteration 1933, loss = 0.13816711\n",
      "Iteration 1934, loss = 0.13811759\n",
      "Iteration 1935, loss = 0.13806809\n",
      "Iteration 1936, loss = 0.13801861\n",
      "Iteration 1937, loss = 0.13796914\n",
      "Iteration 1938, loss = 0.13791969\n",
      "Iteration 1939, loss = 0.13787026\n",
      "Iteration 1940, loss = 0.13782084\n",
      "Iteration 1941, loss = 0.13777144\n",
      "Iteration 1942, loss = 0.13772206\n",
      "Iteration 1943, loss = 0.13767269\n",
      "Iteration 1944, loss = 0.13762334\n",
      "Iteration 1945, loss = 0.13757400\n",
      "Iteration 1946, loss = 0.13752469\n",
      "Iteration 1947, loss = 0.13747538\n",
      "Iteration 1948, loss = 0.13742610\n",
      "Iteration 1949, loss = 0.13737683\n",
      "Iteration 1950, loss = 0.13732758\n",
      "Iteration 1951, loss = 0.13727835\n",
      "Iteration 1952, loss = 0.13722913\n",
      "Iteration 1953, loss = 0.13717993\n",
      "Iteration 1954, loss = 0.13713074\n",
      "Iteration 1955, loss = 0.13708157\n",
      "Iteration 1956, loss = 0.13703242\n",
      "Iteration 1957, loss = 0.13698329\n",
      "Iteration 1958, loss = 0.13693417\n",
      "Iteration 1959, loss = 0.13688507\n",
      "Iteration 1960, loss = 0.13683599\n",
      "Iteration 1961, loss = 0.13678693\n",
      "Iteration 1962, loss = 0.13673788\n",
      "Iteration 1963, loss = 0.13668885\n",
      "Iteration 1964, loss = 0.13663983\n",
      "Iteration 1965, loss = 0.13659083\n",
      "Iteration 1966, loss = 0.13654186\n",
      "Iteration 1967, loss = 0.13649289\n",
      "Iteration 1968, loss = 0.13644395\n",
      "Iteration 1969, loss = 0.13639502\n",
      "Iteration 1970, loss = 0.13634611\n",
      "Iteration 1971, loss = 0.13629722\n",
      "Iteration 1972, loss = 0.13624834\n",
      "Iteration 1973, loss = 0.13619949\n",
      "Iteration 1974, loss = 0.13615065\n",
      "Iteration 1975, loss = 0.13610183\n",
      "Iteration 1976, loss = 0.13605302\n",
      "Iteration 1977, loss = 0.13600424\n",
      "Iteration 1978, loss = 0.13595547\n",
      "Iteration 1979, loss = 0.13590672\n",
      "Iteration 1980, loss = 0.13585799\n",
      "Iteration 1981, loss = 0.13580927\n",
      "Iteration 1982, loss = 0.13576058\n",
      "Iteration 1983, loss = 0.13571190\n",
      "Iteration 1984, loss = 0.13566324\n",
      "Iteration 1985, loss = 0.13561460\n",
      "Iteration 1986, loss = 0.13556598\n",
      "Iteration 1987, loss = 0.13551737\n",
      "Iteration 1988, loss = 0.13546879\n",
      "Iteration 1989, loss = 0.13542022\n",
      "Iteration 1990, loss = 0.13537167\n",
      "Iteration 1991, loss = 0.13532314\n",
      "Iteration 1992, loss = 0.13527463\n",
      "Iteration 1993, loss = 0.13522614\n",
      "Iteration 1994, loss = 0.13517766\n",
      "Iteration 1995, loss = 0.13512921\n",
      "Iteration 1996, loss = 0.13508077\n",
      "Iteration 1997, loss = 0.13503235\n",
      "Iteration 1998, loss = 0.13498395\n",
      "Iteration 1999, loss = 0.13493557\n",
      "Iteration 2000, loss = 0.13488721\n",
      "Iteration 2001, loss = 0.13483887\n",
      "Iteration 2002, loss = 0.13479055\n",
      "Iteration 2003, loss = 0.13474225\n",
      "Iteration 2004, loss = 0.13469396\n",
      "Iteration 2005, loss = 0.13464570\n",
      "Iteration 2006, loss = 0.13459745\n",
      "Iteration 2007, loss = 0.13454922\n",
      "Iteration 2008, loss = 0.13450102\n",
      "Iteration 2009, loss = 0.13445283\n",
      "Iteration 2010, loss = 0.13440466\n",
      "Iteration 2011, loss = 0.13435652\n",
      "Iteration 2012, loss = 0.13430839\n",
      "Iteration 2013, loss = 0.13426028\n",
      "Iteration 2014, loss = 0.13421219\n",
      "Iteration 2015, loss = 0.13416412\n",
      "Iteration 2016, loss = 0.13411607\n",
      "Iteration 2017, loss = 0.13406805\n",
      "Iteration 2018, loss = 0.13402004\n",
      "Iteration 2019, loss = 0.13397205\n",
      "Iteration 2020, loss = 0.13392408\n",
      "Iteration 2021, loss = 0.13387613\n",
      "Iteration 2022, loss = 0.13382820\n",
      "Iteration 2023, loss = 0.13378029\n",
      "Iteration 2024, loss = 0.13373241\n",
      "Iteration 2025, loss = 0.13368454\n",
      "Iteration 2026, loss = 0.13363669\n",
      "Iteration 2027, loss = 0.13358886\n",
      "Iteration 2028, loss = 0.13354106\n",
      "Iteration 2029, loss = 0.13349327\n",
      "Iteration 2030, loss = 0.13344551\n",
      "Iteration 2031, loss = 0.13339776\n",
      "Iteration 2032, loss = 0.13335004\n",
      "Iteration 2033, loss = 0.13330234\n",
      "Iteration 2034, loss = 0.13325466\n",
      "Iteration 2035, loss = 0.13320700\n",
      "Iteration 2036, loss = 0.13315936\n",
      "Iteration 2037, loss = 0.13311174\n",
      "Iteration 2038, loss = 0.13306414\n",
      "Iteration 2039, loss = 0.13301656\n",
      "Iteration 2040, loss = 0.13296901\n",
      "Iteration 2041, loss = 0.13292147\n",
      "Iteration 2042, loss = 0.13287396\n",
      "Iteration 2043, loss = 0.13282647\n",
      "Iteration 2044, loss = 0.13277900\n",
      "Iteration 2045, loss = 0.13273155\n",
      "Iteration 2046, loss = 0.13268412\n",
      "Iteration 2047, loss = 0.13263671\n",
      "Iteration 2048, loss = 0.13258933\n",
      "Iteration 2049, loss = 0.13254197\n",
      "Iteration 2050, loss = 0.13249463\n",
      "Iteration 2051, loss = 0.13244731\n",
      "Iteration 2052, loss = 0.13240001\n",
      "Iteration 2053, loss = 0.13235273\n",
      "Iteration 2054, loss = 0.13230548\n",
      "Iteration 2055, loss = 0.13225825\n",
      "Iteration 2056, loss = 0.13221104\n",
      "Iteration 2057, loss = 0.13216385\n",
      "Iteration 2058, loss = 0.13211668\n",
      "Iteration 2059, loss = 0.13206954\n",
      "Iteration 2060, loss = 0.13202242\n",
      "Iteration 2061, loss = 0.13197532\n",
      "Iteration 2062, loss = 0.13192824\n",
      "Iteration 2063, loss = 0.13188119\n",
      "Iteration 2064, loss = 0.13183416\n",
      "Iteration 2065, loss = 0.13178715\n",
      "Iteration 2066, loss = 0.13174016\n",
      "Iteration 2067, loss = 0.13169320\n",
      "Iteration 2068, loss = 0.13164625\n",
      "Iteration 2069, loss = 0.13159933\n",
      "Iteration 2070, loss = 0.13155244\n",
      "Iteration 2071, loss = 0.13150556\n",
      "Iteration 2072, loss = 0.13145871\n",
      "Iteration 2073, loss = 0.13141189\n",
      "Iteration 2074, loss = 0.13136508\n",
      "Iteration 2075, loss = 0.13131830\n",
      "Iteration 2076, loss = 0.13127154\n",
      "Iteration 2077, loss = 0.13122481\n",
      "Iteration 2078, loss = 0.13117809\n",
      "Iteration 2079, loss = 0.13113140\n",
      "Iteration 2080, loss = 0.13108474\n",
      "Iteration 2081, loss = 0.13103809\n",
      "Iteration 2082, loss = 0.13099148\n",
      "Iteration 2083, loss = 0.13094488\n",
      "Iteration 2084, loss = 0.13089831\n",
      "Iteration 2085, loss = 0.13085176\n",
      "Iteration 2086, loss = 0.13080523\n",
      "Iteration 2087, loss = 0.13075873\n",
      "Iteration 2088, loss = 0.13071225\n",
      "Iteration 2089, loss = 0.13066580\n",
      "Iteration 2090, loss = 0.13061937\n",
      "Iteration 2091, loss = 0.13057296\n",
      "Iteration 2092, loss = 0.13052658\n",
      "Iteration 2093, loss = 0.13048022\n",
      "Iteration 2094, loss = 0.13043389\n",
      "Iteration 2095, loss = 0.13038757\n",
      "Iteration 2096, loss = 0.13034129\n",
      "Iteration 2097, loss = 0.13029503\n",
      "Iteration 2098, loss = 0.13024879\n",
      "Iteration 2099, loss = 0.13020257\n",
      "Iteration 2100, loss = 0.13015638\n",
      "Iteration 2101, loss = 0.13011022\n",
      "Iteration 2102, loss = 0.13006408\n",
      "Iteration 2103, loss = 0.13001796\n",
      "Iteration 2104, loss = 0.12997187\n",
      "Iteration 2105, loss = 0.12992580\n",
      "Iteration 2106, loss = 0.12987976\n",
      "Iteration 2107, loss = 0.12983374\n",
      "Iteration 2108, loss = 0.12978775\n",
      "Iteration 2109, loss = 0.12974178\n",
      "Iteration 2110, loss = 0.12969584\n",
      "Iteration 2111, loss = 0.12964992\n",
      "Iteration 2112, loss = 0.12960402\n",
      "Iteration 2113, loss = 0.12955815\n",
      "Iteration 2114, loss = 0.12951231\n",
      "Iteration 2115, loss = 0.12946649\n",
      "Iteration 2116, loss = 0.12942070\n",
      "Iteration 2117, loss = 0.12937493\n",
      "Iteration 2118, loss = 0.12932919\n",
      "Iteration 2119, loss = 0.12928347\n",
      "Iteration 2120, loss = 0.12923778\n",
      "Iteration 2121, loss = 0.12919211\n",
      "Iteration 2122, loss = 0.12914647\n",
      "Iteration 2123, loss = 0.12910085\n",
      "Iteration 2124, loss = 0.12905526\n",
      "Iteration 2125, loss = 0.12900970\n",
      "Iteration 2126, loss = 0.12896416\n",
      "Iteration 2127, loss = 0.12891864\n",
      "Iteration 2128, loss = 0.12887316\n",
      "Iteration 2129, loss = 0.12882769\n",
      "Iteration 2130, loss = 0.12878226\n",
      "Iteration 2131, loss = 0.12873685\n",
      "Iteration 2132, loss = 0.12869146\n",
      "Iteration 2133, loss = 0.12864611\n",
      "Iteration 2134, loss = 0.12860077\n",
      "Iteration 2135, loss = 0.12855547\n",
      "Iteration 2136, loss = 0.12851019\n",
      "Iteration 2137, loss = 0.12846493\n",
      "Iteration 2138, loss = 0.12841971\n",
      "Iteration 2139, loss = 0.12837451\n",
      "Iteration 2140, loss = 0.12832933\n",
      "Iteration 2141, loss = 0.12828418\n",
      "Iteration 2142, loss = 0.12823906\n",
      "Iteration 2143, loss = 0.12819397\n",
      "Iteration 2144, loss = 0.12814890\n",
      "Iteration 2145, loss = 0.12810386\n",
      "Iteration 2146, loss = 0.12805884\n",
      "Iteration 2147, loss = 0.12801385\n",
      "Iteration 2148, loss = 0.12796889\n",
      "Iteration 2149, loss = 0.12792396\n",
      "Iteration 2150, loss = 0.12787905\n",
      "Iteration 2151, loss = 0.12783417\n",
      "Iteration 2152, loss = 0.12778931\n",
      "Iteration 2153, loss = 0.12774449\n",
      "Iteration 2154, loss = 0.12769969\n",
      "Iteration 2155, loss = 0.12765491\n",
      "Iteration 2156, loss = 0.12761017\n",
      "Iteration 2157, loss = 0.12756545\n",
      "Iteration 2158, loss = 0.12752076\n",
      "Iteration 2159, loss = 0.12747610\n",
      "Iteration 2160, loss = 0.12743146\n",
      "Iteration 2161, loss = 0.12738685\n",
      "Iteration 2162, loss = 0.12734227\n",
      "Iteration 2163, loss = 0.12729772\n",
      "Iteration 2164, loss = 0.12725319\n",
      "Iteration 2165, loss = 0.12720869\n",
      "Iteration 2166, loss = 0.12716422\n",
      "Iteration 2167, loss = 0.12711978\n",
      "Iteration 2168, loss = 0.12707536\n",
      "Iteration 2169, loss = 0.12703098\n",
      "Iteration 2170, loss = 0.12698662\n",
      "Iteration 2171, loss = 0.12694228\n",
      "Iteration 2172, loss = 0.12689798\n",
      "Iteration 2173, loss = 0.12685371\n",
      "Iteration 2174, loss = 0.12680946\n",
      "Iteration 2175, loss = 0.12676524\n",
      "Iteration 2176, loss = 0.12672105\n",
      "Iteration 2177, loss = 0.12667688\n",
      "Iteration 2178, loss = 0.12663275\n",
      "Iteration 2179, loss = 0.12658864\n",
      "Iteration 2180, loss = 0.12654457\n",
      "Iteration 2181, loss = 0.12650052\n",
      "Iteration 2182, loss = 0.12645650\n",
      "Iteration 2183, loss = 0.12641250\n",
      "Iteration 2184, loss = 0.12636854\n",
      "Iteration 2185, loss = 0.12632460\n",
      "Iteration 2186, loss = 0.12628070\n",
      "Iteration 2187, loss = 0.12623682\n",
      "Iteration 2188, loss = 0.12619297\n",
      "Iteration 2189, loss = 0.12614915\n",
      "Iteration 2190, loss = 0.12610536\n",
      "Iteration 2191, loss = 0.12606160\n",
      "Iteration 2192, loss = 0.12601786\n",
      "Iteration 2193, loss = 0.12597416\n",
      "Iteration 2194, loss = 0.12593048\n",
      "Iteration 2195, loss = 0.12588684\n",
      "Iteration 2196, loss = 0.12584322\n",
      "Iteration 2197, loss = 0.12579963\n",
      "Iteration 2198, loss = 0.12575608\n",
      "Iteration 2199, loss = 0.12571255\n",
      "Iteration 2200, loss = 0.12566905\n",
      "Iteration 2201, loss = 0.12562558\n",
      "Iteration 2202, loss = 0.12558214\n",
      "Iteration 2203, loss = 0.12553872\n",
      "Iteration 2204, loss = 0.12549534\n",
      "Iteration 2205, loss = 0.12545199\n",
      "Iteration 2206, loss = 0.12540867\n",
      "Iteration 2207, loss = 0.12536537\n",
      "Iteration 2208, loss = 0.12532211\n",
      "Iteration 2209, loss = 0.12527888\n",
      "Iteration 2210, loss = 0.12523567\n",
      "Iteration 2211, loss = 0.12519250\n",
      "Iteration 2212, loss = 0.12514936\n",
      "Iteration 2213, loss = 0.12510624\n",
      "Iteration 2214, loss = 0.12506316\n",
      "Iteration 2215, loss = 0.12502011\n",
      "Iteration 2216, loss = 0.12497708\n",
      "Iteration 2217, loss = 0.12493409\n",
      "Iteration 2218, loss = 0.12489113\n",
      "Iteration 2219, loss = 0.12484819\n",
      "Iteration 2220, loss = 0.12480529\n",
      "Iteration 2221, loss = 0.12476242\n",
      "Iteration 2222, loss = 0.12471958\n",
      "Iteration 2223, loss = 0.12467676\n",
      "Iteration 2224, loss = 0.12463398\n",
      "Iteration 2225, loss = 0.12459123\n",
      "Iteration 2226, loss = 0.12454851\n",
      "Iteration 2227, loss = 0.12450582\n",
      "Iteration 2228, loss = 0.12446317\n",
      "Iteration 2229, loss = 0.12442054\n",
      "Iteration 2230, loss = 0.12437794\n",
      "Iteration 2231, loss = 0.12433537\n",
      "Iteration 2232, loss = 0.12429284\n",
      "Iteration 2233, loss = 0.12425033\n",
      "Iteration 2234, loss = 0.12420786\n",
      "Iteration 2235, loss = 0.12416542\n",
      "Iteration 2236, loss = 0.12412301\n",
      "Iteration 2237, loss = 0.12408063\n",
      "Iteration 2238, loss = 0.12403828\n",
      "Iteration 2239, loss = 0.12399596\n",
      "Iteration 2240, loss = 0.12395367\n",
      "Iteration 2241, loss = 0.12391142\n",
      "Iteration 2242, loss = 0.12386919\n",
      "Iteration 2243, loss = 0.12382700\n",
      "Iteration 2244, loss = 0.12378484\n",
      "Iteration 2245, loss = 0.12374271\n",
      "Iteration 2246, loss = 0.12370061\n",
      "Iteration 2247, loss = 0.12365854\n",
      "Iteration 2248, loss = 0.12361651\n",
      "Iteration 2249, loss = 0.12357450\n",
      "Iteration 2250, loss = 0.12353253\n",
      "Iteration 2251, loss = 0.12349059\n",
      "Iteration 2252, loss = 0.12344868\n",
      "Iteration 2253, loss = 0.12340680\n",
      "Iteration 2254, loss = 0.12336496\n",
      "Iteration 2255, loss = 0.12332315\n",
      "Iteration 2256, loss = 0.12328137\n",
      "Iteration 2257, loss = 0.12323962\n",
      "Iteration 2258, loss = 0.12319790\n",
      "Iteration 2259, loss = 0.12315621\n",
      "Iteration 2260, loss = 0.12311456\n",
      "Iteration 2261, loss = 0.12307294\n",
      "Iteration 2262, loss = 0.12303135\n",
      "Iteration 2263, loss = 0.12298979\n",
      "Iteration 2264, loss = 0.12294827\n",
      "Iteration 2265, loss = 0.12290678\n",
      "Iteration 2266, loss = 0.12286532\n",
      "Iteration 2267, loss = 0.12282335\n",
      "Iteration 2268, loss = 0.12278036\n",
      "Iteration 2269, loss = 0.12273718\n",
      "Iteration 2270, loss = 0.12269382\n",
      "Iteration 2271, loss = 0.12265031\n",
      "Iteration 2272, loss = 0.12260667\n",
      "Iteration 2273, loss = 0.12256292\n",
      "Iteration 2274, loss = 0.12251906\n",
      "Iteration 2275, loss = 0.12247511\n",
      "Iteration 2276, loss = 0.12243109\n",
      "Iteration 2277, loss = 0.12238699\n",
      "Iteration 2278, loss = 0.12234285\n",
      "Iteration 2279, loss = 0.12229865\n",
      "Iteration 2280, loss = 0.12225442\n",
      "Iteration 2281, loss = 0.12221015\n",
      "Iteration 2282, loss = 0.12216586\n",
      "Iteration 2283, loss = 0.12212155\n",
      "Iteration 2284, loss = 0.12207722\n",
      "Iteration 2285, loss = 0.12203288\n",
      "Iteration 2286, loss = 0.12198854\n",
      "Iteration 2287, loss = 0.12194419\n",
      "Iteration 2288, loss = 0.12189984\n",
      "Iteration 2289, loss = 0.12185550\n",
      "Iteration 2290, loss = 0.12181117\n",
      "Iteration 2291, loss = 0.12176684\n",
      "Iteration 2292, loss = 0.12172253\n",
      "Iteration 2293, loss = 0.12167823\n",
      "Iteration 2294, loss = 0.12163394\n",
      "Iteration 2295, loss = 0.12158968\n",
      "Iteration 2296, loss = 0.12154543\n",
      "Iteration 2297, loss = 0.12150120\n",
      "Iteration 2298, loss = 0.12145700\n",
      "Iteration 2299, loss = 0.12141282\n",
      "Iteration 2300, loss = 0.12136866\n",
      "Iteration 2301, loss = 0.12132452\n",
      "Iteration 2302, loss = 0.12128042\n",
      "Iteration 2303, loss = 0.12123633\n",
      "Iteration 2304, loss = 0.12119228\n",
      "Iteration 2305, loss = 0.12114825\n",
      "Iteration 2306, loss = 0.12110425\n",
      "Iteration 2307, loss = 0.12106028\n",
      "Iteration 2308, loss = 0.12101634\n",
      "Iteration 2309, loss = 0.12097243\n",
      "Iteration 2310, loss = 0.12092855\n",
      "Iteration 2311, loss = 0.12088470\n",
      "Iteration 2312, loss = 0.12084088\n",
      "Iteration 2313, loss = 0.12079709\n",
      "Iteration 2314, loss = 0.12075334\n",
      "Iteration 2315, loss = 0.12070961\n",
      "Iteration 2316, loss = 0.12066592\n",
      "Iteration 2317, loss = 0.12062226\n",
      "Iteration 2318, loss = 0.12057863\n",
      "Iteration 2319, loss = 0.12053503\n",
      "Iteration 2320, loss = 0.12049147\n",
      "Iteration 2321, loss = 0.12044794\n",
      "Iteration 2322, loss = 0.12040444\n",
      "Iteration 2323, loss = 0.12036098\n",
      "Iteration 2324, loss = 0.12031755\n",
      "Iteration 2325, loss = 0.12027415\n",
      "Iteration 2326, loss = 0.12023079\n",
      "Iteration 2327, loss = 0.12018746\n",
      "Iteration 2328, loss = 0.12014417\n",
      "Iteration 2329, loss = 0.12010090\n",
      "Iteration 2330, loss = 0.12005768\n",
      "Iteration 2331, loss = 0.12001448\n",
      "Iteration 2332, loss = 0.11997133\n",
      "Iteration 2333, loss = 0.11992820\n",
      "Iteration 2334, loss = 0.11988511\n",
      "Iteration 2335, loss = 0.11984206\n",
      "Iteration 2336, loss = 0.11979904\n",
      "Iteration 2337, loss = 0.11975605\n",
      "Iteration 2338, loss = 0.11971310\n",
      "Iteration 2339, loss = 0.11967018\n",
      "Iteration 2340, loss = 0.11962730\n",
      "Iteration 2341, loss = 0.11958446\n",
      "Iteration 2342, loss = 0.11954165\n",
      "Iteration 2343, loss = 0.11949887\n",
      "Iteration 2344, loss = 0.11945613\n",
      "Iteration 2345, loss = 0.11941343\n",
      "Iteration 2346, loss = 0.11937076\n",
      "Iteration 2347, loss = 0.11932813\n",
      "Iteration 2348, loss = 0.11928553\n",
      "Iteration 2349, loss = 0.11924297\n",
      "Iteration 2350, loss = 0.11920044\n",
      "Iteration 2351, loss = 0.11915795\n",
      "Iteration 2352, loss = 0.11911550\n",
      "Iteration 2353, loss = 0.11907308\n",
      "Iteration 2354, loss = 0.11903069\n",
      "Iteration 2355, loss = 0.11898835\n",
      "Iteration 2356, loss = 0.11894604\n",
      "Iteration 2357, loss = 0.11890376\n",
      "Iteration 2358, loss = 0.11886153\n",
      "Iteration 2359, loss = 0.11881932\n",
      "Iteration 2360, loss = 0.11877716\n",
      "Iteration 2361, loss = 0.11873503\n",
      "Iteration 2362, loss = 0.11869294\n",
      "Iteration 2363, loss = 0.11865088\n",
      "Iteration 2364, loss = 0.11860887\n",
      "Iteration 2365, loss = 0.11856688\n",
      "Iteration 2366, loss = 0.11852494\n",
      "Iteration 2367, loss = 0.11848303\n",
      "Iteration 2368, loss = 0.11844116\n",
      "Iteration 2369, loss = 0.11839933\n",
      "Iteration 2370, loss = 0.11835753\n",
      "Iteration 2371, loss = 0.11831577\n",
      "Iteration 2372, loss = 0.11827405\n",
      "Iteration 2373, loss = 0.11823236\n",
      "Iteration 2374, loss = 0.11819072\n",
      "Iteration 2375, loss = 0.11814911\n",
      "Iteration 2376, loss = 0.11810753\n",
      "Iteration 2377, loss = 0.11806600\n",
      "Iteration 2378, loss = 0.11802450\n",
      "Iteration 2379, loss = 0.11798304\n",
      "Iteration 2380, loss = 0.11794162\n",
      "Iteration 2381, loss = 0.11790023\n",
      "Iteration 2382, loss = 0.11785889\n",
      "Iteration 2383, loss = 0.11781758\n",
      "Iteration 2384, loss = 0.11777631\n",
      "Iteration 2385, loss = 0.11773508\n",
      "Iteration 2386, loss = 0.11769388\n",
      "Iteration 2387, loss = 0.11765272\n",
      "Iteration 2388, loss = 0.11761161\n",
      "Iteration 2389, loss = 0.11757053\n",
      "Iteration 2390, loss = 0.11752949\n",
      "Iteration 2391, loss = 0.11748848\n",
      "Iteration 2392, loss = 0.11744752\n",
      "Iteration 2393, loss = 0.11740659\n",
      "Iteration 2394, loss = 0.11736571\n",
      "Iteration 2395, loss = 0.11732333\n",
      "Iteration 2396, loss = 0.11728015\n",
      "Iteration 2397, loss = 0.11723675\n",
      "Iteration 2398, loss = 0.11719315\n",
      "Iteration 2399, loss = 0.11714936\n",
      "Iteration 2400, loss = 0.11710542\n",
      "Iteration 2401, loss = 0.11706134\n",
      "Iteration 2402, loss = 0.11701714\n",
      "Iteration 2403, loss = 0.11697283\n",
      "Iteration 2404, loss = 0.11692843\n",
      "Iteration 2405, loss = 0.11688395\n",
      "Iteration 2406, loss = 0.11683941\n",
      "Iteration 2407, loss = 0.11679481\n",
      "Iteration 2408, loss = 0.11675016\n",
      "Iteration 2409, loss = 0.11670547\n",
      "Iteration 2410, loss = 0.11666076\n",
      "Iteration 2411, loss = 0.11661601\n",
      "Iteration 2412, loss = 0.11657125\n",
      "Iteration 2413, loss = 0.11652648\n",
      "Iteration 2414, loss = 0.11648170\n",
      "Iteration 2415, loss = 0.11643691\n",
      "Iteration 2416, loss = 0.11639213\n",
      "Iteration 2417, loss = 0.11634735\n",
      "Iteration 2418, loss = 0.11630257\n",
      "Iteration 2419, loss = 0.11625781\n",
      "Iteration 2420, loss = 0.11621307\n",
      "Iteration 2421, loss = 0.11616833\n",
      "Iteration 2422, loss = 0.11612362\n",
      "Iteration 2423, loss = 0.11607893\n",
      "Iteration 2424, loss = 0.11603425\n",
      "Iteration 2425, loss = 0.11598961\n",
      "Iteration 2426, loss = 0.11594498\n",
      "Iteration 2427, loss = 0.11590039\n",
      "Iteration 2428, loss = 0.11585582\n",
      "Iteration 2429, loss = 0.11581153\n",
      "Iteration 2430, loss = 0.11576801\n",
      "Iteration 2431, loss = 0.11572460\n",
      "Iteration 2432, loss = 0.11568128\n",
      "Iteration 2433, loss = 0.11563806\n",
      "Iteration 2434, loss = 0.11559493\n",
      "Iteration 2435, loss = 0.11555188\n",
      "Iteration 2436, loss = 0.11550891\n",
      "Iteration 2437, loss = 0.11546602\n",
      "Iteration 2438, loss = 0.11542320\n",
      "Iteration 2439, loss = 0.11538045\n",
      "Iteration 2440, loss = 0.11533776\n",
      "Iteration 2441, loss = 0.11529513\n",
      "Iteration 2442, loss = 0.11525257\n",
      "Iteration 2443, loss = 0.11521007\n",
      "Iteration 2444, loss = 0.11516762\n",
      "Iteration 2445, loss = 0.11512523\n",
      "Iteration 2446, loss = 0.11508289\n",
      "Iteration 2447, loss = 0.11504061\n",
      "Iteration 2448, loss = 0.11499838\n",
      "Iteration 2449, loss = 0.11495620\n",
      "Iteration 2450, loss = 0.11491406\n",
      "Iteration 2451, loss = 0.11487198\n",
      "Iteration 2452, loss = 0.11482994\n",
      "Iteration 2453, loss = 0.11478796\n",
      "Iteration 2454, loss = 0.11474601\n",
      "Iteration 2455, loss = 0.11470412\n",
      "Iteration 2456, loss = 0.11466227\n",
      "Iteration 2457, loss = 0.11462046\n",
      "Iteration 2458, loss = 0.11457870\n",
      "Iteration 2459, loss = 0.11453699\n",
      "Iteration 2460, loss = 0.11449532\n",
      "Iteration 2461, loss = 0.11445369\n",
      "Iteration 2462, loss = 0.11441211\n",
      "Iteration 2463, loss = 0.11437057\n",
      "Iteration 2464, loss = 0.11432907\n",
      "Iteration 2465, loss = 0.11428762\n",
      "Iteration 2466, loss = 0.11424621\n",
      "Iteration 2467, loss = 0.11420485\n",
      "Iteration 2468, loss = 0.11416352\n",
      "Iteration 2469, loss = 0.11412224\n",
      "Iteration 2470, loss = 0.11408101\n",
      "Iteration 2471, loss = 0.11403981\n",
      "Iteration 2472, loss = 0.11399866\n",
      "Iteration 2473, loss = 0.11395755\n",
      "Iteration 2474, loss = 0.11391649\n",
      "Iteration 2475, loss = 0.11387546\n",
      "Iteration 2476, loss = 0.11383448\n",
      "Iteration 2477, loss = 0.11379355\n",
      "Iteration 2478, loss = 0.11375265\n",
      "Iteration 2479, loss = 0.11371180\n",
      "Iteration 2480, loss = 0.11367099\n",
      "Iteration 2481, loss = 0.11363023\n",
      "Iteration 2482, loss = 0.11358950\n",
      "Iteration 2483, loss = 0.11354882\n",
      "Iteration 2484, loss = 0.11350819\n",
      "Iteration 2485, loss = 0.11346759\n",
      "Iteration 2486, loss = 0.11342704\n",
      "Iteration 2487, loss = 0.11338653\n",
      "Iteration 2488, loss = 0.11334607\n",
      "Iteration 2489, loss = 0.11330565\n",
      "Iteration 2490, loss = 0.11326527\n",
      "Iteration 2491, loss = 0.11322494\n",
      "Iteration 2492, loss = 0.11318465\n",
      "Iteration 2493, loss = 0.11314440\n",
      "Iteration 2494, loss = 0.11310419\n",
      "Iteration 2495, loss = 0.11306403\n",
      "Iteration 2496, loss = 0.11302392\n",
      "Iteration 2497, loss = 0.11298384\n",
      "Iteration 2498, loss = 0.11294381\n",
      "Iteration 2499, loss = 0.11290383\n",
      "Iteration 2500, loss = 0.11286389\n",
      "Iteration 2501, loss = 0.11282399\n",
      "Iteration 2502, loss = 0.11278414\n",
      "Iteration 2503, loss = 0.11274433\n",
      "Iteration 2504, loss = 0.11270456\n",
      "Iteration 2505, loss = 0.11266484\n",
      "Iteration 2506, loss = 0.11262516\n",
      "Iteration 2507, loss = 0.11258553\n",
      "Iteration 2508, loss = 0.11254594\n",
      "Iteration 2509, loss = 0.11250640\n",
      "Iteration 2510, loss = 0.11246690\n",
      "Iteration 2511, loss = 0.11242745\n",
      "Iteration 2512, loss = 0.11238804\n",
      "Iteration 2513, loss = 0.11234867\n",
      "Iteration 2514, loss = 0.11230935\n",
      "Iteration 2515, loss = 0.11227008\n",
      "Iteration 2516, loss = 0.11223085\n",
      "Iteration 2517, loss = 0.11219166\n",
      "Iteration 2518, loss = 0.11215252\n",
      "Iteration 2519, loss = 0.11211343\n",
      "Iteration 2520, loss = 0.11207438\n",
      "Iteration 2521, loss = 0.11203537\n",
      "Iteration 2522, loss = 0.11199641\n",
      "Iteration 2523, loss = 0.11195750\n",
      "Iteration 2524, loss = 0.11191863\n",
      "Iteration 2525, loss = 0.11187981\n",
      "Iteration 2526, loss = 0.11184103\n",
      "Iteration 2527, loss = 0.11180230\n",
      "Iteration 2528, loss = 0.11176361\n",
      "Iteration 2529, loss = 0.11172497\n",
      "Iteration 2530, loss = 0.11168638\n",
      "Iteration 2531, loss = 0.11164783\n",
      "Iteration 2532, loss = 0.11160933\n",
      "Iteration 2533, loss = 0.11157087\n",
      "Iteration 2534, loss = 0.11153246\n",
      "Iteration 2535, loss = 0.11149410\n",
      "Iteration 2536, loss = 0.11145578\n",
      "Iteration 2537, loss = 0.11141751\n",
      "Iteration 2538, loss = 0.11137928\n",
      "Iteration 2539, loss = 0.11134111\n",
      "Iteration 2540, loss = 0.11130297\n",
      "Iteration 2541, loss = 0.11126489\n",
      "Iteration 2542, loss = 0.11122685\n",
      "Iteration 2543, loss = 0.11118886\n",
      "Iteration 2544, loss = 0.11115091\n",
      "Iteration 2545, loss = 0.11111301\n",
      "Iteration 2546, loss = 0.11107516\n",
      "Iteration 2547, loss = 0.11103735\n",
      "Iteration 2548, loss = 0.11099960\n",
      "Iteration 2549, loss = 0.11096189\n",
      "Iteration 2550, loss = 0.11092422\n",
      "Iteration 2551, loss = 0.11088660\n",
      "Iteration 2552, loss = 0.11084903\n",
      "Iteration 2553, loss = 0.11081151\n",
      "Iteration 2554, loss = 0.11077404\n",
      "Iteration 2555, loss = 0.11073661\n",
      "Iteration 2556, loss = 0.11069923\n",
      "Iteration 2557, loss = 0.11066190\n",
      "Iteration 2558, loss = 0.11062461\n",
      "Iteration 2559, loss = 0.11058737\n",
      "Iteration 2560, loss = 0.11055018\n",
      "Iteration 2561, loss = 0.11051296\n",
      "Iteration 2562, loss = 0.11047560\n",
      "Iteration 2563, loss = 0.11043826\n",
      "Iteration 2564, loss = 0.11040095\n",
      "Iteration 2565, loss = 0.11036367\n",
      "Iteration 2566, loss = 0.11032642\n",
      "Iteration 2567, loss = 0.11028920\n",
      "Iteration 2568, loss = 0.11025201\n",
      "Iteration 2569, loss = 0.11021486\n",
      "Iteration 2570, loss = 0.11017775\n",
      "Iteration 2571, loss = 0.11014068\n",
      "Iteration 2572, loss = 0.11010364\n",
      "Iteration 2573, loss = 0.11006665\n",
      "Iteration 2574, loss = 0.11002969\n",
      "Iteration 2575, loss = 0.10999278\n",
      "Iteration 2576, loss = 0.10995591\n",
      "Iteration 2577, loss = 0.10991908\n",
      "Iteration 2578, loss = 0.10988229\n",
      "Iteration 2579, loss = 0.10984555\n",
      "Iteration 2580, loss = 0.10980885\n",
      "Iteration 2581, loss = 0.10977220\n",
      "Iteration 2582, loss = 0.10973559\n",
      "Iteration 2583, loss = 0.10969903\n",
      "Iteration 2584, loss = 0.10966251\n",
      "Iteration 2585, loss = 0.10962604\n",
      "Iteration 2586, loss = 0.10958962\n",
      "Iteration 2587, loss = 0.10955324\n",
      "Iteration 2588, loss = 0.10951691\n",
      "Iteration 2589, loss = 0.10948062\n",
      "Iteration 2590, loss = 0.10944439\n",
      "Iteration 2591, loss = 0.10940820\n",
      "Iteration 2592, loss = 0.10937205\n",
      "Iteration 2593, loss = 0.10933596\n",
      "Iteration 2594, loss = 0.10929991\n",
      "Iteration 2595, loss = 0.10926391\n",
      "Iteration 2596, loss = 0.10922796\n",
      "Iteration 2597, loss = 0.10919206\n",
      "Iteration 2598, loss = 0.10915620\n",
      "Iteration 2599, loss = 0.10912040\n",
      "Iteration 2600, loss = 0.10908464\n",
      "Iteration 2601, loss = 0.10904893\n",
      "Iteration 2602, loss = 0.10901327\n",
      "Iteration 2603, loss = 0.10897766\n",
      "Iteration 2604, loss = 0.10894210\n",
      "Iteration 2605, loss = 0.10890659\n",
      "Iteration 2606, loss = 0.10887112\n",
      "Iteration 2607, loss = 0.10883571\n",
      "Iteration 2608, loss = 0.10880025\n",
      "Iteration 2609, loss = 0.10876441\n",
      "Iteration 2610, loss = 0.10872857\n",
      "Iteration 2611, loss = 0.10869273\n",
      "Iteration 2612, loss = 0.10865691\n",
      "Iteration 2613, loss = 0.10862110\n",
      "Iteration 2614, loss = 0.10858531\n",
      "Iteration 2615, loss = 0.10854954\n",
      "Iteration 2616, loss = 0.10851380\n",
      "Iteration 2617, loss = 0.10847808\n",
      "Iteration 2618, loss = 0.10844240\n",
      "Iteration 2619, loss = 0.10840674\n",
      "Iteration 2620, loss = 0.10837112\n",
      "Iteration 2621, loss = 0.10833553\n",
      "Iteration 2622, loss = 0.10829998\n",
      "Iteration 2623, loss = 0.10826446\n",
      "Iteration 2624, loss = 0.10822899\n",
      "Iteration 2625, loss = 0.10819355\n",
      "Iteration 2626, loss = 0.10815815\n",
      "Iteration 2627, loss = 0.10812280\n",
      "Iteration 2628, loss = 0.10808749\n",
      "Iteration 2629, loss = 0.10805222\n",
      "Iteration 2630, loss = 0.10801700\n",
      "Iteration 2631, loss = 0.10798182\n",
      "Iteration 2632, loss = 0.10794668\n",
      "Iteration 2633, loss = 0.10791159\n",
      "Iteration 2634, loss = 0.10787655\n",
      "Iteration 2635, loss = 0.10784155\n",
      "Iteration 2636, loss = 0.10780660\n",
      "Iteration 2637, loss = 0.10777170\n",
      "Iteration 2638, loss = 0.10773684\n",
      "Iteration 2639, loss = 0.10770204\n",
      "Iteration 2640, loss = 0.10766728\n",
      "Iteration 2641, loss = 0.10763256\n",
      "Iteration 2642, loss = 0.10759790\n",
      "Iteration 2643, loss = 0.10756328\n",
      "Iteration 2644, loss = 0.10752872\n",
      "Iteration 2645, loss = 0.10749420\n",
      "Iteration 2646, loss = 0.10745973\n",
      "Iteration 2647, loss = 0.10742532\n",
      "Iteration 2648, loss = 0.10739095\n",
      "Iteration 2649, loss = 0.10735663\n",
      "Iteration 2650, loss = 0.10732236\n",
      "Iteration 2651, loss = 0.10728814\n",
      "Iteration 2652, loss = 0.10725396\n",
      "Iteration 2653, loss = 0.10721984\n",
      "Iteration 2654, loss = 0.10718577\n",
      "Iteration 2655, loss = 0.10715175\n",
      "Iteration 2656, loss = 0.10711778\n",
      "Iteration 2657, loss = 0.10708286\n",
      "Iteration 2658, loss = 0.10704636\n",
      "Iteration 2659, loss = 0.10700964\n",
      "Iteration 2660, loss = 0.10697272\n",
      "Iteration 2661, loss = 0.10693562\n",
      "Iteration 2662, loss = 0.10689837\n",
      "Iteration 2663, loss = 0.10686099\n",
      "Iteration 2664, loss = 0.10682349\n",
      "Iteration 2665, loss = 0.10678590\n",
      "Iteration 2666, loss = 0.10674822\n",
      "Iteration 2667, loss = 0.10671048\n",
      "Iteration 2668, loss = 0.10667267\n",
      "Iteration 2669, loss = 0.10663482\n",
      "Iteration 2670, loss = 0.10659693\n",
      "Iteration 2671, loss = 0.10655902\n",
      "Iteration 2672, loss = 0.10652108\n",
      "Iteration 2673, loss = 0.10648313\n",
      "Iteration 2674, loss = 0.10644518\n",
      "Iteration 2675, loss = 0.10640722\n",
      "Iteration 2676, loss = 0.10636927\n",
      "Iteration 2677, loss = 0.10633133\n",
      "Iteration 2678, loss = 0.10629340\n",
      "Iteration 2679, loss = 0.10625549\n",
      "Iteration 2680, loss = 0.10621760\n",
      "Iteration 2681, loss = 0.10617974\n",
      "Iteration 2682, loss = 0.10614190\n",
      "Iteration 2683, loss = 0.10610409\n",
      "Iteration 2684, loss = 0.10606631\n",
      "Iteration 2685, loss = 0.10602856\n",
      "Iteration 2686, loss = 0.10599085\n",
      "Iteration 2687, loss = 0.10595318\n",
      "Iteration 2688, loss = 0.10591555\n",
      "Iteration 2689, loss = 0.10587796\n",
      "Iteration 2690, loss = 0.10584040\n",
      "Iteration 2691, loss = 0.10580289\n",
      "Iteration 2692, loss = 0.10576543\n",
      "Iteration 2693, loss = 0.10572800\n",
      "Iteration 2694, loss = 0.10569063\n",
      "Iteration 2695, loss = 0.10565329\n",
      "Iteration 2696, loss = 0.10561601\n",
      "Iteration 2697, loss = 0.10557877\n",
      "Iteration 2698, loss = 0.10554158\n",
      "Iteration 2699, loss = 0.10550444\n",
      "Iteration 2700, loss = 0.10546734\n",
      "Iteration 2701, loss = 0.10543030\n",
      "Iteration 2702, loss = 0.10539330\n",
      "Iteration 2703, loss = 0.10535635\n",
      "Iteration 2704, loss = 0.10531946\n",
      "Iteration 2705, loss = 0.10528261\n",
      "Iteration 2706, loss = 0.10524582\n",
      "Iteration 2707, loss = 0.10520907\n",
      "Iteration 2708, loss = 0.10517238\n",
      "Iteration 2709, loss = 0.10513549\n",
      "Iteration 2710, loss = 0.10509683\n",
      "Iteration 2711, loss = 0.10505803\n",
      "Iteration 2712, loss = 0.10501910\n",
      "Iteration 2713, loss = 0.10498007\n",
      "Iteration 2714, loss = 0.10494095\n",
      "Iteration 2715, loss = 0.10490175\n",
      "Iteration 2716, loss = 0.10486249\n",
      "Iteration 2717, loss = 0.10482317\n",
      "Iteration 2718, loss = 0.10478382\n",
      "Iteration 2719, loss = 0.10474287\n",
      "Iteration 2720, loss = 0.10470129\n",
      "Iteration 2721, loss = 0.10465947\n",
      "Iteration 2722, loss = 0.10461745\n",
      "Iteration 2723, loss = 0.10457658\n",
      "Iteration 2724, loss = 0.10453695\n",
      "Iteration 2725, loss = 0.10449871\n",
      "Iteration 2726, loss = 0.10446067\n",
      "Iteration 2727, loss = 0.10442281\n",
      "Iteration 2728, loss = 0.10438512\n",
      "Iteration 2729, loss = 0.10434759\n",
      "Iteration 2730, loss = 0.10431020\n",
      "Iteration 2731, loss = 0.10427295\n",
      "Iteration 2732, loss = 0.10423582\n",
      "Iteration 2733, loss = 0.10419881\n",
      "Iteration 2734, loss = 0.10416191\n",
      "Iteration 2735, loss = 0.10412512\n",
      "Iteration 2736, loss = 0.10408842\n",
      "Iteration 2737, loss = 0.10405182\n",
      "Iteration 2738, loss = 0.10401530\n",
      "Iteration 2739, loss = 0.10397887\n",
      "Iteration 2740, loss = 0.10394251\n",
      "Iteration 2741, loss = 0.10390623\n",
      "Iteration 2742, loss = 0.10387002\n",
      "Iteration 2743, loss = 0.10383388\n",
      "Iteration 2744, loss = 0.10379781\n",
      "Iteration 2745, loss = 0.10376180\n",
      "Iteration 2746, loss = 0.10372586\n",
      "Iteration 2747, loss = 0.10368997\n",
      "Iteration 2748, loss = 0.10365414\n",
      "Iteration 2749, loss = 0.10361837\n",
      "Iteration 2750, loss = 0.10358266\n",
      "Iteration 2751, loss = 0.10354700\n",
      "Iteration 2752, loss = 0.10351139\n",
      "Iteration 2753, loss = 0.10347583\n",
      "Iteration 2754, loss = 0.10344033\n",
      "Iteration 2755, loss = 0.10340487\n",
      "Iteration 2756, loss = 0.10336947\n",
      "Iteration 2757, loss = 0.10333411\n",
      "Iteration 2758, loss = 0.10329880\n",
      "Iteration 2759, loss = 0.10326354\n",
      "Iteration 2760, loss = 0.10322833\n",
      "Iteration 2761, loss = 0.10319316\n",
      "Iteration 2762, loss = 0.10315804\n",
      "Iteration 2763, loss = 0.10312297\n",
      "Iteration 2764, loss = 0.10308794\n",
      "Iteration 2765, loss = 0.10305296\n",
      "Iteration 2766, loss = 0.10301802\n",
      "Iteration 2767, loss = 0.10298313\n",
      "Iteration 2768, loss = 0.10294828\n",
      "Iteration 2769, loss = 0.10291348\n",
      "Iteration 2770, loss = 0.10287872\n",
      "Iteration 2771, loss = 0.10284401\n",
      "Iteration 2772, loss = 0.10280934\n",
      "Iteration 2773, loss = 0.10277472\n",
      "Iteration 2774, loss = 0.10274014\n",
      "Iteration 2775, loss = 0.10270560\n",
      "Iteration 2776, loss = 0.10267111\n",
      "Iteration 2777, loss = 0.10263666\n",
      "Iteration 2778, loss = 0.10260225\n",
      "Iteration 2779, loss = 0.10256789\n",
      "Iteration 2780, loss = 0.10253357\n",
      "Iteration 2781, loss = 0.10249929\n",
      "Iteration 2782, loss = 0.10246506\n",
      "Iteration 2783, loss = 0.10243088\n",
      "Iteration 2784, loss = 0.10239673\n",
      "Iteration 2785, loss = 0.10236263\n",
      "Iteration 2786, loss = 0.10232858\n",
      "Iteration 2787, loss = 0.10229456\n",
      "Iteration 2788, loss = 0.10226059\n",
      "Iteration 2789, loss = 0.10222667\n",
      "Iteration 2790, loss = 0.10219278\n",
      "Iteration 2791, loss = 0.10215894\n",
      "Iteration 2792, loss = 0.10212515\n",
      "Iteration 2793, loss = 0.10209140\n",
      "Iteration 2794, loss = 0.10205769\n",
      "Iteration 2795, loss = 0.10202402\n",
      "Iteration 2796, loss = 0.10199040\n",
      "Iteration 2797, loss = 0.10195683\n",
      "Iteration 2798, loss = 0.10192329\n",
      "Iteration 2799, loss = 0.10188980\n",
      "Iteration 2800, loss = 0.10185636\n",
      "Iteration 2801, loss = 0.10182295\n",
      "Iteration 2802, loss = 0.10178959\n",
      "Iteration 2803, loss = 0.10175628\n",
      "Iteration 2804, loss = 0.10172301\n",
      "Iteration 2805, loss = 0.10168978\n",
      "Iteration 2806, loss = 0.10165660\n",
      "Iteration 2807, loss = 0.10162346\n",
      "Iteration 2808, loss = 0.10159036\n",
      "Iteration 2809, loss = 0.10155731\n",
      "Iteration 2810, loss = 0.10152430\n",
      "Iteration 2811, loss = 0.10149134\n",
      "Iteration 2812, loss = 0.10145842\n",
      "Iteration 2813, loss = 0.10142554\n",
      "Iteration 2814, loss = 0.10139271\n",
      "Iteration 2815, loss = 0.10135992\n",
      "Iteration 2816, loss = 0.10132718\n",
      "Iteration 2817, loss = 0.10129448\n",
      "Iteration 2818, loss = 0.10126183\n",
      "Iteration 2819, loss = 0.10122921\n",
      "Iteration 2820, loss = 0.10119665\n",
      "Iteration 2821, loss = 0.10116413\n",
      "Iteration 2822, loss = 0.10113165\n",
      "Iteration 2823, loss = 0.10109921\n",
      "Iteration 2824, loss = 0.10106683\n",
      "Iteration 2825, loss = 0.10103448\n",
      "Iteration 2826, loss = 0.10100218\n",
      "Iteration 2827, loss = 0.10096992\n",
      "Iteration 2828, loss = 0.10093771\n",
      "Iteration 2829, loss = 0.10090555\n",
      "Iteration 2830, loss = 0.10087342\n",
      "Iteration 2831, loss = 0.10084135\n",
      "Iteration 2832, loss = 0.10080931\n",
      "Iteration 2833, loss = 0.10077733\n",
      "Iteration 2834, loss = 0.10074538\n",
      "Iteration 2835, loss = 0.10071348\n",
      "Iteration 2836, loss = 0.10068163\n",
      "Iteration 2837, loss = 0.10064982\n",
      "Iteration 2838, loss = 0.10061805\n",
      "Iteration 2839, loss = 0.10058634\n",
      "Iteration 2840, loss = 0.10055466\n",
      "Iteration 2841, loss = 0.10052303\n",
      "Iteration 2842, loss = 0.10049145\n",
      "Iteration 2843, loss = 0.10045991\n",
      "Iteration 2844, loss = 0.10042841\n",
      "Iteration 2845, loss = 0.10039696\n",
      "Iteration 2846, loss = 0.10036556\n",
      "Iteration 2847, loss = 0.10033420\n",
      "Iteration 2848, loss = 0.10030288\n",
      "Iteration 2849, loss = 0.10027161\n",
      "Iteration 2850, loss = 0.10024039\n",
      "Iteration 2851, loss = 0.10020921\n",
      "Iteration 2852, loss = 0.10017808\n",
      "Iteration 2853, loss = 0.10014699\n",
      "Iteration 2854, loss = 0.10011594\n",
      "Iteration 2855, loss = 0.10008495\n",
      "Iteration 2856, loss = 0.10005399\n",
      "Iteration 2857, loss = 0.10002309\n",
      "Iteration 2858, loss = 0.09999222\n",
      "Iteration 2859, loss = 0.09996141\n",
      "Iteration 2860, loss = 0.09993064\n",
      "Iteration 2861, loss = 0.09989991\n",
      "Iteration 2862, loss = 0.09986923\n",
      "Iteration 2863, loss = 0.09983860\n",
      "Iteration 2864, loss = 0.09980801\n",
      "Iteration 2865, loss = 0.09977746\n",
      "Iteration 2866, loss = 0.09974697\n",
      "Iteration 2867, loss = 0.09971651\n",
      "Iteration 2868, loss = 0.09968611\n",
      "Iteration 2869, loss = 0.09965575\n",
      "Iteration 2870, loss = 0.09962543\n",
      "Iteration 2871, loss = 0.09959516\n",
      "Iteration 2872, loss = 0.09956494\n",
      "Iteration 2873, loss = 0.09953476\n",
      "Iteration 2874, loss = 0.09950463\n",
      "Iteration 2875, loss = 0.09947455\n",
      "Iteration 2876, loss = 0.09944451\n",
      "Iteration 2877, loss = 0.09941451\n",
      "Iteration 2878, loss = 0.09938456\n",
      "Iteration 2879, loss = 0.09935466\n",
      "Iteration 2880, loss = 0.09932480\n",
      "Iteration 2881, loss = 0.09929499\n",
      "Iteration 2882, loss = 0.09926523\n",
      "Iteration 2883, loss = 0.09923551\n",
      "Iteration 2884, loss = 0.09920584\n",
      "Iteration 2885, loss = 0.09917621\n",
      "Iteration 2886, loss = 0.09914663\n",
      "Iteration 2887, loss = 0.09911710\n",
      "Iteration 2888, loss = 0.09908761\n",
      "Iteration 2889, loss = 0.09905817\n",
      "Iteration 2890, loss = 0.09902878\n",
      "Iteration 2891, loss = 0.09899943\n",
      "Iteration 2892, loss = 0.09897012\n",
      "Iteration 2893, loss = 0.09894087\n",
      "Iteration 2894, loss = 0.09891166\n",
      "Iteration 2895, loss = 0.09888249\n",
      "Iteration 2896, loss = 0.09885338\n",
      "Iteration 2897, loss = 0.09882431\n",
      "Iteration 2898, loss = 0.09879528\n",
      "Iteration 2899, loss = 0.09876630\n",
      "Iteration 2900, loss = 0.09873737\n",
      "Iteration 2901, loss = 0.09870848\n",
      "Iteration 2902, loss = 0.09867965\n",
      "Iteration 2903, loss = 0.09865085\n",
      "Iteration 2904, loss = 0.09862211\n",
      "Iteration 2905, loss = 0.09859341\n",
      "Iteration 2906, loss = 0.09856475\n",
      "Iteration 2907, loss = 0.09853615\n",
      "Iteration 2908, loss = 0.09850759\n",
      "Iteration 2909, loss = 0.09847907\n",
      "Iteration 2910, loss = 0.09845061\n",
      "Iteration 2911, loss = 0.09842219\n",
      "Iteration 2912, loss = 0.09839381\n",
      "Iteration 2913, loss = 0.09836548\n",
      "Iteration 2914, loss = 0.09833720\n",
      "Iteration 2915, loss = 0.09830897\n",
      "Iteration 2916, loss = 0.09828078\n",
      "Iteration 2917, loss = 0.09825264\n",
      "Iteration 2918, loss = 0.09822455\n",
      "Iteration 2919, loss = 0.09819650\n",
      "Iteration 2920, loss = 0.09816850\n",
      "Iteration 2921, loss = 0.09814055\n",
      "Iteration 2922, loss = 0.09811264\n",
      "Iteration 2923, loss = 0.09808478\n",
      "Iteration 2924, loss = 0.09805697\n",
      "Iteration 2925, loss = 0.09802920\n",
      "Iteration 2926, loss = 0.09800148\n",
      "Iteration 2927, loss = 0.09797381\n",
      "Iteration 2928, loss = 0.09794618\n",
      "Iteration 2929, loss = 0.09791860\n",
      "Iteration 2930, loss = 0.09789107\n",
      "Iteration 2931, loss = 0.09786383\n",
      "Iteration 2932, loss = 0.09783832\n",
      "Iteration 2933, loss = 0.09781298\n",
      "Iteration 2934, loss = 0.09778780\n",
      "Iteration 2935, loss = 0.09776276\n",
      "Iteration 2936, loss = 0.09773786\n",
      "Iteration 2937, loss = 0.09771308\n",
      "Iteration 2938, loss = 0.09768842\n",
      "Iteration 2939, loss = 0.09766387\n",
      "Iteration 2940, loss = 0.09763943\n",
      "Iteration 2941, loss = 0.09761508\n",
      "Iteration 2942, loss = 0.09759082\n",
      "Iteration 2943, loss = 0.09756665\n",
      "Iteration 2944, loss = 0.09754256\n",
      "Iteration 2945, loss = 0.09751855\n",
      "Iteration 2946, loss = 0.09749461\n",
      "Iteration 2947, loss = 0.09747074\n",
      "Iteration 2948, loss = 0.09744694\n",
      "Iteration 2949, loss = 0.09742321\n",
      "Iteration 2950, loss = 0.09739954\n",
      "Iteration 2951, loss = 0.09737593\n",
      "Iteration 2952, loss = 0.09735238\n",
      "Iteration 2953, loss = 0.09732889\n",
      "Iteration 2954, loss = 0.09730545\n",
      "Iteration 2955, loss = 0.09728206\n",
      "Iteration 2956, loss = 0.09725873\n",
      "Iteration 2957, loss = 0.09723545\n",
      "Iteration 2958, loss = 0.09721222\n",
      "Iteration 2959, loss = 0.09718904\n",
      "Iteration 2960, loss = 0.09716591\n",
      "Iteration 2961, loss = 0.09714283\n",
      "Iteration 2962, loss = 0.09711980\n",
      "Iteration 2963, loss = 0.09709681\n",
      "Iteration 2964, loss = 0.09707386\n",
      "Iteration 2965, loss = 0.09705097\n",
      "Iteration 2966, loss = 0.09702812\n",
      "Iteration 2967, loss = 0.09700531\n",
      "Iteration 2968, loss = 0.09698255\n",
      "Iteration 2969, loss = 0.09695983\n",
      "Iteration 2970, loss = 0.09693709\n",
      "Iteration 2971, loss = 0.09691423\n",
      "Iteration 2972, loss = 0.09689138\n",
      "Iteration 2973, loss = 0.09686856\n",
      "Iteration 2974, loss = 0.09684577\n",
      "Iteration 2975, loss = 0.09682408\n",
      "Iteration 2976, loss = 0.09680261\n",
      "Iteration 2977, loss = 0.09678125\n",
      "Iteration 2978, loss = 0.09676000\n",
      "Iteration 2979, loss = 0.09673885\n",
      "Iteration 2980, loss = 0.09671778\n",
      "Iteration 2981, loss = 0.09669680\n",
      "Iteration 2982, loss = 0.09667590\n",
      "Iteration 2983, loss = 0.09665507\n",
      "Iteration 2984, loss = 0.09663431\n",
      "Iteration 2985, loss = 0.09661362\n",
      "Iteration 2986, loss = 0.09659299\n",
      "Iteration 2987, loss = 0.09657242\n",
      "Iteration 2988, loss = 0.09655190\n",
      "Iteration 2989, loss = 0.09653144\n",
      "Iteration 2990, loss = 0.09651104\n",
      "Iteration 2991, loss = 0.09649068\n",
      "Iteration 2992, loss = 0.09647125\n",
      "Iteration 2993, loss = 0.09645386\n",
      "Iteration 2994, loss = 0.09643655\n",
      "Iteration 2995, loss = 0.09641931\n",
      "Iteration 2996, loss = 0.09640214\n",
      "Iteration 2997, loss = 0.09638505\n",
      "Iteration 2998, loss = 0.09636802\n",
      "Iteration 2999, loss = 0.09635106\n",
      "Iteration 3000, loss = 0.09633416\n",
      "Iteration 3001, loss = 0.09631732\n",
      "Iteration 3002, loss = 0.09630055\n",
      "Iteration 3003, loss = 0.09628383\n",
      "Iteration 3004, loss = 0.09626717\n",
      "Iteration 3005, loss = 0.09625056\n",
      "Iteration 3006, loss = 0.09623401\n",
      "Iteration 3007, loss = 0.09621751\n",
      "Iteration 3008, loss = 0.09620107\n",
      "Iteration 3009, loss = 0.09618467\n",
      "Iteration 3010, loss = 0.09616833\n",
      "Iteration 3011, loss = 0.09615204\n",
      "Iteration 3012, loss = 0.09613580\n",
      "Iteration 3013, loss = 0.09611961\n",
      "Iteration 3014, loss = 0.09610347\n",
      "Iteration 3015, loss = 0.09608737\n",
      "Iteration 3016, loss = 0.09607132\n",
      "Iteration 3017, loss = 0.09605532\n",
      "Iteration 3018, loss = 0.09603937\n",
      "Iteration 3019, loss = 0.09602346\n",
      "Iteration 3020, loss = 0.09600759\n",
      "Iteration 3021, loss = 0.09599177\n",
      "Iteration 3022, loss = 0.09597599\n",
      "Iteration 3023, loss = 0.09596026\n",
      "Iteration 3024, loss = 0.09594457\n",
      "Iteration 3025, loss = 0.09592892\n",
      "Iteration 3026, loss = 0.09591332\n",
      "Iteration 3027, loss = 0.09589775\n",
      "Iteration 3028, loss = 0.09588223\n",
      "Iteration 3029, loss = 0.09586675\n",
      "Iteration 3030, loss = 0.09585130\n",
      "Iteration 3031, loss = 0.09583590\n",
      "Iteration 3032, loss = 0.09582054\n",
      "Iteration 3033, loss = 0.09580521\n",
      "Iteration 3034, loss = 0.09578993\n",
      "Iteration 3035, loss = 0.09577468\n",
      "Iteration 3036, loss = 0.09575947\n",
      "Iteration 3037, loss = 0.09574430\n",
      "Iteration 3038, loss = 0.09572917\n",
      "Iteration 3039, loss = 0.09571407\n",
      "Iteration 3040, loss = 0.09569901\n",
      "Iteration 3041, loss = 0.09568399\n",
      "Iteration 3042, loss = 0.09566900\n",
      "Iteration 3043, loss = 0.09565405\n",
      "Iteration 3044, loss = 0.09563914\n",
      "Iteration 3045, loss = 0.09562426\n",
      "Iteration 3046, loss = 0.09560941\n",
      "Iteration 3047, loss = 0.09559461\n",
      "Iteration 3048, loss = 0.09557983\n",
      "Iteration 3049, loss = 0.09556510\n",
      "Iteration 3050, loss = 0.09555039\n",
      "Iteration 3051, loss = 0.09553572\n",
      "Iteration 3052, loss = 0.09552109\n",
      "Iteration 3053, loss = 0.09550649\n",
      "Iteration 3054, loss = 0.09549192\n",
      "Iteration 3055, loss = 0.09547739\n",
      "Iteration 3056, loss = 0.09546289\n",
      "Iteration 3057, loss = 0.09544842\n",
      "Iteration 3058, loss = 0.09543399\n",
      "Iteration 3059, loss = 0.09541959\n",
      "Iteration 3060, loss = 0.09540522\n",
      "Iteration 3061, loss = 0.09539089\n",
      "Iteration 3062, loss = 0.09537659\n",
      "Iteration 3063, loss = 0.09536232\n",
      "Iteration 3064, loss = 0.09534808\n",
      "Iteration 3065, loss = 0.09533388\n",
      "Iteration 3066, loss = 0.09531971\n",
      "Iteration 3067, loss = 0.09530557\n",
      "Iteration 3068, loss = 0.09529146\n",
      "Iteration 3069, loss = 0.09527739\n",
      "Iteration 3070, loss = 0.09526335\n",
      "Iteration 3071, loss = 0.09524934\n",
      "Iteration 3072, loss = 0.09523536\n",
      "Iteration 3073, loss = 0.09522141\n",
      "Iteration 3074, loss = 0.09520750\n",
      "Iteration 3075, loss = 0.09519361\n",
      "Iteration 3076, loss = 0.09517976\n",
      "Iteration 3077, loss = 0.09516594\n",
      "Iteration 3078, loss = 0.09515215\n",
      "Iteration 3079, loss = 0.09513839\n",
      "Iteration 3080, loss = 0.09512466\n",
      "Iteration 3081, loss = 0.09511096\n",
      "Iteration 3082, loss = 0.09509730\n",
      "Iteration 3083, loss = 0.09508366\n",
      "Iteration 3084, loss = 0.09507006\n",
      "Iteration 3085, loss = 0.09505649\n",
      "Iteration 3086, loss = 0.09504294\n",
      "Iteration 3087, loss = 0.09502943\n",
      "Iteration 3088, loss = 0.09501595\n",
      "Iteration 3089, loss = 0.09500250\n",
      "Iteration 3090, loss = 0.09498908\n",
      "Iteration 3091, loss = 0.09497569\n",
      "Iteration 3092, loss = 0.09496233\n",
      "Iteration 3093, loss = 0.09494900\n",
      "Iteration 3094, loss = 0.09493570\n",
      "Iteration 3095, loss = 0.09492244\n",
      "Iteration 3096, loss = 0.09490920\n",
      "Iteration 3097, loss = 0.09489599\n",
      "Iteration 3098, loss = 0.09488281\n",
      "Iteration 3099, loss = 0.09486966\n",
      "Iteration 3100, loss = 0.09485654\n",
      "Iteration 3101, loss = 0.09484346\n",
      "Iteration 3102, loss = 0.09483040\n",
      "Iteration 3103, loss = 0.09481737\n",
      "Iteration 3104, loss = 0.09480437\n",
      "Iteration 3105, loss = 0.09479140\n",
      "Iteration 3106, loss = 0.09477849\n",
      "Iteration 3107, loss = 0.09476561\n",
      "Iteration 3108, loss = 0.09475276\n",
      "Iteration 3109, loss = 0.09473994\n",
      "Iteration 3110, loss = 0.09472715\n",
      "Iteration 3111, loss = 0.09471440\n",
      "Iteration 3112, loss = 0.09470167\n",
      "Iteration 3113, loss = 0.09468898\n",
      "Iteration 3114, loss = 0.09467631\n",
      "Iteration 3115, loss = 0.09466367\n",
      "Iteration 3116, loss = 0.09465106\n",
      "Iteration 3117, loss = 0.09463848\n",
      "Iteration 3118, loss = 0.09462593\n",
      "Iteration 3119, loss = 0.09461341\n",
      "Iteration 3120, loss = 0.09460092\n",
      "Iteration 3121, loss = 0.09458846\n",
      "Iteration 3122, loss = 0.09457602\n",
      "Iteration 3123, loss = 0.09456361\n",
      "Iteration 3124, loss = 0.09455123\n",
      "Iteration 3125, loss = 0.09453888\n",
      "Iteration 3126, loss = 0.09452656\n",
      "Iteration 3127, loss = 0.09451426\n",
      "Iteration 3128, loss = 0.09450199\n",
      "Iteration 3129, loss = 0.09448975\n",
      "Iteration 3130, loss = 0.09447754\n",
      "Iteration 3131, loss = 0.09446535\n",
      "Iteration 3132, loss = 0.09445319\n",
      "Iteration 3133, loss = 0.09444106\n",
      "Iteration 3134, loss = 0.09442896\n",
      "Iteration 3135, loss = 0.09441688\n",
      "Iteration 3136, loss = 0.09440483\n",
      "Iteration 3137, loss = 0.09439281\n",
      "Iteration 3138, loss = 0.09438081\n",
      "Iteration 3139, loss = 0.09436884\n",
      "Iteration 3140, loss = 0.09435690\n",
      "Iteration 3141, loss = 0.09434499\n",
      "Iteration 3142, loss = 0.09433310\n",
      "Iteration 3143, loss = 0.09432124\n",
      "Iteration 3144, loss = 0.09430940\n",
      "Iteration 3145, loss = 0.09429759\n",
      "Iteration 3146, loss = 0.09428581\n",
      "Iteration 3147, loss = 0.09427406\n",
      "Iteration 3148, loss = 0.09426233\n",
      "Iteration 3149, loss = 0.09425062\n",
      "Iteration 3150, loss = 0.09423895\n",
      "Iteration 3151, loss = 0.09422730\n",
      "Iteration 3152, loss = 0.09421567\n",
      "Iteration 3153, loss = 0.09420408\n",
      "Iteration 3154, loss = 0.09419250\n",
      "Iteration 3155, loss = 0.09418096\n",
      "Iteration 3156, loss = 0.09416944\n",
      "Iteration 3157, loss = 0.09415795\n",
      "Iteration 3158, loss = 0.09414648\n",
      "Iteration 3159, loss = 0.09413504\n",
      "Iteration 3160, loss = 0.09412362\n",
      "Iteration 3161, loss = 0.09411223\n",
      "Iteration 3162, loss = 0.09410086\n",
      "Iteration 3163, loss = 0.09408952\n",
      "Iteration 3164, loss = 0.09407821\n",
      "Iteration 3165, loss = 0.09406692\n",
      "Iteration 3166, loss = 0.09405566\n",
      "Iteration 3167, loss = 0.09404442\n",
      "Iteration 3168, loss = 0.09403321\n",
      "Iteration 3169, loss = 0.09402202\n",
      "Iteration 3170, loss = 0.09401086\n",
      "Iteration 3171, loss = 0.09399973\n",
      "Iteration 3172, loss = 0.09398862\n",
      "Iteration 3173, loss = 0.09397753\n",
      "Iteration 3174, loss = 0.09396647\n",
      "Iteration 3175, loss = 0.09395543\n",
      "Iteration 3176, loss = 0.09394442\n",
      "Iteration 3177, loss = 0.09393344\n",
      "Iteration 3178, loss = 0.09392248\n",
      "Iteration 3179, loss = 0.09391154\n",
      "Iteration 3180, loss = 0.09390063\n",
      "Iteration 3181, loss = 0.09388975\n",
      "Iteration 3182, loss = 0.09387888\n",
      "Iteration 3183, loss = 0.09386805\n",
      "Iteration 3184, loss = 0.09385723\n",
      "Iteration 3185, loss = 0.09384645\n",
      "Iteration 3186, loss = 0.09383568\n",
      "Iteration 3187, loss = 0.09382495\n",
      "Iteration 3188, loss = 0.09381423\n",
      "Iteration 3189, loss = 0.09380354\n",
      "Iteration 3190, loss = 0.09379288\n",
      "Iteration 3191, loss = 0.09378223\n",
      "Iteration 3192, loss = 0.09377162\n",
      "Iteration 3193, loss = 0.09376102\n",
      "Iteration 3194, loss = 0.09375046\n",
      "Iteration 3195, loss = 0.09373991\n",
      "Iteration 3196, loss = 0.09372939\n",
      "Iteration 3197, loss = 0.09371889\n",
      "Iteration 3198, loss = 0.09370842\n",
      "Iteration 3199, loss = 0.09369797\n",
      "Iteration 3200, loss = 0.09368755\n",
      "Iteration 3201, loss = 0.09367715\n",
      "Iteration 3202, loss = 0.09366677\n",
      "Iteration 3203, loss = 0.09365641\n",
      "Iteration 3204, loss = 0.09364608\n",
      "Iteration 3205, loss = 0.09363578\n",
      "Iteration 3206, loss = 0.09362549\n",
      "Iteration 3207, loss = 0.09361523\n",
      "Iteration 3208, loss = 0.09360500\n",
      "Iteration 3209, loss = 0.09359479\n",
      "Iteration 3210, loss = 0.09358460\n",
      "Iteration 3211, loss = 0.09357443\n",
      "Iteration 3212, loss = 0.09356429\n",
      "Iteration 3213, loss = 0.09355417\n",
      "Iteration 3214, loss = 0.09354407\n",
      "Iteration 3215, loss = 0.09353400\n",
      "Iteration 3216, loss = 0.09352395\n",
      "Iteration 3217, loss = 0.09351392\n",
      "Iteration 3218, loss = 0.09350392\n",
      "Iteration 3219, loss = 0.09349394\n",
      "Iteration 3220, loss = 0.09348398\n",
      "Iteration 3221, loss = 0.09347404\n",
      "Iteration 3222, loss = 0.09346413\n",
      "Iteration 3223, loss = 0.09345424\n",
      "Iteration 3224, loss = 0.09344437\n",
      "Iteration 3225, loss = 0.09343453\n",
      "Iteration 3226, loss = 0.09342471\n",
      "Iteration 3227, loss = 0.09341491\n",
      "Iteration 3228, loss = 0.09340513\n",
      "Iteration 3229, loss = 0.09339537\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "MLP.score(X, y)=0.73\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def PlotMLP(mlp, X, y):\n",
    "\n",
    "    # NOTE: local function is such a nifty feature of Python!\n",
    "    def CalcPredAndScore(mlp, X, y):\n",
    "        y_pred_model1 = mlp.predict(X)\n",
    "\n",
    "        # call r2\n",
    "        score_model1 = r2_score(y, y_pred_model1)\n",
    "\n",
    "        return y_pred_model1, score_model1\n",
    "\n",
    "    y_pred_model1, score_model1 = CalcPredAndScore(\n",
    "        mlp, X, y)\n",
    "\n",
    "    plt.plot(X, y_pred_model1, \"r.-\")\n",
    "    plt.scatter(X, y)\n",
    "    plt.xlabel(\"GDP per capita\")\n",
    "    plt.ylabel(\"Life satisfaction\")\n",
    "    plt.legend([\"MLP\", \"X OECD data\"])\n",
    "    \n",
    "    print(f\"MLP.score(X, y)={score_model1:0.2f}\")\n",
    "\n",
    "normalizedX = (X-min(X))/(max(X)-min(X))\n",
    "\n",
    "# Setup MLPRegressor\n",
    "mlp.fit(normalizedX, y)\n",
    "\n",
    "# lets make a MLP regressor prediction and redo the plots\n",
    "\n",
    "PlotMLP(mlp, normalizedX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb) Scikit-learn Pipelines\n",
    "\n",
    "Now, rescale again, but use the `sklearn.preprocessing.MinMaxScaler`.\n",
    "\n",
    "When this works put both the MLP and the scaler into a composite construction via `sklearn.pipeline.Pipeline`. This composite is just a new Scikit-learn estimator, and can be used just like any other `fit-predict` models, try it, and document it for the journal.\n",
    "\n",
    "(You could reuse the `PlotModels()` function by also retraining the linear regressor on the scaled data, or just write your own plot code.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.42339562\n",
      "Iteration 2, loss = 19.39423028\n",
      "Iteration 3, loss = 19.36508278\n",
      "Iteration 4, loss = 19.33595344\n",
      "Iteration 5, loss = 19.30684256\n",
      "Iteration 6, loss = 19.27775039\n",
      "Iteration 7, loss = 19.24867567\n",
      "Iteration 8, loss = 19.21960129\n",
      "Iteration 9, loss = 19.19054516\n",
      "Iteration 10, loss = 19.16150760\n",
      "Iteration 11, loss = 19.13248884\n",
      "Iteration 12, loss = 19.10348908\n",
      "Iteration 13, loss = 19.07450849\n",
      "Iteration 14, loss = 19.04554720\n",
      "Iteration 15, loss = 19.01660532\n",
      "Iteration 16, loss = 18.98766703\n",
      "Iteration 17, loss = 18.95873930\n",
      "Iteration 18, loss = 18.92982942\n",
      "Iteration 19, loss = 18.90093751\n",
      "Iteration 20, loss = 18.87206368\n",
      "Iteration 21, loss = 18.84320798\n",
      "Iteration 22, loss = 18.81437042\n",
      "Iteration 23, loss = 18.78555099\n",
      "Iteration 24, loss = 18.75672912\n",
      "Iteration 25, loss = 18.72790735\n",
      "Iteration 26, loss = 18.69910061\n",
      "Iteration 27, loss = 18.67030901\n",
      "Iteration 28, loss = 18.64152294\n",
      "Iteration 29, loss = 18.61271952\n",
      "Iteration 30, loss = 18.58390099\n",
      "Iteration 31, loss = 18.55508202\n",
      "Iteration 32, loss = 18.52627233\n",
      "Iteration 33, loss = 18.49747224\n",
      "Iteration 34, loss = 18.46868195\n",
      "Iteration 35, loss = 18.43988355\n",
      "Iteration 36, loss = 18.41106456\n",
      "Iteration 37, loss = 18.38222300\n",
      "Iteration 38, loss = 18.35336474\n",
      "Iteration 39, loss = 18.32447083\n",
      "Iteration 40, loss = 18.29556000\n",
      "Iteration 41, loss = 18.26663534\n",
      "Iteration 42, loss = 18.23756210\n",
      "Iteration 43, loss = 18.20840336\n",
      "Iteration 44, loss = 18.17922880\n",
      "Iteration 45, loss = 18.15004001\n",
      "Iteration 46, loss = 18.12083834\n",
      "Iteration 47, loss = 18.09162489\n",
      "Iteration 48, loss = 18.06240053\n",
      "Iteration 49, loss = 18.03316596\n",
      "Iteration 50, loss = 18.00392173\n",
      "Iteration 51, loss = 17.97464999\n",
      "Iteration 52, loss = 17.94530685\n",
      "Iteration 53, loss = 17.91579165\n",
      "Iteration 54, loss = 17.88613803\n",
      "Iteration 55, loss = 17.85638643\n",
      "Iteration 56, loss = 17.82659949\n",
      "Iteration 57, loss = 17.79677956\n",
      "Iteration 58, loss = 17.76692856\n",
      "Iteration 59, loss = 17.73696081\n",
      "Iteration 60, loss = 17.70695099\n",
      "Iteration 61, loss = 17.67690722\n",
      "Iteration 62, loss = 17.64683103\n",
      "Iteration 63, loss = 17.61672368\n",
      "Iteration 64, loss = 17.58658620\n",
      "Iteration 65, loss = 17.55641946\n",
      "Iteration 66, loss = 17.52622415\n",
      "Iteration 67, loss = 17.49600085\n",
      "Iteration 68, loss = 17.46575001\n",
      "Iteration 69, loss = 17.43547203\n",
      "Iteration 70, loss = 17.40516718\n",
      "Iteration 71, loss = 17.37483572\n",
      "Iteration 72, loss = 17.34447783\n",
      "Iteration 73, loss = 17.31409363\n",
      "Iteration 74, loss = 17.28368323\n",
      "Iteration 75, loss = 17.25324669\n",
      "Iteration 76, loss = 17.22278406\n",
      "Iteration 77, loss = 17.19229534\n",
      "Iteration 78, loss = 17.16178053\n",
      "Iteration 79, loss = 17.13123962\n",
      "Iteration 80, loss = 17.10067256\n",
      "Iteration 81, loss = 17.07007933\n",
      "Iteration 82, loss = 17.03945986\n",
      "Iteration 83, loss = 17.00881409\n",
      "Iteration 84, loss = 16.97814197\n",
      "Iteration 85, loss = 16.94744341\n",
      "Iteration 86, loss = 16.91671836\n",
      "Iteration 87, loss = 16.88596673\n",
      "Iteration 88, loss = 16.85518845\n",
      "Iteration 89, loss = 16.82438344\n",
      "Iteration 90, loss = 16.79355162\n",
      "Iteration 91, loss = 16.76269293\n",
      "Iteration 92, loss = 16.73180728\n",
      "Iteration 93, loss = 16.70089460\n",
      "Iteration 94, loss = 16.66995482\n",
      "Iteration 95, loss = 16.63898787\n",
      "Iteration 96, loss = 16.60799369\n",
      "Iteration 97, loss = 16.57697221\n",
      "Iteration 98, loss = 16.54592336\n",
      "Iteration 99, loss = 16.51484709\n",
      "Iteration 100, loss = 16.48374334\n",
      "Iteration 101, loss = 16.45261206\n",
      "Iteration 102, loss = 16.42145320\n",
      "Iteration 103, loss = 16.39026671\n",
      "Iteration 104, loss = 16.35905255\n",
      "Iteration 105, loss = 16.32781069\n",
      "Iteration 106, loss = 16.29654107\n",
      "Iteration 107, loss = 16.26524368\n",
      "Iteration 108, loss = 16.23391848\n",
      "Iteration 109, loss = 16.20256545\n",
      "Iteration 110, loss = 16.17118455\n",
      "Iteration 111, loss = 16.13977579\n",
      "Iteration 112, loss = 16.10833913\n",
      "Iteration 113, loss = 16.07687457\n",
      "Iteration 114, loss = 16.04538210\n",
      "Iteration 115, loss = 16.01386171\n",
      "Iteration 116, loss = 15.98231341\n",
      "Iteration 117, loss = 15.95073719\n",
      "Iteration 118, loss = 15.91913306\n",
      "Iteration 119, loss = 15.88750103\n",
      "Iteration 120, loss = 15.85584110\n",
      "Iteration 121, loss = 15.82415331\n",
      "Iteration 122, loss = 15.79243765\n",
      "Iteration 123, loss = 15.76069416\n",
      "Iteration 124, loss = 15.72892287\n",
      "Iteration 125, loss = 15.69712379\n",
      "Iteration 126, loss = 15.66529696\n",
      "Iteration 127, loss = 15.63344241\n",
      "Iteration 128, loss = 15.60156019\n",
      "Iteration 129, loss = 15.56965033\n",
      "Iteration 130, loss = 15.53771288\n",
      "Iteration 131, loss = 15.50574789\n",
      "Iteration 132, loss = 15.47375540\n",
      "Iteration 133, loss = 15.44173547\n",
      "Iteration 134, loss = 15.40968815\n",
      "Iteration 135, loss = 15.37761351\n",
      "Iteration 136, loss = 15.34551160\n",
      "Iteration 137, loss = 15.31338250\n",
      "Iteration 138, loss = 15.28122626\n",
      "Iteration 139, loss = 15.24904297\n",
      "Iteration 140, loss = 15.21683269\n",
      "Iteration 141, loss = 15.18459550\n",
      "Iteration 142, loss = 15.15233148\n",
      "Iteration 143, loss = 15.12004072\n",
      "Iteration 144, loss = 15.08772329\n",
      "Iteration 145, loss = 15.05537929\n",
      "Iteration 146, loss = 15.02300881\n",
      "Iteration 147, loss = 14.99061194\n",
      "Iteration 148, loss = 14.95818878\n",
      "Iteration 149, loss = 14.92573943\n",
      "Iteration 150, loss = 14.89326399\n",
      "Iteration 151, loss = 14.86076255\n",
      "Iteration 152, loss = 14.82823524\n",
      "Iteration 153, loss = 14.79568217\n",
      "Iteration 154, loss = 14.76310343\n",
      "Iteration 155, loss = 14.73049915\n",
      "Iteration 156, loss = 14.69786945\n",
      "Iteration 157, loss = 14.66521445\n",
      "Iteration 158, loss = 14.63253427\n",
      "Iteration 159, loss = 14.59982903\n",
      "Iteration 160, loss = 14.56709886\n",
      "Iteration 161, loss = 14.53434390\n",
      "Iteration 162, loss = 14.50156428\n",
      "Iteration 163, loss = 14.46876013\n",
      "Iteration 164, loss = 14.43593159\n",
      "Iteration 165, loss = 14.40307881\n",
      "Iteration 166, loss = 14.37020192\n",
      "Iteration 167, loss = 14.33730107\n",
      "Iteration 168, loss = 14.30437641\n",
      "Iteration 169, loss = 14.27142810\n",
      "Iteration 170, loss = 14.23845627\n",
      "Iteration 171, loss = 14.20546110\n",
      "Iteration 172, loss = 14.17244273\n",
      "Iteration 173, loss = 14.13940133\n",
      "Iteration 174, loss = 14.10633706\n",
      "Iteration 175, loss = 14.07325008\n",
      "Iteration 176, loss = 14.04014057\n",
      "Iteration 177, loss = 14.00700869\n",
      "Iteration 178, loss = 13.97385462\n",
      "Iteration 179, loss = 13.94067852\n",
      "Iteration 180, loss = 13.90748058\n",
      "Iteration 181, loss = 13.87426098\n",
      "Iteration 182, loss = 13.84101989\n",
      "Iteration 183, loss = 13.80775750\n",
      "Iteration 184, loss = 13.77447400\n",
      "Iteration 185, loss = 13.74116957\n",
      "Iteration 186, loss = 13.70784441\n",
      "Iteration 187, loss = 13.67449870\n",
      "Iteration 188, loss = 13.64113265\n",
      "Iteration 189, loss = 13.60774645\n",
      "Iteration 190, loss = 13.57434029\n",
      "Iteration 191, loss = 13.54091439\n",
      "Iteration 192, loss = 13.50746894\n",
      "Iteration 193, loss = 13.47400415\n",
      "Iteration 194, loss = 13.44052023\n",
      "Iteration 195, loss = 13.40701739\n",
      "Iteration 196, loss = 13.37349584\n",
      "Iteration 197, loss = 13.33995579\n",
      "Iteration 198, loss = 13.30639747\n",
      "Iteration 199, loss = 13.27282109\n",
      "Iteration 200, loss = 13.23922687\n",
      "Iteration 201, loss = 13.20561503\n",
      "Iteration 202, loss = 13.17198581\n",
      "Iteration 203, loss = 13.13833942\n",
      "Iteration 204, loss = 13.10467610\n",
      "Iteration 205, loss = 13.07099607\n",
      "Iteration 206, loss = 13.03729958\n",
      "Iteration 207, loss = 13.00358685\n",
      "Iteration 208, loss = 12.96985812\n",
      "Iteration 209, loss = 12.93611364\n",
      "Iteration 210, loss = 12.90235364\n",
      "Iteration 211, loss = 12.86857836\n",
      "Iteration 212, loss = 12.83478806\n",
      "Iteration 213, loss = 12.80098298\n",
      "Iteration 214, loss = 12.76716336\n",
      "Iteration 215, loss = 12.73332947\n",
      "Iteration 216, loss = 12.69948154\n",
      "Iteration 217, loss = 12.66561985\n",
      "Iteration 218, loss = 12.63174463\n",
      "Iteration 219, loss = 12.59785616\n",
      "Iteration 220, loss = 12.56395470\n",
      "Iteration 221, loss = 12.53004050\n",
      "Iteration 222, loss = 12.49611383\n",
      "Iteration 223, loss = 12.46217495\n",
      "Iteration 224, loss = 12.42822414\n",
      "Iteration 225, loss = 12.39426166\n",
      "Iteration 226, loss = 12.36028779\n",
      "Iteration 227, loss = 12.32630280\n",
      "Iteration 228, loss = 12.29230697\n",
      "Iteration 229, loss = 12.25830056\n",
      "Iteration 230, loss = 12.22428387\n",
      "Iteration 231, loss = 12.19025716\n",
      "Iteration 232, loss = 12.15622073\n",
      "Iteration 233, loss = 12.12217486\n",
      "Iteration 234, loss = 12.08811983\n",
      "Iteration 235, loss = 12.05405594\n",
      "Iteration 236, loss = 12.01998346\n",
      "Iteration 237, loss = 11.98590269\n",
      "Iteration 238, loss = 11.95181393\n",
      "Iteration 239, loss = 11.91771747\n",
      "Iteration 240, loss = 11.88361360\n",
      "Iteration 241, loss = 11.84950262\n",
      "Iteration 242, loss = 11.81538483\n",
      "Iteration 243, loss = 11.78126054\n",
      "Iteration 244, loss = 11.74713004\n",
      "Iteration 245, loss = 11.71299364\n",
      "Iteration 246, loss = 11.67885164\n",
      "Iteration 247, loss = 11.64470436\n",
      "Iteration 248, loss = 11.61055210\n",
      "Iteration 249, loss = 11.57639516\n",
      "Iteration 250, loss = 11.54223387\n",
      "Iteration 251, loss = 11.50806854\n",
      "Iteration 252, loss = 11.47389948\n",
      "Iteration 253, loss = 11.43972701\n",
      "Iteration 254, loss = 11.40555145\n",
      "Iteration 255, loss = 11.37137311\n",
      "Iteration 256, loss = 11.33719232\n",
      "Iteration 257, loss = 11.30300940\n",
      "Iteration 258, loss = 11.26882467\n",
      "Iteration 259, loss = 11.23463846\n",
      "Iteration 260, loss = 11.20045110\n",
      "Iteration 261, loss = 11.16626291\n",
      "Iteration 262, loss = 11.13207423\n",
      "Iteration 263, loss = 11.09788538\n",
      "Iteration 264, loss = 11.06369670\n",
      "Iteration 265, loss = 11.02950853\n",
      "Iteration 266, loss = 10.99532118\n",
      "Iteration 267, loss = 10.96113502\n",
      "Iteration 268, loss = 10.92695036\n",
      "Iteration 269, loss = 10.89276755\n",
      "Iteration 270, loss = 10.85858694\n",
      "Iteration 271, loss = 10.82440885\n",
      "Iteration 272, loss = 10.79023365\n",
      "Iteration 273, loss = 10.75606166\n",
      "Iteration 274, loss = 10.72189324\n",
      "Iteration 275, loss = 10.68772873\n",
      "Iteration 276, loss = 10.65356848\n",
      "Iteration 277, loss = 10.61941285\n",
      "Iteration 278, loss = 10.58526217\n",
      "Iteration 279, loss = 10.55111681\n",
      "Iteration 280, loss = 10.51697711\n",
      "Iteration 281, loss = 10.48284343\n",
      "Iteration 282, loss = 10.44871612\n",
      "Iteration 283, loss = 10.41459555\n",
      "Iteration 284, loss = 10.38048206\n",
      "Iteration 285, loss = 10.34637602\n",
      "Iteration 286, loss = 10.31227779\n",
      "Iteration 287, loss = 10.27818773\n",
      "Iteration 288, loss = 10.24410619\n",
      "Iteration 289, loss = 10.21003355\n",
      "Iteration 290, loss = 10.17597016\n",
      "Iteration 291, loss = 10.14191640\n",
      "Iteration 292, loss = 10.10787262\n",
      "Iteration 293, loss = 10.07383920\n",
      "Iteration 294, loss = 10.03981650\n",
      "Iteration 295, loss = 10.00580489\n",
      "Iteration 296, loss = 9.97180475\n",
      "Iteration 297, loss = 9.93781643\n",
      "Iteration 298, loss = 9.90384032\n",
      "Iteration 299, loss = 9.86987679\n",
      "Iteration 300, loss = 9.83592621\n",
      "Iteration 301, loss = 9.80198895\n",
      "Iteration 302, loss = 9.76806540\n",
      "Iteration 303, loss = 9.73415592\n",
      "Iteration 304, loss = 9.70026090\n",
      "Iteration 305, loss = 9.66638071\n",
      "Iteration 306, loss = 9.63251574\n",
      "Iteration 307, loss = 9.59866635\n",
      "Iteration 308, loss = 9.56483294\n",
      "Iteration 309, loss = 9.53101588\n",
      "Iteration 310, loss = 9.49721556\n",
      "Iteration 311, loss = 9.46343235\n",
      "Iteration 312, loss = 9.42966665\n",
      "Iteration 313, loss = 9.39591883\n",
      "Iteration 314, loss = 9.36218928\n",
      "Iteration 315, loss = 9.32847839\n",
      "Iteration 316, loss = 9.29478655\n",
      "Iteration 317, loss = 9.26111413\n",
      "Iteration 318, loss = 9.22746153\n",
      "Iteration 319, loss = 9.19382914\n",
      "Iteration 320, loss = 9.16021735\n",
      "Iteration 321, loss = 9.12662653\n",
      "Iteration 322, loss = 9.09305710\n",
      "Iteration 323, loss = 9.05950943\n",
      "Iteration 324, loss = 9.02598391\n",
      "Iteration 325, loss = 8.99248095\n",
      "Iteration 326, loss = 8.95900092\n",
      "Iteration 327, loss = 8.92554423\n",
      "Iteration 328, loss = 8.89211127\n",
      "Iteration 329, loss = 8.85870243\n",
      "Iteration 330, loss = 8.82531810\n",
      "Iteration 331, loss = 8.79195869\n",
      "Iteration 332, loss = 8.75862458\n",
      "Iteration 333, loss = 8.72531617\n",
      "Iteration 334, loss = 8.69203385\n",
      "Iteration 335, loss = 8.65877804\n",
      "Iteration 336, loss = 8.62554911\n",
      "Iteration 337, loss = 8.59234746\n",
      "Iteration 338, loss = 8.55917351\n",
      "Iteration 339, loss = 8.52602763\n",
      "Iteration 340, loss = 8.49291023\n",
      "Iteration 341, loss = 8.45982172\n",
      "Iteration 342, loss = 8.42676248\n",
      "Iteration 343, loss = 8.39373291\n",
      "Iteration 344, loss = 8.36073343\n",
      "Iteration 345, loss = 8.32776441\n",
      "Iteration 346, loss = 8.29482627\n",
      "Iteration 347, loss = 8.26191940\n",
      "Iteration 348, loss = 8.22904421\n",
      "Iteration 349, loss = 8.19620109\n",
      "Iteration 350, loss = 8.16339045\n",
      "Iteration 351, loss = 8.13061267\n",
      "Iteration 352, loss = 8.09786817\n",
      "Iteration 353, loss = 8.06515735\n",
      "Iteration 354, loss = 8.03248060\n",
      "Iteration 355, loss = 7.99983833\n",
      "Iteration 356, loss = 7.96723093\n",
      "Iteration 357, loss = 7.93465881\n",
      "Iteration 358, loss = 7.90212237\n",
      "Iteration 359, loss = 7.86962200\n",
      "Iteration 360, loss = 7.83715811\n",
      "Iteration 361, loss = 7.80473110\n",
      "Iteration 362, loss = 7.77234137\n",
      "Iteration 363, loss = 7.73998932\n",
      "Iteration 364, loss = 7.70767535\n",
      "Iteration 365, loss = 7.67539986\n",
      "Iteration 366, loss = 7.64316324\n",
      "Iteration 367, loss = 7.61096591\n",
      "Iteration 368, loss = 7.57880825\n",
      "Iteration 369, loss = 7.54669067\n",
      "Iteration 370, loss = 7.51461357\n",
      "Iteration 371, loss = 7.48257734\n",
      "Iteration 372, loss = 7.45058238\n",
      "Iteration 373, loss = 7.41862910\n",
      "Iteration 374, loss = 7.38671789\n",
      "Iteration 375, loss = 7.35484914\n",
      "Iteration 376, loss = 7.32302326\n",
      "Iteration 377, loss = 7.29124064\n",
      "Iteration 378, loss = 7.25950168\n",
      "Iteration 379, loss = 7.22780677\n",
      "Iteration 380, loss = 7.19615631\n",
      "Iteration 381, loss = 7.16455070\n",
      "Iteration 382, loss = 7.13299033\n",
      "Iteration 383, loss = 7.10147559\n",
      "Iteration 384, loss = 7.07000688\n",
      "Iteration 385, loss = 7.03858459\n",
      "Iteration 386, loss = 7.00720911\n",
      "Iteration 387, loss = 6.97588084\n",
      "Iteration 388, loss = 6.94460017\n",
      "Iteration 389, loss = 6.91336749\n",
      "Iteration 390, loss = 6.88218318\n",
      "Iteration 391, loss = 6.85104765\n",
      "Iteration 392, loss = 6.81996127\n",
      "Iteration 393, loss = 6.78892444\n",
      "Iteration 394, loss = 6.75793754\n",
      "Iteration 395, loss = 6.72700096\n",
      "Iteration 396, loss = 6.69611510\n",
      "Iteration 397, loss = 6.66528032\n",
      "Iteration 398, loss = 6.63449702\n",
      "Iteration 399, loss = 6.60376559\n",
      "Iteration 400, loss = 6.57308640\n",
      "Iteration 401, loss = 6.54245983\n",
      "Iteration 402, loss = 6.51188628\n",
      "Iteration 403, loss = 6.48136611\n",
      "Iteration 404, loss = 6.45089971\n",
      "Iteration 405, loss = 6.42048746\n",
      "Iteration 406, loss = 6.39012973\n",
      "Iteration 407, loss = 6.35982691\n",
      "Iteration 408, loss = 6.32957936\n",
      "Iteration 409, loss = 6.29938747\n",
      "Iteration 410, loss = 6.26925160\n",
      "Iteration 411, loss = 6.23917213\n",
      "Iteration 412, loss = 6.20914943\n",
      "Iteration 413, loss = 6.17918387\n",
      "Iteration 414, loss = 6.14927582\n",
      "Iteration 415, loss = 6.11942566\n",
      "Iteration 416, loss = 6.08963373\n",
      "Iteration 417, loss = 6.05990042\n",
      "Iteration 418, loss = 6.03022609\n",
      "Iteration 419, loss = 6.00061110\n",
      "Iteration 420, loss = 5.97105581\n",
      "Iteration 421, loss = 5.94156058\n",
      "Iteration 422, loss = 5.91212578\n",
      "Iteration 423, loss = 5.88275175\n",
      "Iteration 424, loss = 5.85343887\n",
      "Iteration 425, loss = 5.82418748\n",
      "Iteration 426, loss = 5.79499795\n",
      "Iteration 427, loss = 5.76587061\n",
      "Iteration 428, loss = 5.73680583\n",
      "Iteration 429, loss = 5.70780395\n",
      "Iteration 430, loss = 5.67886533\n",
      "Iteration 431, loss = 5.64999031\n",
      "Iteration 432, loss = 5.62117923\n",
      "Iteration 433, loss = 5.59243244\n",
      "Iteration 434, loss = 5.56375029\n",
      "Iteration 435, loss = 5.53513311\n",
      "Iteration 436, loss = 5.50658124\n",
      "Iteration 437, loss = 5.47809503\n",
      "Iteration 438, loss = 5.44967480\n",
      "Iteration 439, loss = 5.42132090\n",
      "Iteration 440, loss = 5.39303366\n",
      "Iteration 441, loss = 5.36481340\n",
      "Iteration 442, loss = 5.33666046\n",
      "Iteration 443, loss = 5.30857516\n",
      "Iteration 444, loss = 5.28055783\n",
      "Iteration 445, loss = 5.25260880\n",
      "Iteration 446, loss = 5.22472839\n",
      "Iteration 447, loss = 5.19691692\n",
      "Iteration 448, loss = 5.16917470\n",
      "Iteration 449, loss = 5.14150206\n",
      "Iteration 450, loss = 5.11389932\n",
      "Iteration 451, loss = 5.08636677\n",
      "Iteration 452, loss = 5.05890474\n",
      "Iteration 453, loss = 5.03151353\n",
      "Iteration 454, loss = 5.00419346\n",
      "Iteration 455, loss = 4.97694482\n",
      "Iteration 456, loss = 4.94976793\n",
      "Iteration 457, loss = 4.92266308\n",
      "Iteration 458, loss = 4.89563057\n",
      "Iteration 459, loss = 4.86867070\n",
      "Iteration 460, loss = 4.84178376\n",
      "Iteration 461, loss = 4.81497006\n",
      "Iteration 462, loss = 4.78822987\n",
      "Iteration 463, loss = 4.76156349\n",
      "Iteration 464, loss = 4.73497120\n",
      "Iteration 465, loss = 4.70845330\n",
      "Iteration 466, loss = 4.68201006\n",
      "Iteration 467, loss = 4.65564176\n",
      "Iteration 468, loss = 4.62934868\n",
      "Iteration 469, loss = 4.60313109\n",
      "Iteration 470, loss = 4.57698928\n",
      "Iteration 471, loss = 4.55092350\n",
      "Iteration 472, loss = 4.52493404\n",
      "Iteration 473, loss = 4.49902115\n",
      "Iteration 474, loss = 4.47318510\n",
      "Iteration 475, loss = 4.44742615\n",
      "Iteration 476, loss = 4.42174456\n",
      "Iteration 477, loss = 4.39614059\n",
      "Iteration 478, loss = 4.37061449\n",
      "Iteration 479, loss = 4.34516652\n",
      "Iteration 480, loss = 4.31979692\n",
      "Iteration 481, loss = 4.29450594\n",
      "Iteration 482, loss = 4.26929382\n",
      "Iteration 483, loss = 4.24416081\n",
      "Iteration 484, loss = 4.21910715\n",
      "Iteration 485, loss = 4.19413308\n",
      "Iteration 486, loss = 4.16923882\n",
      "Iteration 487, loss = 4.14442461\n",
      "Iteration 488, loss = 4.11969068\n",
      "Iteration 489, loss = 4.09503725\n",
      "Iteration 490, loss = 4.07046455\n",
      "Iteration 491, loss = 4.04597281\n",
      "Iteration 492, loss = 4.02156223\n",
      "Iteration 493, loss = 3.99723304\n",
      "Iteration 494, loss = 3.97298544\n",
      "Iteration 495, loss = 3.94881966\n",
      "Iteration 496, loss = 3.92473588\n",
      "Iteration 497, loss = 3.90073433\n",
      "Iteration 498, loss = 3.87681521\n",
      "Iteration 499, loss = 3.85297870\n",
      "Iteration 500, loss = 3.82922501\n",
      "Iteration 501, loss = 3.80555434\n",
      "Iteration 502, loss = 3.78196687\n",
      "Iteration 503, loss = 3.75846279\n",
      "Iteration 504, loss = 3.73504228\n",
      "Iteration 505, loss = 3.71170553\n",
      "Iteration 506, loss = 3.68845272\n",
      "Iteration 507, loss = 3.66528403\n",
      "Iteration 508, loss = 3.64219962\n",
      "Iteration 509, loss = 3.61919967\n",
      "Iteration 510, loss = 3.59628435\n",
      "Iteration 511, loss = 3.57345382\n",
      "Iteration 512, loss = 3.55070824\n",
      "Iteration 513, loss = 3.52804777\n",
      "Iteration 514, loss = 3.50547256\n",
      "Iteration 515, loss = 3.48298278\n",
      "Iteration 516, loss = 3.46057857\n",
      "Iteration 517, loss = 3.43826007\n",
      "Iteration 518, loss = 3.41602743\n",
      "Iteration 519, loss = 3.39388079\n",
      "Iteration 520, loss = 3.37182028\n",
      "Iteration 521, loss = 3.34984605\n",
      "Iteration 522, loss = 3.32795822\n",
      "Iteration 523, loss = 3.30615692\n",
      "Iteration 524, loss = 3.28444228\n",
      "Iteration 525, loss = 3.26281442\n",
      "Iteration 526, loss = 3.24127345\n",
      "Iteration 527, loss = 3.21981950\n",
      "Iteration 528, loss = 3.19845267\n",
      "Iteration 529, loss = 3.17717307\n",
      "Iteration 530, loss = 3.15598082\n",
      "Iteration 531, loss = 3.13487601\n",
      "Iteration 532, loss = 3.11385874\n",
      "Iteration 533, loss = 3.09292912\n",
      "Iteration 534, loss = 3.07208722\n",
      "Iteration 535, loss = 3.05133316\n",
      "Iteration 536, loss = 3.03066700\n",
      "Iteration 537, loss = 3.01008884\n",
      "Iteration 538, loss = 2.98959875\n",
      "Iteration 539, loss = 2.96919681\n",
      "Iteration 540, loss = 2.94888311\n",
      "Iteration 541, loss = 2.92865770\n",
      "Iteration 542, loss = 2.90852066\n",
      "Iteration 543, loss = 2.88847204\n",
      "Iteration 544, loss = 2.86851192\n",
      "Iteration 545, loss = 2.84864035\n",
      "Iteration 546, loss = 2.82885738\n",
      "Iteration 547, loss = 2.80916307\n",
      "Iteration 548, loss = 2.78955745\n",
      "Iteration 549, loss = 2.77004059\n",
      "Iteration 550, loss = 2.75061252\n",
      "Iteration 551, loss = 2.73127327\n",
      "Iteration 552, loss = 2.71202289\n",
      "Iteration 553, loss = 2.69286140\n",
      "Iteration 554, loss = 2.67378884\n",
      "Iteration 555, loss = 2.65480522\n",
      "Iteration 556, loss = 2.63591057\n",
      "Iteration 557, loss = 2.61710491\n",
      "Iteration 558, loss = 2.59838825\n",
      "Iteration 559, loss = 2.57976061\n",
      "Iteration 560, loss = 2.56122200\n",
      "Iteration 561, loss = 2.54277241\n",
      "Iteration 562, loss = 2.52441185\n",
      "Iteration 563, loss = 2.50614032\n",
      "Iteration 564, loss = 2.48795782\n",
      "Iteration 565, loss = 2.46986434\n",
      "Iteration 566, loss = 2.45185986\n",
      "Iteration 567, loss = 2.43394438\n",
      "Iteration 568, loss = 2.41611787\n",
      "Iteration 569, loss = 2.39838031\n",
      "Iteration 570, loss = 2.38073168\n",
      "Iteration 571, loss = 2.36317196\n",
      "Iteration 572, loss = 2.34570110\n",
      "Iteration 573, loss = 2.32831908\n",
      "Iteration 574, loss = 2.31102586\n",
      "Iteration 575, loss = 2.29382141\n",
      "Iteration 576, loss = 2.27670566\n",
      "Iteration 577, loss = 2.25967859\n",
      "Iteration 578, loss = 2.24274013\n",
      "Iteration 579, loss = 2.22589024\n",
      "Iteration 580, loss = 2.20912885\n",
      "Iteration 581, loss = 2.19245591\n",
      "Iteration 582, loss = 2.17587136\n",
      "Iteration 583, loss = 2.15937513\n",
      "Iteration 584, loss = 2.14296714\n",
      "Iteration 585, loss = 2.12664733\n",
      "Iteration 586, loss = 2.11041562\n",
      "Iteration 587, loss = 2.09427193\n",
      "Iteration 588, loss = 2.07821617\n",
      "Iteration 589, loss = 2.06224827\n",
      "Iteration 590, loss = 2.04636814\n",
      "Iteration 591, loss = 2.03057568\n",
      "Iteration 592, loss = 2.01487079\n",
      "Iteration 593, loss = 1.99925338\n",
      "Iteration 594, loss = 1.98372336\n",
      "Iteration 595, loss = 1.96828060\n",
      "Iteration 596, loss = 1.95292502\n",
      "Iteration 597, loss = 1.93765649\n",
      "Iteration 598, loss = 1.92247490\n",
      "Iteration 599, loss = 1.90738014\n",
      "Iteration 600, loss = 1.89237208\n",
      "Iteration 601, loss = 1.87745061\n",
      "Iteration 602, loss = 1.86261560\n",
      "Iteration 603, loss = 1.84786692\n",
      "Iteration 604, loss = 1.83320444\n",
      "Iteration 605, loss = 1.81862802\n",
      "Iteration 606, loss = 1.80413752\n",
      "Iteration 607, loss = 1.78973282\n",
      "Iteration 608, loss = 1.77541375\n",
      "Iteration 609, loss = 1.76118019\n",
      "Iteration 610, loss = 1.74703197\n",
      "Iteration 611, loss = 1.73296894\n",
      "Iteration 612, loss = 1.71899096\n",
      "Iteration 613, loss = 1.70509785\n",
      "Iteration 614, loss = 1.69128947\n",
      "Iteration 615, loss = 1.67756565\n",
      "Iteration 616, loss = 1.66392622\n",
      "Iteration 617, loss = 1.65037101\n",
      "Iteration 618, loss = 1.63689985\n",
      "Iteration 619, loss = 1.62351257\n",
      "Iteration 620, loss = 1.61020899\n",
      "Iteration 621, loss = 1.59698893\n",
      "Iteration 622, loss = 1.58385220\n",
      "Iteration 623, loss = 1.57079863\n",
      "Iteration 624, loss = 1.55782802\n",
      "Iteration 625, loss = 1.54494018\n",
      "Iteration 626, loss = 1.53213492\n",
      "Iteration 627, loss = 1.51941204\n",
      "Iteration 628, loss = 1.50677135\n",
      "Iteration 629, loss = 1.49421264\n",
      "Iteration 630, loss = 1.48173571\n",
      "Iteration 631, loss = 1.46934035\n",
      "Iteration 632, loss = 1.45702636\n",
      "Iteration 633, loss = 1.44479353\n",
      "Iteration 634, loss = 1.43264163\n",
      "Iteration 635, loss = 1.42057046\n",
      "Iteration 636, loss = 1.40857980\n",
      "Iteration 637, loss = 1.39666942\n",
      "Iteration 638, loss = 1.38483910\n",
      "Iteration 639, loss = 1.37308863\n",
      "Iteration 640, loss = 1.36141776\n",
      "Iteration 641, loss = 1.34982626\n",
      "Iteration 642, loss = 1.33831392\n",
      "Iteration 643, loss = 1.32688049\n",
      "Iteration 644, loss = 1.31552573\n",
      "Iteration 645, loss = 1.30424941\n",
      "Iteration 646, loss = 1.29305128\n",
      "Iteration 647, loss = 1.28193110\n",
      "Iteration 648, loss = 1.27088863\n",
      "Iteration 649, loss = 1.25992362\n",
      "Iteration 650, loss = 1.24903582\n",
      "Iteration 651, loss = 1.23822497\n",
      "Iteration 652, loss = 1.22749082\n",
      "Iteration 653, loss = 1.21683312\n",
      "Iteration 654, loss = 1.20625161\n",
      "Iteration 655, loss = 1.19574603\n",
      "Iteration 656, loss = 1.18531612\n",
      "Iteration 657, loss = 1.17496161\n",
      "Iteration 658, loss = 1.16468224\n",
      "Iteration 659, loss = 1.15447774\n",
      "Iteration 660, loss = 1.14434783\n",
      "Iteration 661, loss = 1.13429226\n",
      "Iteration 662, loss = 1.12431075\n",
      "Iteration 663, loss = 1.11440302\n",
      "Iteration 664, loss = 1.10456880\n",
      "Iteration 665, loss = 1.09480780\n",
      "Iteration 666, loss = 1.08511976\n",
      "Iteration 667, loss = 1.07550438\n",
      "Iteration 668, loss = 1.06596138\n",
      "Iteration 669, loss = 1.05649049\n",
      "Iteration 670, loss = 1.04709140\n",
      "Iteration 671, loss = 1.03776385\n",
      "Iteration 672, loss = 1.02850753\n",
      "Iteration 673, loss = 1.01932216\n",
      "Iteration 674, loss = 1.01020744\n",
      "Iteration 675, loss = 1.00116308\n",
      "Iteration 676, loss = 0.99218879\n",
      "Iteration 677, loss = 0.98328426\n",
      "Iteration 678, loss = 0.97444921\n",
      "Iteration 679, loss = 0.96568333\n",
      "Iteration 680, loss = 0.95698632\n",
      "Iteration 681, loss = 0.94835788\n",
      "Iteration 682, loss = 0.93979771\n",
      "Iteration 683, loss = 0.93130550\n",
      "Iteration 684, loss = 0.92288095\n",
      "Iteration 685, loss = 0.91452375\n",
      "Iteration 686, loss = 0.90623360\n",
      "Iteration 687, loss = 0.89801018\n",
      "Iteration 688, loss = 0.88985319\n",
      "Iteration 689, loss = 0.88176231\n",
      "Iteration 690, loss = 0.87373724\n",
      "Iteration 691, loss = 0.86577766\n",
      "Iteration 692, loss = 0.85788325\n",
      "Iteration 693, loss = 0.85005371\n",
      "Iteration 694, loss = 0.84228871\n",
      "Iteration 695, loss = 0.83458795\n",
      "Iteration 696, loss = 0.82695110\n",
      "Iteration 697, loss = 0.81937784\n",
      "Iteration 698, loss = 0.81186786\n",
      "Iteration 699, loss = 0.80442084\n",
      "Iteration 700, loss = 0.79703645\n",
      "Iteration 701, loss = 0.78971438\n",
      "Iteration 702, loss = 0.78245430\n",
      "Iteration 703, loss = 0.77525590\n",
      "Iteration 704, loss = 0.76811884\n",
      "Iteration 705, loss = 0.76104280\n",
      "Iteration 706, loss = 0.75402747\n",
      "Iteration 707, loss = 0.74707251\n",
      "Iteration 708, loss = 0.74017760\n",
      "Iteration 709, loss = 0.73334242\n",
      "Iteration 710, loss = 0.72656664\n",
      "Iteration 711, loss = 0.71984992\n",
      "Iteration 712, loss = 0.71319195\n",
      "Iteration 713, loss = 0.70659240\n",
      "Iteration 714, loss = 0.70005094\n",
      "Iteration 715, loss = 0.69356724\n",
      "Iteration 716, loss = 0.68714097\n",
      "Iteration 717, loss = 0.68077181\n",
      "Iteration 718, loss = 0.67445942\n",
      "Iteration 719, loss = 0.66820348\n",
      "Iteration 720, loss = 0.66200365\n",
      "Iteration 721, loss = 0.65585961\n",
      "Iteration 722, loss = 0.64977103\n",
      "Iteration 723, loss = 0.64373758\n",
      "Iteration 724, loss = 0.63775893\n",
      "Iteration 725, loss = 0.63183474\n",
      "Iteration 726, loss = 0.62596469\n",
      "Iteration 727, loss = 0.62014845\n",
      "Iteration 728, loss = 0.61438568\n",
      "Iteration 729, loss = 0.60867607\n",
      "Iteration 730, loss = 0.60301927\n",
      "Iteration 731, loss = 0.59741496\n",
      "Iteration 732, loss = 0.59186281\n",
      "Iteration 733, loss = 0.58636248\n",
      "Iteration 734, loss = 0.58091366\n",
      "Iteration 735, loss = 0.57551601\n",
      "Iteration 736, loss = 0.57016919\n",
      "Iteration 737, loss = 0.56487289\n",
      "Iteration 738, loss = 0.55962678\n",
      "Iteration 739, loss = 0.55443052\n",
      "Iteration 740, loss = 0.54928379\n",
      "Iteration 741, loss = 0.54418625\n",
      "Iteration 742, loss = 0.53913759\n",
      "Iteration 743, loss = 0.53413748\n",
      "Iteration 744, loss = 0.52918558\n",
      "Iteration 745, loss = 0.52428158\n",
      "Iteration 746, loss = 0.51942515\n",
      "Iteration 747, loss = 0.51461595\n",
      "Iteration 748, loss = 0.50985368\n",
      "Iteration 749, loss = 0.50513799\n",
      "Iteration 750, loss = 0.50046858\n",
      "Iteration 751, loss = 0.49584511\n",
      "Iteration 752, loss = 0.49126727\n",
      "Iteration 753, loss = 0.48673473\n",
      "Iteration 754, loss = 0.48224716\n",
      "Iteration 755, loss = 0.47780426\n",
      "Iteration 756, loss = 0.47340570\n",
      "Iteration 757, loss = 0.46905115\n",
      "Iteration 758, loss = 0.46474031\n",
      "Iteration 759, loss = 0.46047284\n",
      "Iteration 760, loss = 0.45624845\n",
      "Iteration 761, loss = 0.45206680\n",
      "Iteration 762, loss = 0.44792758\n",
      "Iteration 763, loss = 0.44383048\n",
      "Iteration 764, loss = 0.43977518\n",
      "Iteration 765, loss = 0.43576138\n",
      "Iteration 766, loss = 0.43178875\n",
      "Iteration 767, loss = 0.42785698\n",
      "Iteration 768, loss = 0.42396577\n",
      "Iteration 769, loss = 0.42011481\n",
      "Iteration 770, loss = 0.41630377\n",
      "Iteration 771, loss = 0.41253237\n",
      "Iteration 772, loss = 0.40880028\n",
      "Iteration 773, loss = 0.40510721\n",
      "Iteration 774, loss = 0.40145284\n",
      "Iteration 775, loss = 0.39783688\n",
      "Iteration 776, loss = 0.39425902\n",
      "Iteration 777, loss = 0.39071895\n",
      "Iteration 778, loss = 0.38721638\n",
      "Iteration 779, loss = 0.38375101\n",
      "Iteration 780, loss = 0.38032253\n",
      "Iteration 781, loss = 0.37693065\n",
      "Iteration 782, loss = 0.37357507\n",
      "Iteration 783, loss = 0.37025550\n",
      "Iteration 784, loss = 0.36697163\n",
      "Iteration 785, loss = 0.36372319\n",
      "Iteration 786, loss = 0.36050986\n",
      "Iteration 787, loss = 0.35733137\n",
      "Iteration 788, loss = 0.35418742\n",
      "Iteration 789, loss = 0.35107773\n",
      "Iteration 790, loss = 0.34800199\n",
      "Iteration 791, loss = 0.34495994\n",
      "Iteration 792, loss = 0.34195128\n",
      "Iteration 793, loss = 0.33897573\n",
      "Iteration 794, loss = 0.33603300\n",
      "Iteration 795, loss = 0.33312282\n",
      "Iteration 796, loss = 0.33024489\n",
      "Iteration 797, loss = 0.32739896\n",
      "Iteration 798, loss = 0.32458472\n",
      "Iteration 799, loss = 0.32180192\n",
      "Iteration 800, loss = 0.31905027\n",
      "Iteration 801, loss = 0.31632950\n",
      "Iteration 802, loss = 0.31363934\n",
      "Iteration 803, loss = 0.31097951\n",
      "Iteration 804, loss = 0.30834974\n",
      "Iteration 805, loss = 0.30574978\n",
      "Iteration 806, loss = 0.30317934\n",
      "Iteration 807, loss = 0.30063816\n",
      "Iteration 808, loss = 0.29812598\n",
      "Iteration 809, loss = 0.29564253\n",
      "Iteration 810, loss = 0.29318756\n",
      "Iteration 811, loss = 0.29076080\n",
      "Iteration 812, loss = 0.28836199\n",
      "Iteration 813, loss = 0.28599088\n",
      "Iteration 814, loss = 0.28364720\n",
      "Iteration 815, loss = 0.28133071\n",
      "Iteration 816, loss = 0.27904115\n",
      "Iteration 817, loss = 0.27677826\n",
      "Iteration 818, loss = 0.27454181\n",
      "Iteration 819, loss = 0.27233153\n",
      "Iteration 820, loss = 0.27014718\n",
      "Iteration 821, loss = 0.26798851\n",
      "Iteration 822, loss = 0.26585528\n",
      "Iteration 823, loss = 0.26374725\n",
      "Iteration 824, loss = 0.26166417\n",
      "Iteration 825, loss = 0.25960581\n",
      "Iteration 826, loss = 0.25757191\n",
      "Iteration 827, loss = 0.25556226\n",
      "Iteration 828, loss = 0.25357660\n",
      "Iteration 829, loss = 0.25161470\n",
      "Iteration 830, loss = 0.24967634\n",
      "Iteration 831, loss = 0.24776128\n",
      "Iteration 832, loss = 0.24586929\n",
      "Iteration 833, loss = 0.24400014\n",
      "Iteration 834, loss = 0.24215361\n",
      "Iteration 835, loss = 0.24032946\n",
      "Iteration 836, loss = 0.23852747\n",
      "Iteration 837, loss = 0.23674743\n",
      "Iteration 838, loss = 0.23498911\n",
      "Iteration 839, loss = 0.23325228\n",
      "Iteration 840, loss = 0.23153674\n",
      "Iteration 841, loss = 0.22984226\n",
      "Iteration 842, loss = 0.22816863\n",
      "Iteration 843, loss = 0.22651564\n",
      "Iteration 844, loss = 0.22488307\n",
      "Iteration 845, loss = 0.22327071\n",
      "Iteration 846, loss = 0.22167836\n",
      "Iteration 847, loss = 0.22010580\n",
      "Iteration 848, loss = 0.21855283\n",
      "Iteration 849, loss = 0.21701925\n",
      "Iteration 850, loss = 0.21550484\n",
      "Iteration 851, loss = 0.21400942\n",
      "Iteration 852, loss = 0.21253277\n",
      "Iteration 853, loss = 0.21107470\n",
      "Iteration 854, loss = 0.20963502\n",
      "Iteration 855, loss = 0.20821352\n",
      "Iteration 856, loss = 0.20681002\n",
      "Iteration 857, loss = 0.20542431\n",
      "Iteration 858, loss = 0.20405621\n",
      "Iteration 859, loss = 0.20270553\n",
      "Iteration 860, loss = 0.20137207\n",
      "Iteration 861, loss = 0.20005566\n",
      "Iteration 862, loss = 0.19875611\n",
      "Iteration 863, loss = 0.19747323\n",
      "Iteration 864, loss = 0.19620684\n",
      "Iteration 865, loss = 0.19495676\n",
      "Iteration 866, loss = 0.19372280\n",
      "Iteration 867, loss = 0.19250480\n",
      "Iteration 868, loss = 0.19130257\n",
      "Iteration 869, loss = 0.19011594\n",
      "Iteration 870, loss = 0.18894474\n",
      "Iteration 871, loss = 0.18778879\n",
      "Iteration 872, loss = 0.18664792\n",
      "Iteration 873, loss = 0.18552196\n",
      "Iteration 874, loss = 0.18441074\n",
      "Iteration 875, loss = 0.18331410\n",
      "Iteration 876, loss = 0.18223188\n",
      "Iteration 877, loss = 0.18116390\n",
      "Iteration 878, loss = 0.18011001\n",
      "Iteration 879, loss = 0.17907004\n",
      "Iteration 880, loss = 0.17804383\n",
      "Iteration 881, loss = 0.17703123\n",
      "Iteration 882, loss = 0.17603208\n",
      "Iteration 883, loss = 0.17504623\n",
      "Iteration 884, loss = 0.17407351\n",
      "Iteration 885, loss = 0.17311378\n",
      "Iteration 886, loss = 0.17216689\n",
      "Iteration 887, loss = 0.17123268\n",
      "Iteration 888, loss = 0.17031101\n",
      "Iteration 889, loss = 0.16940173\n",
      "Iteration 890, loss = 0.16850469\n",
      "Iteration 891, loss = 0.16761975\n",
      "Iteration 892, loss = 0.16674676\n",
      "Iteration 893, loss = 0.16588559\n",
      "Iteration 894, loss = 0.16503609\n",
      "Iteration 895, loss = 0.16419813\n",
      "Iteration 896, loss = 0.16337156\n",
      "Iteration 897, loss = 0.16255625\n",
      "Iteration 898, loss = 0.16175206\n",
      "Iteration 899, loss = 0.16095886\n",
      "Iteration 900, loss = 0.16017651\n",
      "Iteration 901, loss = 0.15940489\n",
      "Iteration 902, loss = 0.15864387\n",
      "Iteration 903, loss = 0.15789331\n",
      "Iteration 904, loss = 0.15715308\n",
      "Iteration 905, loss = 0.15642307\n",
      "Iteration 906, loss = 0.15570314\n",
      "Iteration 907, loss = 0.15499317\n",
      "Iteration 908, loss = 0.15429304\n",
      "Iteration 909, loss = 0.15360263\n",
      "Iteration 910, loss = 0.15292181\n",
      "Iteration 911, loss = 0.15225047\n",
      "Iteration 912, loss = 0.15158849\n",
      "Iteration 913, loss = 0.15093575\n",
      "Iteration 914, loss = 0.15029214\n",
      "Iteration 915, loss = 0.14965754\n",
      "Iteration 916, loss = 0.14903183\n",
      "Iteration 917, loss = 0.14841491\n",
      "Iteration 918, loss = 0.14780667\n",
      "Iteration 919, loss = 0.14720699\n",
      "Iteration 920, loss = 0.14661577\n",
      "Iteration 921, loss = 0.14603290\n",
      "Iteration 922, loss = 0.14545827\n",
      "Iteration 923, loss = 0.14489177\n",
      "Iteration 924, loss = 0.14433331\n",
      "Iteration 925, loss = 0.14378278\n",
      "Iteration 926, loss = 0.14324008\n",
      "Iteration 927, loss = 0.14270510\n",
      "Iteration 928, loss = 0.14217775\n",
      "Iteration 929, loss = 0.14165792\n",
      "Iteration 930, loss = 0.14114553\n",
      "Iteration 931, loss = 0.14064047\n",
      "Iteration 932, loss = 0.14014264\n",
      "Iteration 933, loss = 0.13965196\n",
      "Iteration 934, loss = 0.13916833\n",
      "Iteration 935, loss = 0.13869166\n",
      "Iteration 936, loss = 0.13822185\n",
      "Iteration 937, loss = 0.13775881\n",
      "Iteration 938, loss = 0.13730246\n",
      "Iteration 939, loss = 0.13685270\n",
      "Iteration 940, loss = 0.13640946\n",
      "Iteration 941, loss = 0.13597264\n",
      "Iteration 942, loss = 0.13554215\n",
      "Iteration 943, loss = 0.13511792\n",
      "Iteration 944, loss = 0.13469985\n",
      "Iteration 945, loss = 0.13428787\n",
      "Iteration 946, loss = 0.13388189\n",
      "Iteration 947, loss = 0.13348184\n",
      "Iteration 948, loss = 0.13308762\n",
      "Iteration 949, loss = 0.13269918\n",
      "Iteration 950, loss = 0.13231641\n",
      "Iteration 951, loss = 0.13193926\n",
      "Iteration 952, loss = 0.13156764\n",
      "Iteration 953, loss = 0.13120147\n",
      "Iteration 954, loss = 0.13084069\n",
      "Iteration 955, loss = 0.13048521\n",
      "Iteration 956, loss = 0.13013497\n",
      "Iteration 957, loss = 0.12978989\n",
      "Iteration 958, loss = 0.12944991\n",
      "Iteration 959, loss = 0.12911494\n",
      "Iteration 960, loss = 0.12878493\n",
      "Iteration 961, loss = 0.12845981\n",
      "Iteration 962, loss = 0.12813949\n",
      "Iteration 963, loss = 0.12782393\n",
      "Iteration 964, loss = 0.12751305\n",
      "Iteration 965, loss = 0.12720679\n",
      "Iteration 966, loss = 0.12690508\n",
      "Iteration 967, loss = 0.12660786\n",
      "Iteration 968, loss = 0.12631506\n",
      "Iteration 969, loss = 0.12602663\n",
      "Iteration 970, loss = 0.12574250\n",
      "Iteration 971, loss = 0.12546261\n",
      "Iteration 972, loss = 0.12518691\n",
      "Iteration 973, loss = 0.12491532\n",
      "Iteration 974, loss = 0.12464780\n",
      "Iteration 975, loss = 0.12438428\n",
      "Iteration 976, loss = 0.12412471\n",
      "Iteration 977, loss = 0.12386903\n",
      "Iteration 978, loss = 0.12361719\n",
      "Iteration 979, loss = 0.12336913\n",
      "Iteration 980, loss = 0.12312480\n",
      "Iteration 981, loss = 0.12288414\n",
      "Iteration 982, loss = 0.12264710\n",
      "Iteration 983, loss = 0.12241362\n",
      "Iteration 984, loss = 0.12218367\n",
      "Iteration 985, loss = 0.12195718\n",
      "Iteration 986, loss = 0.12173410\n",
      "Iteration 987, loss = 0.12151439\n",
      "Iteration 988, loss = 0.12129799\n",
      "Iteration 989, loss = 0.12108486\n",
      "Iteration 990, loss = 0.12087495\n",
      "Iteration 991, loss = 0.12066822\n",
      "Iteration 992, loss = 0.12046461\n",
      "Iteration 993, loss = 0.12026408\n",
      "Iteration 994, loss = 0.12006658\n",
      "Iteration 995, loss = 0.11987208\n",
      "Iteration 996, loss = 0.11968052\n",
      "Iteration 997, loss = 0.11949186\n",
      "Iteration 998, loss = 0.11930606\n",
      "Iteration 999, loss = 0.11912308\n",
      "Iteration 1000, loss = 0.11894287\n",
      "Iteration 1001, loss = 0.11876540\n",
      "Iteration 1002, loss = 0.11859062\n",
      "Iteration 1003, loss = 0.11841849\n",
      "Iteration 1004, loss = 0.11824897\n",
      "Iteration 1005, loss = 0.11808202\n",
      "Iteration 1006, loss = 0.11791761\n",
      "Iteration 1007, loss = 0.11775570\n",
      "Iteration 1008, loss = 0.11759624\n",
      "Iteration 1009, loss = 0.11743921\n",
      "Iteration 1010, loss = 0.11728456\n",
      "Iteration 1011, loss = 0.11713226\n",
      "Iteration 1012, loss = 0.11698228\n",
      "Iteration 1013, loss = 0.11683457\n",
      "Iteration 1014, loss = 0.11668910\n",
      "Iteration 1015, loss = 0.11654584\n",
      "Iteration 1016, loss = 0.11640476\n",
      "Iteration 1017, loss = 0.11626582\n",
      "Iteration 1018, loss = 0.11612899\n",
      "Iteration 1019, loss = 0.11599423\n",
      "Iteration 1020, loss = 0.11586152\n",
      "Iteration 1021, loss = 0.11573083\n",
      "Iteration 1022, loss = 0.11560211\n",
      "Iteration 1023, loss = 0.11547535\n",
      "Iteration 1024, loss = 0.11535051\n",
      "Iteration 1025, loss = 0.11522755\n",
      "Iteration 1026, loss = 0.11510647\n",
      "Iteration 1027, loss = 0.11498721\n",
      "Iteration 1028, loss = 0.11486976\n",
      "Iteration 1029, loss = 0.11475408\n",
      "Iteration 1030, loss = 0.11464016\n",
      "Iteration 1031, loss = 0.11452795\n",
      "Iteration 1032, loss = 0.11441744\n",
      "Iteration 1033, loss = 0.11430860\n",
      "Iteration 1034, loss = 0.11420140\n",
      "Iteration 1035, loss = 0.11409581\n",
      "Iteration 1036, loss = 0.11399182\n",
      "Iteration 1037, loss = 0.11388939\n",
      "Iteration 1038, loss = 0.11378850\n",
      "Iteration 1039, loss = 0.11368913\n",
      "Iteration 1040, loss = 0.11359125\n",
      "Iteration 1041, loss = 0.11349483\n",
      "Iteration 1042, loss = 0.11339986\n",
      "Iteration 1043, loss = 0.11330632\n",
      "Iteration 1044, loss = 0.11321417\n",
      "Iteration 1045, loss = 0.11312340\n",
      "Iteration 1046, loss = 0.11303398\n",
      "Iteration 1047, loss = 0.11294590\n",
      "Iteration 1048, loss = 0.11285913\n",
      "Iteration 1049, loss = 0.11277365\n",
      "Iteration 1050, loss = 0.11268944\n",
      "Iteration 1051, loss = 0.11260648\n",
      "Iteration 1052, loss = 0.11252474\n",
      "Iteration 1053, loss = 0.11244422\n",
      "Iteration 1054, loss = 0.11236489\n",
      "Iteration 1055, loss = 0.11228672\n",
      "Iteration 1056, loss = 0.11220971\n",
      "Iteration 1057, loss = 0.11213383\n",
      "Iteration 1058, loss = 0.11205907\n",
      "Iteration 1059, loss = 0.11198540\n",
      "Iteration 1060, loss = 0.11191281\n",
      "Iteration 1061, loss = 0.11184128\n",
      "Iteration 1062, loss = 0.11177080\n",
      "Iteration 1063, loss = 0.11170134\n",
      "Iteration 1064, loss = 0.11163289\n",
      "Iteration 1065, loss = 0.11156543\n",
      "Iteration 1066, loss = 0.11149895\n",
      "Iteration 1067, loss = 0.11143343\n",
      "Iteration 1068, loss = 0.11136885\n",
      "Iteration 1069, loss = 0.11130521\n",
      "Iteration 1070, loss = 0.11124247\n",
      "Iteration 1071, loss = 0.11118064\n",
      "Iteration 1072, loss = 0.11111969\n",
      "Iteration 1073, loss = 0.11105960\n",
      "Iteration 1074, loss = 0.11100037\n",
      "Iteration 1075, loss = 0.11094199\n",
      "Iteration 1076, loss = 0.11088442\n",
      "Iteration 1077, loss = 0.11082767\n",
      "Iteration 1078, loss = 0.11077172\n",
      "Iteration 1079, loss = 0.11071656\n",
      "Iteration 1080, loss = 0.11066216\n",
      "Iteration 1081, loss = 0.11060853\n",
      "Iteration 1082, loss = 0.11055564\n",
      "Iteration 1083, loss = 0.11050348\n",
      "Iteration 1084, loss = 0.11045205\n",
      "Iteration 1085, loss = 0.11040133\n",
      "Iteration 1086, loss = 0.11035130\n",
      "Iteration 1087, loss = 0.11030196\n",
      "Iteration 1088, loss = 0.11025329\n",
      "Iteration 1089, loss = 0.11020529\n",
      "Iteration 1090, loss = 0.11015793\n",
      "Iteration 1091, loss = 0.11011122\n",
      "Iteration 1092, loss = 0.11006514\n",
      "Iteration 1093, loss = 0.11001967\n",
      "Iteration 1094, loss = 0.10997481\n",
      "Iteration 1095, loss = 0.10993055\n",
      "Iteration 1096, loss = 0.10988688\n",
      "Iteration 1097, loss = 0.10984379\n",
      "Iteration 1098, loss = 0.10980126\n",
      "Iteration 1099, loss = 0.10975929\n",
      "Iteration 1100, loss = 0.10971787\n",
      "Iteration 1101, loss = 0.10967698\n",
      "Iteration 1102, loss = 0.10963663\n",
      "Iteration 1103, loss = 0.10959679\n",
      "Iteration 1104, loss = 0.10955747\n",
      "Iteration 1105, loss = 0.10951865\n",
      "Iteration 1106, loss = 0.10948032\n",
      "Iteration 1107, loss = 0.10944248\n",
      "Iteration 1108, loss = 0.10940512\n",
      "Iteration 1109, loss = 0.10936822\n",
      "Iteration 1110, loss = 0.10933178\n",
      "Iteration 1111, loss = 0.10929580\n",
      "Iteration 1112, loss = 0.10926026\n",
      "Iteration 1113, loss = 0.10922515\n",
      "Iteration 1114, loss = 0.10919048\n",
      "Iteration 1115, loss = 0.10915622\n",
      "Iteration 1116, loss = 0.10912238\n",
      "Iteration 1117, loss = 0.10908895\n",
      "Iteration 1118, loss = 0.10905592\n",
      "Iteration 1119, loss = 0.10902328\n",
      "Iteration 1120, loss = 0.10899102\n",
      "Iteration 1121, loss = 0.10895914\n",
      "Iteration 1122, loss = 0.10892764\n",
      "Iteration 1123, loss = 0.10889650\n",
      "Iteration 1124, loss = 0.10886572\n",
      "Iteration 1125, loss = 0.10883529\n",
      "Iteration 1126, loss = 0.10880521\n",
      "Iteration 1127, loss = 0.10877547\n",
      "Iteration 1128, loss = 0.10874607\n",
      "Iteration 1129, loss = 0.10871699\n",
      "Iteration 1130, loss = 0.10868823\n",
      "Iteration 1131, loss = 0.10865979\n",
      "Iteration 1132, loss = 0.10863167\n",
      "Iteration 1133, loss = 0.10860384\n",
      "Iteration 1134, loss = 0.10857632\n",
      "Iteration 1135, loss = 0.10854909\n",
      "Iteration 1136, loss = 0.10852216\n",
      "Iteration 1137, loss = 0.10849550\n",
      "Iteration 1138, loss = 0.10846913\n",
      "Iteration 1139, loss = 0.10844302\n",
      "Iteration 1140, loss = 0.10841719\n",
      "Iteration 1141, loss = 0.10839162\n",
      "Iteration 1142, loss = 0.10836631\n",
      "Iteration 1143, loss = 0.10834126\n",
      "Iteration 1144, loss = 0.10831646\n",
      "Iteration 1145, loss = 0.10829190\n",
      "Iteration 1146, loss = 0.10826758\n",
      "Iteration 1147, loss = 0.10824350\n",
      "Iteration 1148, loss = 0.10821965\n",
      "Iteration 1149, loss = 0.10819603\n",
      "Iteration 1150, loss = 0.10817264\n",
      "Iteration 1151, loss = 0.10814946\n",
      "Iteration 1152, loss = 0.10812650\n",
      "Iteration 1153, loss = 0.10810375\n",
      "Iteration 1154, loss = 0.10808122\n",
      "Iteration 1155, loss = 0.10805888\n",
      "Iteration 1156, loss = 0.10803675\n",
      "Iteration 1157, loss = 0.10801481\n",
      "Iteration 1158, loss = 0.10799306\n",
      "Iteration 1159, loss = 0.10797151\n",
      "Iteration 1160, loss = 0.10795014\n",
      "Iteration 1161, loss = 0.10792895\n",
      "Iteration 1162, loss = 0.10790795\n",
      "Iteration 1163, loss = 0.10788712\n",
      "Iteration 1164, loss = 0.10786646\n",
      "Iteration 1165, loss = 0.10784597\n",
      "Iteration 1166, loss = 0.10782565\n",
      "Iteration 1167, loss = 0.10780549\n",
      "Iteration 1168, loss = 0.10778549\n",
      "Iteration 1169, loss = 0.10776565\n",
      "Iteration 1170, loss = 0.10774597\n",
      "Iteration 1171, loss = 0.10772643\n",
      "Iteration 1172, loss = 0.10770705\n",
      "Iteration 1173, loss = 0.10768781\n",
      "Iteration 1174, loss = 0.10766871\n",
      "Iteration 1175, loss = 0.10764976\n",
      "Iteration 1176, loss = 0.10763094\n",
      "Iteration 1177, loss = 0.10761226\n",
      "Iteration 1178, loss = 0.10759371\n",
      "Iteration 1179, loss = 0.10757529\n",
      "Iteration 1180, loss = 0.10755700\n",
      "Iteration 1181, loss = 0.10753883\n",
      "Iteration 1182, loss = 0.10752079\n",
      "Iteration 1183, loss = 0.10750287\n",
      "Iteration 1184, loss = 0.10748507\n",
      "Iteration 1185, loss = 0.10746738\n",
      "Iteration 1186, loss = 0.10744981\n",
      "Iteration 1187, loss = 0.10743235\n",
      "Iteration 1188, loss = 0.10741500\n",
      "Iteration 1189, loss = 0.10739775\n",
      "Iteration 1190, loss = 0.10738062\n",
      "Iteration 1191, loss = 0.10736358\n",
      "Iteration 1192, loss = 0.10734665\n",
      "Iteration 1193, loss = 0.10732982\n",
      "Iteration 1194, loss = 0.10731309\n",
      "Iteration 1195, loss = 0.10729645\n",
      "Iteration 1196, loss = 0.10727990\n",
      "Iteration 1197, loss = 0.10726345\n",
      "Iteration 1198, loss = 0.10724709\n",
      "Iteration 1199, loss = 0.10723082\n",
      "Iteration 1200, loss = 0.10721463\n",
      "Iteration 1201, loss = 0.10719854\n",
      "Iteration 1202, loss = 0.10718252\n",
      "Iteration 1203, loss = 0.10716659\n",
      "Iteration 1204, loss = 0.10715074\n",
      "Iteration 1205, loss = 0.10713496\n",
      "Iteration 1206, loss = 0.10711927\n",
      "Iteration 1207, loss = 0.10710365\n",
      "Iteration 1208, loss = 0.10708810\n",
      "Iteration 1209, loss = 0.10707263\n",
      "Iteration 1210, loss = 0.10705723\n",
      "Iteration 1211, loss = 0.10704190\n",
      "Iteration 1212, loss = 0.10702665\n",
      "Iteration 1213, loss = 0.10701145\n",
      "Iteration 1214, loss = 0.10699633\n",
      "Iteration 1215, loss = 0.10698127\n",
      "Iteration 1216, loss = 0.10696627\n",
      "Iteration 1217, loss = 0.10695134\n",
      "Iteration 1218, loss = 0.10693647\n",
      "Iteration 1219, loss = 0.10692165\n",
      "Iteration 1220, loss = 0.10690690\n",
      "Iteration 1221, loss = 0.10689221\n",
      "Iteration 1222, loss = 0.10687757\n",
      "Iteration 1223, loss = 0.10686299\n",
      "Iteration 1224, loss = 0.10684846\n",
      "Iteration 1225, loss = 0.10683399\n",
      "Iteration 1226, loss = 0.10681956\n",
      "Iteration 1227, loss = 0.10680519\n",
      "Iteration 1228, loss = 0.10679087\n",
      "Iteration 1229, loss = 0.10677661\n",
      "Iteration 1230, loss = 0.10676238\n",
      "Iteration 1231, loss = 0.10674821\n",
      "Iteration 1232, loss = 0.10673408\n",
      "Iteration 1233, loss = 0.10672000\n",
      "Iteration 1234, loss = 0.10670597\n",
      "Iteration 1235, loss = 0.10669197\n",
      "Iteration 1236, loss = 0.10667802\n",
      "Iteration 1237, loss = 0.10666412\n",
      "Iteration 1238, loss = 0.10665025\n",
      "Iteration 1239, loss = 0.10663643\n",
      "Iteration 1240, loss = 0.10662264\n",
      "Iteration 1241, loss = 0.10660890\n",
      "Iteration 1242, loss = 0.10659519\n",
      "Iteration 1243, loss = 0.10658152\n",
      "Iteration 1244, loss = 0.10656789\n",
      "Iteration 1245, loss = 0.10655429\n",
      "Iteration 1246, loss = 0.10654073\n",
      "Iteration 1247, loss = 0.10652720\n",
      "Iteration 1248, loss = 0.10651371\n",
      "Iteration 1249, loss = 0.10650025\n",
      "Iteration 1250, loss = 0.10648683\n",
      "Iteration 1251, loss = 0.10647343\n",
      "Iteration 1252, loss = 0.10646007\n",
      "Iteration 1253, loss = 0.10644674\n",
      "Iteration 1254, loss = 0.10643343\n",
      "Iteration 1255, loss = 0.10642016\n",
      "Iteration 1256, loss = 0.10640692\n",
      "Iteration 1257, loss = 0.10639370\n",
      "Iteration 1258, loss = 0.10638052\n",
      "Iteration 1259, loss = 0.10636736\n",
      "Iteration 1260, loss = 0.10635422\n",
      "Iteration 1261, loss = 0.10634112\n",
      "Iteration 1262, loss = 0.10632804\n",
      "Iteration 1263, loss = 0.10631498\n",
      "Iteration 1264, loss = 0.10630195\n",
      "Iteration 1265, loss = 0.10628894\n",
      "Iteration 1266, loss = 0.10627596\n",
      "Iteration 1267, loss = 0.10626300\n",
      "Iteration 1268, loss = 0.10625007\n",
      "Iteration 1269, loss = 0.10623715\n",
      "Iteration 1270, loss = 0.10622426\n",
      "Iteration 1271, loss = 0.10621139\n",
      "Iteration 1272, loss = 0.10619854\n",
      "Iteration 1273, loss = 0.10618571\n",
      "Iteration 1274, loss = 0.10617290\n",
      "Iteration 1275, loss = 0.10616012\n",
      "Iteration 1276, loss = 0.10614735\n",
      "Iteration 1277, loss = 0.10613460\n",
      "Iteration 1278, loss = 0.10612187\n",
      "Iteration 1279, loss = 0.10610916\n",
      "Iteration 1280, loss = 0.10609646\n",
      "Iteration 1281, loss = 0.10608379\n",
      "Iteration 1282, loss = 0.10607113\n",
      "Iteration 1283, loss = 0.10605848\n",
      "Iteration 1284, loss = 0.10604586\n",
      "Iteration 1285, loss = 0.10603325\n",
      "Iteration 1286, loss = 0.10602066\n",
      "Iteration 1287, loss = 0.10600808\n",
      "Iteration 1288, loss = 0.10599552\n",
      "Iteration 1289, loss = 0.10598297\n",
      "Iteration 1290, loss = 0.10597044\n",
      "Iteration 1291, loss = 0.10595792\n",
      "Iteration 1292, loss = 0.10594542\n",
      "Iteration 1293, loss = 0.10593293\n",
      "Iteration 1294, loss = 0.10592046\n",
      "Iteration 1295, loss = 0.10590800\n",
      "Iteration 1296, loss = 0.10589555\n",
      "Iteration 1297, loss = 0.10588311\n",
      "Iteration 1298, loss = 0.10587069\n",
      "Iteration 1299, loss = 0.10585828\n",
      "Iteration 1300, loss = 0.10584588\n",
      "Iteration 1301, loss = 0.10583350\n",
      "Iteration 1302, loss = 0.10582113\n",
      "Iteration 1303, loss = 0.10580876\n",
      "Iteration 1304, loss = 0.10579641\n",
      "Iteration 1305, loss = 0.10578407\n",
      "Iteration 1306, loss = 0.10577174\n",
      "Iteration 1307, loss = 0.10575943\n",
      "Iteration 1308, loss = 0.10574712\n",
      "Iteration 1309, loss = 0.10573482\n",
      "Iteration 1310, loss = 0.10572254\n",
      "Iteration 1311, loss = 0.10571026\n",
      "Iteration 1312, loss = 0.10569799\n",
      "Iteration 1313, loss = 0.10568573\n",
      "Iteration 1314, loss = 0.10567349\n",
      "Iteration 1315, loss = 0.10566125\n",
      "Iteration 1316, loss = 0.10564902\n",
      "Iteration 1317, loss = 0.10563680\n",
      "Iteration 1318, loss = 0.10562458\n",
      "Iteration 1319, loss = 0.10561238\n",
      "Iteration 1320, loss = 0.10560019\n",
      "Iteration 1321, loss = 0.10558800\n",
      "Iteration 1322, loss = 0.10557582\n",
      "Iteration 1323, loss = 0.10556365\n",
      "Iteration 1324, loss = 0.10555149\n",
      "Iteration 1325, loss = 0.10553933\n",
      "Iteration 1326, loss = 0.10552719\n",
      "Iteration 1327, loss = 0.10551505\n",
      "Iteration 1328, loss = 0.10550292\n",
      "Iteration 1329, loss = 0.10549079\n",
      "Iteration 1330, loss = 0.10547867\n",
      "Iteration 1331, loss = 0.10546656\n",
      "Iteration 1332, loss = 0.10545446\n",
      "Iteration 1333, loss = 0.10544236\n",
      "Iteration 1334, loss = 0.10543027\n",
      "Iteration 1335, loss = 0.10541819\n",
      "Iteration 1336, loss = 0.10540611\n",
      "Iteration 1337, loss = 0.10539404\n",
      "Iteration 1338, loss = 0.10538197\n",
      "Iteration 1339, loss = 0.10536991\n",
      "Iteration 1340, loss = 0.10535786\n",
      "Iteration 1341, loss = 0.10534581\n",
      "Iteration 1342, loss = 0.10533377\n",
      "Iteration 1343, loss = 0.10532174\n",
      "Iteration 1344, loss = 0.10530971\n",
      "Iteration 1345, loss = 0.10529768\n",
      "Iteration 1346, loss = 0.10528567\n",
      "Iteration 1347, loss = 0.10527365\n",
      "Iteration 1348, loss = 0.10526164\n",
      "Iteration 1349, loss = 0.10524964\n",
      "Iteration 1350, loss = 0.10523764\n",
      "Iteration 1351, loss = 0.10522565\n",
      "Iteration 1352, loss = 0.10521366\n",
      "Iteration 1353, loss = 0.10520168\n",
      "Iteration 1354, loss = 0.10518970\n",
      "Iteration 1355, loss = 0.10517773\n",
      "Iteration 1356, loss = 0.10516576\n",
      "Iteration 1357, loss = 0.10515380\n",
      "Iteration 1358, loss = 0.10514184\n",
      "Iteration 1359, loss = 0.10512989\n",
      "Iteration 1360, loss = 0.10511794\n",
      "Iteration 1361, loss = 0.10510599\n",
      "Iteration 1362, loss = 0.10509405\n",
      "Iteration 1363, loss = 0.10508211\n",
      "Iteration 1364, loss = 0.10507018\n",
      "Iteration 1365, loss = 0.10505825\n",
      "Iteration 1366, loss = 0.10504633\n",
      "Iteration 1367, loss = 0.10503441\n",
      "Iteration 1368, loss = 0.10502249\n",
      "Iteration 1369, loss = 0.10501058\n",
      "Iteration 1370, loss = 0.10499867\n",
      "Iteration 1371, loss = 0.10498677\n",
      "Iteration 1372, loss = 0.10497487\n",
      "Iteration 1373, loss = 0.10496297\n",
      "Iteration 1374, loss = 0.10495108\n",
      "Iteration 1375, loss = 0.10493919\n",
      "Iteration 1376, loss = 0.10492731\n",
      "Iteration 1377, loss = 0.10491543\n",
      "Iteration 1378, loss = 0.10490355\n",
      "Iteration 1379, loss = 0.10489167\n",
      "Iteration 1380, loss = 0.10487980\n",
      "Iteration 1381, loss = 0.10486794\n",
      "Iteration 1382, loss = 0.10485607\n",
      "Iteration 1383, loss = 0.10484421\n",
      "Iteration 1384, loss = 0.10483236\n",
      "Iteration 1385, loss = 0.10482050\n",
      "Iteration 1386, loss = 0.10480865\n",
      "Iteration 1387, loss = 0.10479681\n",
      "Iteration 1388, loss = 0.10478496\n",
      "Iteration 1389, loss = 0.10477312\n",
      "Iteration 1390, loss = 0.10476129\n",
      "Iteration 1391, loss = 0.10474945\n",
      "Iteration 1392, loss = 0.10473762\n",
      "Iteration 1393, loss = 0.10472579\n",
      "Iteration 1394, loss = 0.10471397\n",
      "Iteration 1395, loss = 0.10470215\n",
      "Iteration 1396, loss = 0.10469033\n",
      "Iteration 1397, loss = 0.10467852\n",
      "Iteration 1398, loss = 0.10466670\n",
      "Iteration 1399, loss = 0.10465490\n",
      "Iteration 1400, loss = 0.10464309\n",
      "Iteration 1401, loss = 0.10463129\n",
      "Iteration 1402, loss = 0.10461949\n",
      "Iteration 1403, loss = 0.10460769\n",
      "Iteration 1404, loss = 0.10459590\n",
      "Iteration 1405, loss = 0.10458411\n",
      "Iteration 1406, loss = 0.10457232\n",
      "Iteration 1407, loss = 0.10456053\n",
      "Iteration 1408, loss = 0.10454875\n",
      "Iteration 1409, loss = 0.10453697\n",
      "Iteration 1410, loss = 0.10452519\n",
      "Iteration 1411, loss = 0.10451342\n",
      "Iteration 1412, loss = 0.10450165\n",
      "Iteration 1413, loss = 0.10448988\n",
      "Iteration 1414, loss = 0.10447811\n",
      "Iteration 1415, loss = 0.10446635\n",
      "Iteration 1416, loss = 0.10445459\n",
      "Iteration 1417, loss = 0.10444284\n",
      "Iteration 1418, loss = 0.10443108\n",
      "Iteration 1419, loss = 0.10441933\n",
      "Iteration 1420, loss = 0.10440758\n",
      "Iteration 1421, loss = 0.10439583\n",
      "Iteration 1422, loss = 0.10438409\n",
      "Iteration 1423, loss = 0.10437235\n",
      "Iteration 1424, loss = 0.10436061\n",
      "Iteration 1425, loss = 0.10434888\n",
      "Iteration 1426, loss = 0.10433714\n",
      "Iteration 1427, loss = 0.10432541\n",
      "Iteration 1428, loss = 0.10431369\n",
      "Iteration 1429, loss = 0.10430196\n",
      "Iteration 1430, loss = 0.10429024\n",
      "Iteration 1431, loss = 0.10427852\n",
      "Iteration 1432, loss = 0.10426680\n",
      "Iteration 1433, loss = 0.10425509\n",
      "Iteration 1434, loss = 0.10424338\n",
      "Iteration 1435, loss = 0.10423167\n",
      "Iteration 1436, loss = 0.10421996\n",
      "Iteration 1437, loss = 0.10420826\n",
      "Iteration 1438, loss = 0.10419656\n",
      "Iteration 1439, loss = 0.10418486\n",
      "Iteration 1440, loss = 0.10417316\n",
      "Iteration 1441, loss = 0.10416147\n",
      "Iteration 1442, loss = 0.10414978\n",
      "Iteration 1443, loss = 0.10413809\n",
      "Iteration 1444, loss = 0.10412640\n",
      "Iteration 1445, loss = 0.10411472\n",
      "Iteration 1446, loss = 0.10410304\n",
      "Iteration 1447, loss = 0.10409136\n",
      "Iteration 1448, loss = 0.10407969\n",
      "Iteration 1449, loss = 0.10406801\n",
      "Iteration 1450, loss = 0.10405634\n",
      "Iteration 1451, loss = 0.10404468\n",
      "Iteration 1452, loss = 0.10403301\n",
      "Iteration 1453, loss = 0.10402135\n",
      "Iteration 1454, loss = 0.10400969\n",
      "Iteration 1455, loss = 0.10399803\n",
      "Iteration 1456, loss = 0.10398638\n",
      "Iteration 1457, loss = 0.10397473\n",
      "Iteration 1458, loss = 0.10396308\n",
      "Iteration 1459, loss = 0.10395143\n",
      "Iteration 1460, loss = 0.10393979\n",
      "Iteration 1461, loss = 0.10392814\n",
      "Iteration 1462, loss = 0.10391650\n",
      "Iteration 1463, loss = 0.10390487\n",
      "Iteration 1464, loss = 0.10389323\n",
      "Iteration 1465, loss = 0.10388160\n",
      "Iteration 1466, loss = 0.10386997\n",
      "Iteration 1467, loss = 0.10385835\n",
      "Iteration 1468, loss = 0.10384672\n",
      "Iteration 1469, loss = 0.10383510\n",
      "Iteration 1470, loss = 0.10382349\n",
      "Iteration 1471, loss = 0.10381187\n",
      "Iteration 1472, loss = 0.10380026\n",
      "Iteration 1473, loss = 0.10378865\n",
      "Iteration 1474, loss = 0.10377704\n",
      "Iteration 1475, loss = 0.10376543\n",
      "Iteration 1476, loss = 0.10375383\n",
      "Iteration 1477, loss = 0.10374223\n",
      "Iteration 1478, loss = 0.10373063\n",
      "Iteration 1479, loss = 0.10371904\n",
      "Iteration 1480, loss = 0.10370745\n",
      "Iteration 1481, loss = 0.10369586\n",
      "Iteration 1482, loss = 0.10368427\n",
      "Iteration 1483, loss = 0.10367269\n",
      "Iteration 1484, loss = 0.10366111\n",
      "Iteration 1485, loss = 0.10364953\n",
      "Iteration 1486, loss = 0.10363795\n",
      "Iteration 1487, loss = 0.10362638\n",
      "Iteration 1488, loss = 0.10361481\n",
      "Iteration 1489, loss = 0.10360324\n",
      "Iteration 1490, loss = 0.10359167\n",
      "Iteration 1491, loss = 0.10358011\n",
      "Iteration 1492, loss = 0.10356855\n",
      "Iteration 1493, loss = 0.10355699\n",
      "Iteration 1494, loss = 0.10354544\n",
      "Iteration 1495, loss = 0.10353389\n",
      "Iteration 1496, loss = 0.10352234\n",
      "Iteration 1497, loss = 0.10351079\n",
      "Iteration 1498, loss = 0.10349925\n",
      "Iteration 1499, loss = 0.10348771\n",
      "Iteration 1500, loss = 0.10347617\n",
      "Iteration 1501, loss = 0.10346463\n",
      "Iteration 1502, loss = 0.10345310\n",
      "Iteration 1503, loss = 0.10344157\n",
      "Iteration 1504, loss = 0.10343004\n",
      "Iteration 1505, loss = 0.10341852\n",
      "Iteration 1506, loss = 0.10340700\n",
      "Iteration 1507, loss = 0.10339548\n",
      "Iteration 1508, loss = 0.10338396\n",
      "Iteration 1509, loss = 0.10337245\n",
      "Iteration 1510, loss = 0.10336094\n",
      "Iteration 1511, loss = 0.10334943\n",
      "Iteration 1512, loss = 0.10333792\n",
      "Iteration 1513, loss = 0.10332642\n",
      "Iteration 1514, loss = 0.10331492\n",
      "Iteration 1515, loss = 0.10330342\n",
      "Iteration 1516, loss = 0.10329193\n",
      "Iteration 1517, loss = 0.10328044\n",
      "Iteration 1518, loss = 0.10326895\n",
      "Iteration 1519, loss = 0.10325747\n",
      "Iteration 1520, loss = 0.10324598\n",
      "Iteration 1521, loss = 0.10323450\n",
      "Iteration 1522, loss = 0.10322303\n",
      "Iteration 1523, loss = 0.10321155\n",
      "Iteration 1524, loss = 0.10320008\n",
      "Iteration 1525, loss = 0.10318861\n",
      "Iteration 1526, loss = 0.10317715\n",
      "Iteration 1527, loss = 0.10316569\n",
      "Iteration 1528, loss = 0.10315423\n",
      "Iteration 1529, loss = 0.10314277\n",
      "Iteration 1530, loss = 0.10313132\n",
      "Iteration 1531, loss = 0.10311987\n",
      "Iteration 1532, loss = 0.10310842\n",
      "Iteration 1533, loss = 0.10309697\n",
      "Iteration 1534, loss = 0.10308553\n",
      "Iteration 1535, loss = 0.10307409\n",
      "Iteration 1536, loss = 0.10306266\n",
      "Iteration 1537, loss = 0.10305122\n",
      "Iteration 1538, loss = 0.10303979\n",
      "Iteration 1539, loss = 0.10302837\n",
      "Iteration 1540, loss = 0.10301694\n",
      "Iteration 1541, loss = 0.10300552\n",
      "Iteration 1542, loss = 0.10299410\n",
      "Iteration 1543, loss = 0.10298269\n",
      "Iteration 1544, loss = 0.10297128\n",
      "Iteration 1545, loss = 0.10295987\n",
      "Iteration 1546, loss = 0.10294846\n",
      "Iteration 1547, loss = 0.10293706\n",
      "Iteration 1548, loss = 0.10292566\n",
      "Iteration 1549, loss = 0.10291426\n",
      "Iteration 1550, loss = 0.10290287\n",
      "Iteration 1551, loss = 0.10289148\n",
      "Iteration 1552, loss = 0.10288009\n",
      "Iteration 1553, loss = 0.10286870\n",
      "Iteration 1554, loss = 0.10285732\n",
      "Iteration 1555, loss = 0.10284594\n",
      "Iteration 1556, loss = 0.10283457\n",
      "Iteration 1557, loss = 0.10282319\n",
      "Iteration 1558, loss = 0.10281183\n",
      "Iteration 1559, loss = 0.10280046\n",
      "Iteration 1560, loss = 0.10278910\n",
      "Iteration 1561, loss = 0.10277774\n",
      "Iteration 1562, loss = 0.10276638\n",
      "Iteration 1563, loss = 0.10275503\n",
      "Iteration 1564, loss = 0.10274367\n",
      "Iteration 1565, loss = 0.10273233\n",
      "Iteration 1566, loss = 0.10272098\n",
      "Iteration 1567, loss = 0.10270964\n",
      "Iteration 1568, loss = 0.10269830\n",
      "Iteration 1569, loss = 0.10268697\n",
      "Iteration 1570, loss = 0.10267564\n",
      "Iteration 1571, loss = 0.10266431\n",
      "Iteration 1572, loss = 0.10265298\n",
      "Iteration 1573, loss = 0.10264166\n",
      "Iteration 1574, loss = 0.10263034\n",
      "Iteration 1575, loss = 0.10261903\n",
      "Iteration 1576, loss = 0.10260772\n",
      "Iteration 1577, loss = 0.10259641\n",
      "Iteration 1578, loss = 0.10258510\n",
      "Iteration 1579, loss = 0.10257380\n",
      "Iteration 1580, loss = 0.10256250\n",
      "Iteration 1581, loss = 0.10255120\n",
      "Iteration 1582, loss = 0.10253991\n",
      "Iteration 1583, loss = 0.10252862\n",
      "Iteration 1584, loss = 0.10251734\n",
      "Iteration 1585, loss = 0.10250605\n",
      "Iteration 1586, loss = 0.10249477\n",
      "Iteration 1587, loss = 0.10248350\n",
      "Iteration 1588, loss = 0.10247222\n",
      "Iteration 1589, loss = 0.10246095\n",
      "Iteration 1590, loss = 0.10244969\n",
      "Iteration 1591, loss = 0.10243842\n",
      "Iteration 1592, loss = 0.10242717\n",
      "Iteration 1593, loss = 0.10241591\n",
      "Iteration 1594, loss = 0.10240466\n",
      "Iteration 1595, loss = 0.10239341\n",
      "Iteration 1596, loss = 0.10238216\n",
      "Iteration 1597, loss = 0.10237092\n",
      "Iteration 1598, loss = 0.10235968\n",
      "Iteration 1599, loss = 0.10234844\n",
      "Iteration 1600, loss = 0.10233721\n",
      "Iteration 1601, loss = 0.10232598\n",
      "Iteration 1602, loss = 0.10231476\n",
      "Iteration 1603, loss = 0.10230353\n",
      "Iteration 1604, loss = 0.10229231\n",
      "Iteration 1605, loss = 0.10228110\n",
      "Iteration 1606, loss = 0.10226989\n",
      "Iteration 1607, loss = 0.10225868\n",
      "Iteration 1608, loss = 0.10224747\n",
      "Iteration 1609, loss = 0.10223627\n",
      "Iteration 1610, loss = 0.10222507\n",
      "Iteration 1611, loss = 0.10221388\n",
      "Iteration 1612, loss = 0.10220269\n",
      "Iteration 1613, loss = 0.10219150\n",
      "Iteration 1614, loss = 0.10218032\n",
      "Iteration 1615, loss = 0.10216914\n",
      "Iteration 1616, loss = 0.10215796\n",
      "Iteration 1617, loss = 0.10214678\n",
      "Iteration 1618, loss = 0.10213561\n",
      "Iteration 1619, loss = 0.10212445\n",
      "Iteration 1620, loss = 0.10211329\n",
      "Iteration 1621, loss = 0.10210213\n",
      "Iteration 1622, loss = 0.10209097\n",
      "Iteration 1623, loss = 0.10207982\n",
      "Iteration 1624, loss = 0.10206867\n",
      "Iteration 1625, loss = 0.10205752\n",
      "Iteration 1626, loss = 0.10204638\n",
      "Iteration 1627, loss = 0.10203524\n",
      "Iteration 1628, loss = 0.10202411\n",
      "Iteration 1629, loss = 0.10201298\n",
      "Iteration 1630, loss = 0.10200185\n",
      "Iteration 1631, loss = 0.10199073\n",
      "Iteration 1632, loss = 0.10197961\n",
      "Iteration 1633, loss = 0.10196849\n",
      "Iteration 1634, loss = 0.10195738\n",
      "Iteration 1635, loss = 0.10194627\n",
      "Iteration 1636, loss = 0.10193517\n",
      "Iteration 1637, loss = 0.10192406\n",
      "Iteration 1638, loss = 0.10191297\n",
      "Iteration 1639, loss = 0.10190187\n",
      "Iteration 1640, loss = 0.10189078\n",
      "Iteration 1641, loss = 0.10187970\n",
      "Iteration 1642, loss = 0.10186861\n",
      "Iteration 1643, loss = 0.10185753\n",
      "Iteration 1644, loss = 0.10184646\n",
      "Iteration 1645, loss = 0.10183539\n",
      "Iteration 1646, loss = 0.10182432\n",
      "Iteration 1647, loss = 0.10181325\n",
      "Iteration 1648, loss = 0.10180219\n",
      "Iteration 1649, loss = 0.10179113\n",
      "Iteration 1650, loss = 0.10178008\n",
      "Iteration 1651, loss = 0.10176903\n",
      "Iteration 1652, loss = 0.10175799\n",
      "Iteration 1653, loss = 0.10174694\n",
      "Iteration 1654, loss = 0.10173591\n",
      "Iteration 1655, loss = 0.10172487\n",
      "Iteration 1656, loss = 0.10171384\n",
      "Iteration 1657, loss = 0.10170281\n",
      "Iteration 1658, loss = 0.10169179\n",
      "Iteration 1659, loss = 0.10168077\n",
      "Iteration 1660, loss = 0.10166976\n",
      "Iteration 1661, loss = 0.10165875\n",
      "Iteration 1662, loss = 0.10164774\n",
      "Iteration 1663, loss = 0.10163673\n",
      "Iteration 1664, loss = 0.10162573\n",
      "Iteration 1665, loss = 0.10161474\n",
      "Iteration 1666, loss = 0.10160375\n",
      "Iteration 1667, loss = 0.10159276\n",
      "Iteration 1668, loss = 0.10158177\n",
      "Iteration 1669, loss = 0.10157079\n",
      "Iteration 1670, loss = 0.10155982\n",
      "Iteration 1671, loss = 0.10154884\n",
      "Iteration 1672, loss = 0.10153787\n",
      "Iteration 1673, loss = 0.10152691\n",
      "Iteration 1674, loss = 0.10151595\n",
      "Iteration 1675, loss = 0.10150499\n",
      "Iteration 1676, loss = 0.10149404\n",
      "Iteration 1677, loss = 0.10148309\n",
      "Iteration 1678, loss = 0.10147214\n",
      "Iteration 1679, loss = 0.10146120\n",
      "Iteration 1680, loss = 0.10145027\n",
      "Iteration 1681, loss = 0.10143933\n",
      "Iteration 1682, loss = 0.10142840\n",
      "Iteration 1683, loss = 0.10141748\n",
      "Iteration 1684, loss = 0.10140656\n",
      "Iteration 1685, loss = 0.10139564\n",
      "Iteration 1686, loss = 0.10138472\n",
      "Iteration 1687, loss = 0.10137382\n",
      "Iteration 1688, loss = 0.10136291\n",
      "Iteration 1689, loss = 0.10135201\n",
      "Iteration 1690, loss = 0.10134111\n",
      "Iteration 1691, loss = 0.10133022\n",
      "Iteration 1692, loss = 0.10131933\n",
      "Iteration 1693, loss = 0.10130844\n",
      "Iteration 1694, loss = 0.10129756\n",
      "Iteration 1695, loss = 0.10128669\n",
      "Iteration 1696, loss = 0.10127581\n",
      "Iteration 1697, loss = 0.10126494\n",
      "Iteration 1698, loss = 0.10125408\n",
      "Iteration 1699, loss = 0.10124322\n",
      "Iteration 1700, loss = 0.10123236\n",
      "Iteration 1701, loss = 0.10122151\n",
      "Iteration 1702, loss = 0.10121066\n",
      "Iteration 1703, loss = 0.10119982\n",
      "Iteration 1704, loss = 0.10118898\n",
      "Iteration 1705, loss = 0.10117814\n",
      "Iteration 1706, loss = 0.10116731\n",
      "Iteration 1707, loss = 0.10115648\n",
      "Iteration 1708, loss = 0.10114566\n",
      "Iteration 1709, loss = 0.10113484\n",
      "Iteration 1710, loss = 0.10112402\n",
      "Iteration 1711, loss = 0.10111321\n",
      "Iteration 1712, loss = 0.10110240\n",
      "Iteration 1713, loss = 0.10109160\n",
      "Iteration 1714, loss = 0.10108080\n",
      "Iteration 1715, loss = 0.10107001\n",
      "Iteration 1716, loss = 0.10105922\n",
      "Iteration 1717, loss = 0.10104843\n",
      "Iteration 1718, loss = 0.10103765\n",
      "Iteration 1719, loss = 0.10102687\n",
      "Iteration 1720, loss = 0.10101610\n",
      "Iteration 1721, loss = 0.10100533\n",
      "Iteration 1722, loss = 0.10099456\n",
      "Iteration 1723, loss = 0.10098380\n",
      "Iteration 1724, loss = 0.10097305\n",
      "Iteration 1725, loss = 0.10096230\n",
      "Iteration 1726, loss = 0.10095155\n",
      "Iteration 1727, loss = 0.10094080\n",
      "Iteration 1728, loss = 0.10093006\n",
      "Iteration 1729, loss = 0.10091933\n",
      "Iteration 1730, loss = 0.10090860\n",
      "Iteration 1731, loss = 0.10089787\n",
      "Iteration 1732, loss = 0.10088715\n",
      "Iteration 1733, loss = 0.10087643\n",
      "Iteration 1734, loss = 0.10086572\n",
      "Iteration 1735, loss = 0.10085501\n",
      "Iteration 1736, loss = 0.10084431\n",
      "Iteration 1737, loss = 0.10083360\n",
      "Iteration 1738, loss = 0.10082291\n",
      "Iteration 1739, loss = 0.10081222\n",
      "Iteration 1740, loss = 0.10080153\n",
      "Iteration 1741, loss = 0.10079085\n",
      "Iteration 1742, loss = 0.10078017\n",
      "Iteration 1743, loss = 0.10076949\n",
      "Iteration 1744, loss = 0.10075882\n",
      "Iteration 1745, loss = 0.10074816\n",
      "Iteration 1746, loss = 0.10073750\n",
      "Iteration 1747, loss = 0.10072684\n",
      "Iteration 1748, loss = 0.10071619\n",
      "Iteration 1749, loss = 0.10070554\n",
      "Iteration 1750, loss = 0.10069490\n",
      "Iteration 1751, loss = 0.10068426\n",
      "Iteration 1752, loss = 0.10067362\n",
      "Iteration 1753, loss = 0.10066299\n",
      "Iteration 1754, loss = 0.10065237\n",
      "Iteration 1755, loss = 0.10064174\n",
      "Iteration 1756, loss = 0.10063113\n",
      "Iteration 1757, loss = 0.10062052\n",
      "Iteration 1758, loss = 0.10060991\n",
      "Iteration 1759, loss = 0.10059930\n",
      "Iteration 1760, loss = 0.10058870\n",
      "Iteration 1761, loss = 0.10057811\n",
      "Iteration 1762, loss = 0.10056752\n",
      "Iteration 1763, loss = 0.10055693\n",
      "Iteration 1764, loss = 0.10054635\n",
      "Iteration 1765, loss = 0.10053578\n",
      "Iteration 1766, loss = 0.10052520\n",
      "Iteration 1767, loss = 0.10051464\n",
      "Iteration 1768, loss = 0.10050407\n",
      "Iteration 1769, loss = 0.10049352\n",
      "Iteration 1770, loss = 0.10048296\n",
      "Iteration 1771, loss = 0.10047241\n",
      "Iteration 1772, loss = 0.10046187\n",
      "Iteration 1773, loss = 0.10045133\n",
      "Iteration 1774, loss = 0.10044079\n",
      "Iteration 1775, loss = 0.10043026\n",
      "Iteration 1776, loss = 0.10041973\n",
      "Iteration 1777, loss = 0.10040921\n",
      "Iteration 1778, loss = 0.10039869\n",
      "Iteration 1779, loss = 0.10038818\n",
      "Iteration 1780, loss = 0.10037767\n",
      "Iteration 1781, loss = 0.10036717\n",
      "Iteration 1782, loss = 0.10035667\n",
      "Iteration 1783, loss = 0.10034618\n",
      "Iteration 1784, loss = 0.10033569\n",
      "Iteration 1785, loss = 0.10032520\n",
      "Iteration 1786, loss = 0.10031472\n",
      "Iteration 1787, loss = 0.10030424\n",
      "Iteration 1788, loss = 0.10029377\n",
      "Iteration 1789, loss = 0.10028331\n",
      "Iteration 1790, loss = 0.10027285\n",
      "Iteration 1791, loss = 0.10026239\n",
      "Iteration 1792, loss = 0.10025194\n",
      "Iteration 1793, loss = 0.10024149\n",
      "Iteration 1794, loss = 0.10023104\n",
      "Iteration 1795, loss = 0.10022061\n",
      "Iteration 1796, loss = 0.10021017\n",
      "Iteration 1797, loss = 0.10019974\n",
      "Iteration 1798, loss = 0.10018932\n",
      "Iteration 1799, loss = 0.10017890\n",
      "Iteration 1800, loss = 0.10016848\n",
      "Iteration 1801, loss = 0.10015807\n",
      "Iteration 1802, loss = 0.10014767\n",
      "Iteration 1803, loss = 0.10013727\n",
      "Iteration 1804, loss = 0.10012687\n",
      "Iteration 1805, loss = 0.10011648\n",
      "Iteration 1806, loss = 0.10010609\n",
      "Iteration 1807, loss = 0.10009571\n",
      "Iteration 1808, loss = 0.10008533\n",
      "Iteration 1809, loss = 0.10007496\n",
      "Iteration 1810, loss = 0.10006459\n",
      "Iteration 1811, loss = 0.10005423\n",
      "Iteration 1812, loss = 0.10004387\n",
      "Iteration 1813, loss = 0.10003352\n",
      "Iteration 1814, loss = 0.10002317\n",
      "Iteration 1815, loss = 0.10001283\n",
      "Iteration 1816, loss = 0.10000249\n",
      "Iteration 1817, loss = 0.09999216\n",
      "Iteration 1818, loss = 0.09998183\n",
      "Iteration 1819, loss = 0.09997150\n",
      "Iteration 1820, loss = 0.09996118\n",
      "Iteration 1821, loss = 0.09995087\n",
      "Iteration 1822, loss = 0.09994056\n",
      "Iteration 1823, loss = 0.09993025\n",
      "Iteration 1824, loss = 0.09991995\n",
      "Iteration 1825, loss = 0.09990966\n",
      "Iteration 1826, loss = 0.09989937\n",
      "Iteration 1827, loss = 0.09988908\n",
      "Iteration 1828, loss = 0.09987880\n",
      "Iteration 1829, loss = 0.09986853\n",
      "Iteration 1830, loss = 0.09985826\n",
      "Iteration 1831, loss = 0.09984799\n",
      "Iteration 1832, loss = 0.09983773\n",
      "Iteration 1833, loss = 0.09982747\n",
      "Iteration 1834, loss = 0.09981722\n",
      "Iteration 1835, loss = 0.09980698\n",
      "Iteration 1836, loss = 0.09979673\n",
      "Iteration 1837, loss = 0.09978650\n",
      "Iteration 1838, loss = 0.09977627\n",
      "Iteration 1839, loss = 0.09976604\n",
      "Iteration 1840, loss = 0.09975582\n",
      "Iteration 1841, loss = 0.09974560\n",
      "Iteration 1842, loss = 0.09973539\n",
      "Iteration 1843, loss = 0.09972518\n",
      "Iteration 1844, loss = 0.09971498\n",
      "Iteration 1845, loss = 0.09970478\n",
      "Iteration 1846, loss = 0.09969459\n",
      "Iteration 1847, loss = 0.09968440\n",
      "Iteration 1848, loss = 0.09967422\n",
      "Iteration 1849, loss = 0.09966405\n",
      "Iteration 1850, loss = 0.09965387\n",
      "Iteration 1851, loss = 0.09964371\n",
      "Iteration 1852, loss = 0.09963354\n",
      "Iteration 1853, loss = 0.09962339\n",
      "Iteration 1854, loss = 0.09961324\n",
      "Iteration 1855, loss = 0.09960309\n",
      "Iteration 1856, loss = 0.09959295\n",
      "Iteration 1857, loss = 0.09958281\n",
      "Iteration 1858, loss = 0.09957268\n",
      "Iteration 1859, loss = 0.09956255\n",
      "Iteration 1860, loss = 0.09955243\n",
      "Iteration 1861, loss = 0.09954231\n",
      "Iteration 1862, loss = 0.09953220\n",
      "Iteration 1863, loss = 0.09952210\n",
      "Iteration 1864, loss = 0.09951199\n",
      "Iteration 1865, loss = 0.09950190\n",
      "Iteration 1866, loss = 0.09949181\n",
      "Iteration 1867, loss = 0.09948172\n",
      "Iteration 1868, loss = 0.09947164\n",
      "Iteration 1869, loss = 0.09946156\n",
      "Iteration 1870, loss = 0.09945149\n",
      "Iteration 1871, loss = 0.09944143\n",
      "Iteration 1872, loss = 0.09943136\n",
      "Iteration 1873, loss = 0.09942131\n",
      "Iteration 1874, loss = 0.09941126\n",
      "Iteration 1875, loss = 0.09940121\n",
      "Iteration 1876, loss = 0.09939117\n",
      "Iteration 1877, loss = 0.09938114\n",
      "Iteration 1878, loss = 0.09937111\n",
      "Iteration 1879, loss = 0.09936108\n",
      "Iteration 1880, loss = 0.09935106\n",
      "Iteration 1881, loss = 0.09934105\n",
      "Iteration 1882, loss = 0.09933104\n",
      "Iteration 1883, loss = 0.09932104\n",
      "Iteration 1884, loss = 0.09931104\n",
      "Iteration 1885, loss = 0.09930104\n",
      "Iteration 1886, loss = 0.09929105\n",
      "Iteration 1887, loss = 0.09928107\n",
      "Iteration 1888, loss = 0.09927109\n",
      "Iteration 1889, loss = 0.09926112\n",
      "Iteration 1890, loss = 0.09925115\n",
      "Iteration 1891, loss = 0.09924119\n",
      "Iteration 1892, loss = 0.09923123\n",
      "Iteration 1893, loss = 0.09922128\n",
      "Iteration 1894, loss = 0.09921133\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "mlp.score=0.71\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "M_scaled = scaler.transform(M)\n",
    "\n",
    "mlp.fit(X_scaled, y)\n",
    "y_pred_mlp = mlp.predict(M_scaled)\n",
    "\n",
    "print(f\"mlp.score={mlp.score(X_scaled, y):0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 19.05892661\n",
      "Iteration 2, loss = 19.03772919\n",
      "Iteration 3, loss = 19.01661337\n",
      "Iteration 4, loss = 18.99564702\n",
      "Iteration 5, loss = 18.97467771\n",
      "Iteration 6, loss = 18.95377333\n",
      "Iteration 7, loss = 18.93302266\n",
      "Iteration 8, loss = 18.91226755\n",
      "Iteration 9, loss = 18.89150790\n",
      "Iteration 10, loss = 18.87074362\n",
      "Iteration 11, loss = 18.84997460\n",
      "Iteration 12, loss = 18.82920072\n",
      "Iteration 13, loss = 18.80842187\n",
      "Iteration 14, loss = 18.78763790\n",
      "Iteration 15, loss = 18.76684869\n",
      "Iteration 16, loss = 18.74605409\n",
      "Iteration 17, loss = 18.72525396\n",
      "Iteration 18, loss = 18.70444814\n",
      "Iteration 19, loss = 18.68363646\n",
      "Iteration 20, loss = 18.66281877\n",
      "Iteration 21, loss = 18.64199490\n",
      "Iteration 22, loss = 18.62116467\n",
      "Iteration 23, loss = 18.60032789\n",
      "Iteration 24, loss = 18.57948440\n",
      "Iteration 25, loss = 18.55863401\n",
      "Iteration 26, loss = 18.53777652\n",
      "Iteration 27, loss = 18.51691174\n",
      "Iteration 28, loss = 18.49603949\n",
      "Iteration 29, loss = 18.47515956\n",
      "Iteration 30, loss = 18.45427175\n",
      "Iteration 31, loss = 18.43337587\n",
      "Iteration 32, loss = 18.41247172\n",
      "Iteration 33, loss = 18.39155909\n",
      "Iteration 34, loss = 18.37063777\n",
      "Iteration 35, loss = 18.34970757\n",
      "Iteration 36, loss = 18.32876828\n",
      "Iteration 37, loss = 18.30781969\n",
      "Iteration 38, loss = 18.28686160\n",
      "Iteration 39, loss = 18.26589379\n",
      "Iteration 40, loss = 18.24491607\n",
      "Iteration 41, loss = 18.22392823\n",
      "Iteration 42, loss = 18.20293007\n",
      "Iteration 43, loss = 18.18192137\n",
      "Iteration 44, loss = 18.16090193\n",
      "Iteration 45, loss = 18.13987156\n",
      "Iteration 46, loss = 18.11878404\n",
      "Iteration 47, loss = 18.09767918\n",
      "Iteration 48, loss = 18.07655878\n",
      "Iteration 49, loss = 18.05542301\n",
      "Iteration 50, loss = 18.03427199\n",
      "Iteration 51, loss = 18.01310583\n",
      "Iteration 52, loss = 17.99192457\n",
      "Iteration 53, loss = 17.97072826\n",
      "Iteration 54, loss = 17.94951691\n",
      "Iteration 55, loss = 17.92829050\n",
      "Iteration 56, loss = 17.90704900\n",
      "Iteration 57, loss = 17.88579238\n",
      "Iteration 58, loss = 17.86452057\n",
      "Iteration 59, loss = 17.84323351\n",
      "Iteration 60, loss = 17.82193111\n",
      "Iteration 61, loss = 17.80061327\n",
      "Iteration 62, loss = 17.77927991\n",
      "Iteration 63, loss = 17.75793090\n",
      "Iteration 64, loss = 17.73654508\n",
      "Iteration 65, loss = 17.71511295\n",
      "Iteration 66, loss = 17.69366073\n",
      "Iteration 67, loss = 17.67218866\n",
      "Iteration 68, loss = 17.65069695\n",
      "Iteration 69, loss = 17.62918576\n",
      "Iteration 70, loss = 17.60765524\n",
      "Iteration 71, loss = 17.58610547\n",
      "Iteration 72, loss = 17.56453654\n",
      "Iteration 73, loss = 17.54294849\n",
      "Iteration 74, loss = 17.52134136\n",
      "Iteration 75, loss = 17.49971515\n",
      "Iteration 76, loss = 17.47805745\n",
      "Iteration 77, loss = 17.45634138\n",
      "Iteration 78, loss = 17.43460200\n",
      "Iteration 79, loss = 17.41283967\n",
      "Iteration 80, loss = 17.39105468\n",
      "Iteration 81, loss = 17.36924728\n",
      "Iteration 82, loss = 17.34741767\n",
      "Iteration 83, loss = 17.32556602\n",
      "Iteration 84, loss = 17.30369248\n",
      "Iteration 85, loss = 17.28177893\n",
      "Iteration 86, loss = 17.25979706\n",
      "Iteration 87, loss = 17.23778825\n",
      "Iteration 88, loss = 17.21575306\n",
      "Iteration 89, loss = 17.19369194\n",
      "Iteration 90, loss = 17.17160531\n",
      "Iteration 91, loss = 17.14949353\n",
      "Iteration 92, loss = 17.12735690\n",
      "Iteration 93, loss = 17.10519568\n",
      "Iteration 94, loss = 17.08301008\n",
      "Iteration 95, loss = 17.06080030\n",
      "Iteration 96, loss = 17.03856650\n",
      "Iteration 97, loss = 17.01630878\n",
      "Iteration 98, loss = 16.99402727\n",
      "Iteration 99, loss = 16.97172204\n",
      "Iteration 100, loss = 16.94936938\n",
      "Iteration 101, loss = 16.92693966\n",
      "Iteration 102, loss = 16.90448004\n",
      "Iteration 103, loss = 16.88199114\n",
      "Iteration 104, loss = 16.85947350\n",
      "Iteration 105, loss = 16.83692759\n",
      "Iteration 106, loss = 16.81435383\n",
      "Iteration 107, loss = 16.79175258\n",
      "Iteration 108, loss = 16.76912415\n",
      "Iteration 109, loss = 16.74646882\n",
      "Iteration 110, loss = 16.72378682\n",
      "Iteration 111, loss = 16.70107836\n",
      "Iteration 112, loss = 16.67834360\n",
      "Iteration 113, loss = 16.65558270\n",
      "Iteration 114, loss = 16.63279578\n",
      "Iteration 115, loss = 16.60998294\n",
      "Iteration 116, loss = 16.58714428\n",
      "Iteration 117, loss = 16.56427986\n",
      "Iteration 118, loss = 16.54138974\n",
      "Iteration 119, loss = 16.51847397\n",
      "Iteration 120, loss = 16.49553258\n",
      "Iteration 121, loss = 16.47256559\n",
      "Iteration 122, loss = 16.44957303\n",
      "Iteration 123, loss = 16.42655490\n",
      "Iteration 124, loss = 16.40351121\n",
      "Iteration 125, loss = 16.38044196\n",
      "Iteration 126, loss = 16.35734714\n",
      "Iteration 127, loss = 16.33422674\n",
      "Iteration 128, loss = 16.31108076\n",
      "Iteration 129, loss = 16.28790918\n",
      "Iteration 130, loss = 16.26471199\n",
      "Iteration 131, loss = 16.24148916\n",
      "Iteration 132, loss = 16.21824068\n",
      "Iteration 133, loss = 16.19496653\n",
      "Iteration 134, loss = 16.17166669\n",
      "Iteration 135, loss = 16.14834115\n",
      "Iteration 136, loss = 16.12498987\n",
      "Iteration 137, loss = 16.10161284\n",
      "Iteration 138, loss = 16.07821005\n",
      "Iteration 139, loss = 16.05478148\n",
      "Iteration 140, loss = 16.03132710\n",
      "Iteration 141, loss = 16.00784691\n",
      "Iteration 142, loss = 15.98431406\n",
      "Iteration 143, loss = 15.96069769\n",
      "Iteration 144, loss = 15.93704851\n",
      "Iteration 145, loss = 15.91335956\n",
      "Iteration 146, loss = 15.88957583\n",
      "Iteration 147, loss = 15.86575506\n",
      "Iteration 148, loss = 15.84189830\n",
      "Iteration 149, loss = 15.81800648\n",
      "Iteration 150, loss = 15.79408045\n",
      "Iteration 151, loss = 15.77012096\n",
      "Iteration 152, loss = 15.74612868\n",
      "Iteration 153, loss = 15.72210423\n",
      "Iteration 154, loss = 15.69804816\n",
      "Iteration 155, loss = 15.67396094\n",
      "Iteration 156, loss = 15.64984303\n",
      "Iteration 157, loss = 15.62569483\n",
      "Iteration 158, loss = 15.60151670\n",
      "Iteration 159, loss = 15.57730896\n",
      "Iteration 160, loss = 15.55307190\n",
      "Iteration 161, loss = 15.52880581\n",
      "Iteration 162, loss = 15.50451092\n",
      "Iteration 163, loss = 15.48018745\n",
      "Iteration 164, loss = 15.45583562\n",
      "Iteration 165, loss = 15.43145560\n",
      "Iteration 166, loss = 15.40704757\n",
      "Iteration 167, loss = 15.38261168\n",
      "Iteration 168, loss = 15.35814810\n",
      "Iteration 169, loss = 15.33365695\n",
      "Iteration 170, loss = 15.30913836\n",
      "Iteration 171, loss = 15.28459245\n",
      "Iteration 172, loss = 15.26001935\n",
      "Iteration 173, loss = 15.23541915\n",
      "Iteration 174, loss = 15.21079197\n",
      "Iteration 175, loss = 15.18613789\n",
      "Iteration 176, loss = 15.16145703\n",
      "Iteration 177, loss = 15.13674947\n",
      "Iteration 178, loss = 15.11201531\n",
      "Iteration 179, loss = 15.08725462\n",
      "Iteration 180, loss = 15.06246751\n",
      "Iteration 181, loss = 15.03765405\n",
      "Iteration 182, loss = 15.01274430\n",
      "Iteration 183, loss = 14.98778626\n",
      "Iteration 184, loss = 14.96279457\n",
      "Iteration 185, loss = 14.93777004\n",
      "Iteration 186, loss = 14.91271343\n",
      "Iteration 187, loss = 14.88756670\n",
      "Iteration 188, loss = 14.86234585\n",
      "Iteration 189, loss = 14.83708615\n",
      "Iteration 190, loss = 14.81178898\n",
      "Iteration 191, loss = 14.78645555\n",
      "Iteration 192, loss = 14.76108698\n",
      "Iteration 193, loss = 14.73568425\n",
      "Iteration 194, loss = 14.71024829\n",
      "Iteration 195, loss = 14.68477991\n",
      "Iteration 196, loss = 14.65927986\n",
      "Iteration 197, loss = 14.63374883\n",
      "Iteration 198, loss = 14.60818744\n",
      "Iteration 199, loss = 14.58259625\n",
      "Iteration 200, loss = 14.55697580\n",
      "Iteration 201, loss = 14.53132655\n",
      "Iteration 202, loss = 14.50564895\n",
      "Iteration 203, loss = 14.47994341\n",
      "Iteration 204, loss = 14.45421030\n",
      "Iteration 205, loss = 14.42844998\n",
      "Iteration 206, loss = 14.40266276\n",
      "Iteration 207, loss = 14.37684897\n",
      "Iteration 208, loss = 14.35100888\n",
      "Iteration 209, loss = 14.32514277\n",
      "Iteration 210, loss = 14.29925089\n",
      "Iteration 211, loss = 14.27333348\n",
      "Iteration 212, loss = 14.24739078\n",
      "Iteration 213, loss = 14.22142301\n",
      "Iteration 214, loss = 14.19543038\n",
      "Iteration 215, loss = 14.16941310\n",
      "Iteration 216, loss = 14.14337136\n",
      "Iteration 217, loss = 14.11730536\n",
      "Iteration 218, loss = 14.09121528\n",
      "Iteration 219, loss = 14.06510131\n",
      "Iteration 220, loss = 14.03896362\n",
      "Iteration 221, loss = 14.01280239\n",
      "Iteration 222, loss = 13.98661780\n",
      "Iteration 223, loss = 13.96041001\n",
      "Iteration 224, loss = 13.93417920\n",
      "Iteration 225, loss = 13.90792553\n",
      "Iteration 226, loss = 13.88164916\n",
      "Iteration 227, loss = 13.85535027\n",
      "Iteration 228, loss = 13.82902902\n",
      "Iteration 229, loss = 13.80268556\n",
      "Iteration 230, loss = 13.77632007\n",
      "Iteration 231, loss = 13.74993529\n",
      "Iteration 232, loss = 13.72353059\n",
      "Iteration 233, loss = 13.69710446\n",
      "Iteration 234, loss = 13.67065704\n",
      "Iteration 235, loss = 13.64418843\n",
      "Iteration 236, loss = 13.61769877\n",
      "Iteration 237, loss = 13.59118819\n",
      "Iteration 238, loss = 13.56465683\n",
      "Iteration 239, loss = 13.53810483\n",
      "Iteration 240, loss = 13.51153234\n",
      "Iteration 241, loss = 13.48493951\n",
      "Iteration 242, loss = 13.45832649\n",
      "Iteration 243, loss = 13.43169343\n",
      "Iteration 244, loss = 13.40504050\n",
      "Iteration 245, loss = 13.37836786\n",
      "Iteration 246, loss = 13.35167568\n",
      "Iteration 247, loss = 13.32496411\n",
      "Iteration 248, loss = 13.29823335\n",
      "Iteration 249, loss = 13.27148354\n",
      "Iteration 250, loss = 13.24471488\n",
      "Iteration 251, loss = 13.21792753\n",
      "Iteration 252, loss = 13.19112167\n",
      "Iteration 253, loss = 13.16429750\n",
      "Iteration 254, loss = 13.13745517\n",
      "Iteration 255, loss = 13.11059489\n",
      "Iteration 256, loss = 13.08371684\n",
      "Iteration 257, loss = 13.05682120\n",
      "Iteration 258, loss = 13.02990816\n",
      "Iteration 259, loss = 13.00297791\n",
      "Iteration 260, loss = 12.97603064\n",
      "Iteration 261, loss = 12.94906655\n",
      "Iteration 262, loss = 12.92208583\n",
      "Iteration 263, loss = 12.89508867\n",
      "Iteration 264, loss = 12.86807528\n",
      "Iteration 265, loss = 12.84104584\n",
      "Iteration 266, loss = 12.81400055\n",
      "Iteration 267, loss = 12.78693963\n",
      "Iteration 268, loss = 12.75986325\n",
      "Iteration 269, loss = 12.73277267\n",
      "Iteration 270, loss = 12.70566733\n",
      "Iteration 271, loss = 12.67854716\n",
      "Iteration 272, loss = 12.65141234\n",
      "Iteration 273, loss = 12.62426307\n",
      "Iteration 274, loss = 12.59709953\n",
      "Iteration 275, loss = 12.56992193\n",
      "Iteration 276, loss = 12.54273046\n",
      "Iteration 277, loss = 12.51552534\n",
      "Iteration 278, loss = 12.48830675\n",
      "Iteration 279, loss = 12.46107491\n",
      "Iteration 280, loss = 12.43383002\n",
      "Iteration 281, loss = 12.40657253\n",
      "Iteration 282, loss = 12.37930286\n",
      "Iteration 283, loss = 12.35202076\n",
      "Iteration 284, loss = 12.32472643\n",
      "Iteration 285, loss = 12.29742008\n",
      "Iteration 286, loss = 12.27010193\n",
      "Iteration 287, loss = 12.24277217\n",
      "Iteration 288, loss = 12.21543104\n",
      "Iteration 289, loss = 12.18807880\n",
      "Iteration 290, loss = 12.16071599\n",
      "Iteration 291, loss = 12.13334243\n",
      "Iteration 292, loss = 12.10595851\n",
      "Iteration 293, loss = 12.07856445\n",
      "Iteration 294, loss = 12.05116027\n",
      "Iteration 295, loss = 12.02374617\n",
      "Iteration 296, loss = 11.99632239\n",
      "Iteration 297, loss = 11.96888913\n",
      "Iteration 298, loss = 11.94144662\n",
      "Iteration 299, loss = 11.91399509\n",
      "Iteration 300, loss = 11.88653476\n",
      "Iteration 301, loss = 11.85906586\n",
      "Iteration 302, loss = 11.83158863\n",
      "Iteration 303, loss = 11.80410328\n",
      "Iteration 304, loss = 11.77661006\n",
      "Iteration 305, loss = 11.74910919\n",
      "Iteration 306, loss = 11.72160092\n",
      "Iteration 307, loss = 11.69408547\n",
      "Iteration 308, loss = 11.66656308\n",
      "Iteration 309, loss = 11.63903399\n",
      "Iteration 310, loss = 11.61149844\n",
      "Iteration 311, loss = 11.58395666\n",
      "Iteration 312, loss = 11.55640890\n",
      "Iteration 313, loss = 11.52885539\n",
      "Iteration 314, loss = 11.50129637\n",
      "Iteration 315, loss = 11.47373209\n",
      "Iteration 316, loss = 11.44616279\n",
      "Iteration 317, loss = 11.41858870\n",
      "Iteration 318, loss = 11.39101007\n",
      "Iteration 319, loss = 11.36342715\n",
      "Iteration 320, loss = 11.33584017\n",
      "Iteration 321, loss = 11.30824937\n",
      "Iteration 322, loss = 11.28065501\n",
      "Iteration 323, loss = 11.25305733\n",
      "Iteration 324, loss = 11.22545657\n",
      "Iteration 325, loss = 11.19785297\n",
      "Iteration 326, loss = 11.17024678\n",
      "Iteration 327, loss = 11.14263825\n",
      "Iteration 328, loss = 11.11502761\n",
      "Iteration 329, loss = 11.08741512\n",
      "Iteration 330, loss = 11.05980102\n",
      "Iteration 331, loss = 11.03218556\n",
      "Iteration 332, loss = 11.00456898\n",
      "Iteration 333, loss = 10.97695153\n",
      "Iteration 334, loss = 10.94933345\n",
      "Iteration 335, loss = 10.92171500\n",
      "Iteration 336, loss = 10.89409641\n",
      "Iteration 337, loss = 10.86647793\n",
      "Iteration 338, loss = 10.83885982\n",
      "Iteration 339, loss = 10.81124231\n",
      "Iteration 340, loss = 10.78362566\n",
      "Iteration 341, loss = 10.75601011\n",
      "Iteration 342, loss = 10.72839590\n",
      "Iteration 343, loss = 10.70078329\n",
      "Iteration 344, loss = 10.67317252\n",
      "Iteration 345, loss = 10.64556384\n",
      "Iteration 346, loss = 10.61795750\n",
      "Iteration 347, loss = 10.59035374\n",
      "Iteration 348, loss = 10.56275281\n",
      "Iteration 349, loss = 10.53515495\n",
      "Iteration 350, loss = 10.50756042\n",
      "Iteration 351, loss = 10.47996946\n",
      "Iteration 352, loss = 10.45238232\n",
      "Iteration 353, loss = 10.42479925\n",
      "Iteration 354, loss = 10.39722048\n",
      "Iteration 355, loss = 10.36964627\n",
      "Iteration 356, loss = 10.34207687\n",
      "Iteration 357, loss = 10.31451252\n",
      "Iteration 358, loss = 10.28695347\n",
      "Iteration 359, loss = 10.25939996\n",
      "Iteration 360, loss = 10.23185224\n",
      "Iteration 361, loss = 10.20431056\n",
      "Iteration 362, loss = 10.17677517\n",
      "Iteration 363, loss = 10.14924630\n",
      "Iteration 364, loss = 10.12172421\n",
      "Iteration 365, loss = 10.09420914\n",
      "Iteration 366, loss = 10.06670134\n",
      "Iteration 367, loss = 10.03920105\n",
      "Iteration 368, loss = 10.01170828\n",
      "Iteration 369, loss = 9.98422309\n",
      "Iteration 370, loss = 9.95674604\n",
      "Iteration 371, loss = 9.92927703\n",
      "Iteration 372, loss = 9.90181624\n",
      "Iteration 373, loss = 9.87436419\n",
      "Iteration 374, loss = 9.84692112\n",
      "Iteration 375, loss = 9.81948728\n",
      "Iteration 376, loss = 9.79206291\n",
      "Iteration 377, loss = 9.76464825\n",
      "Iteration 378, loss = 9.73724353\n",
      "Iteration 379, loss = 9.70984901\n",
      "Iteration 380, loss = 9.68246407\n",
      "Iteration 381, loss = 9.65508961\n",
      "Iteration 382, loss = 9.62772588\n",
      "Iteration 383, loss = 9.60037315\n",
      "Iteration 384, loss = 9.57303165\n",
      "Iteration 385, loss = 9.54570161\n",
      "Iteration 386, loss = 9.51838328\n",
      "Iteration 387, loss = 9.49107689\n",
      "Iteration 388, loss = 9.46378268\n",
      "Iteration 389, loss = 9.43650089\n",
      "Iteration 390, loss = 9.40923175\n",
      "Iteration 391, loss = 9.38197551\n",
      "Iteration 392, loss = 9.35473174\n",
      "Iteration 393, loss = 9.32750016\n",
      "Iteration 394, loss = 9.30028191\n",
      "Iteration 395, loss = 9.27307722\n",
      "Iteration 396, loss = 9.24588635\n",
      "Iteration 397, loss = 9.21870951\n",
      "Iteration 398, loss = 9.19154695\n",
      "Iteration 399, loss = 9.16439890\n",
      "Iteration 400, loss = 9.13726559\n",
      "Iteration 401, loss = 9.11014725\n",
      "Iteration 402, loss = 9.08304411\n",
      "Iteration 403, loss = 9.05595641\n",
      "Iteration 404, loss = 9.02888437\n",
      "Iteration 405, loss = 9.00182822\n",
      "Iteration 406, loss = 8.97478818\n",
      "Iteration 407, loss = 8.94776448\n",
      "Iteration 408, loss = 8.92075736\n",
      "Iteration 409, loss = 8.89376702\n",
      "Iteration 410, loss = 8.86679369\n",
      "Iteration 411, loss = 8.83983761\n",
      "Iteration 412, loss = 8.81289898\n",
      "Iteration 413, loss = 8.78597804\n",
      "Iteration 414, loss = 8.75907500\n",
      "Iteration 415, loss = 8.73219008\n",
      "Iteration 416, loss = 8.70532350\n",
      "Iteration 417, loss = 8.67847549\n",
      "Iteration 418, loss = 8.65164625\n",
      "Iteration 419, loss = 8.62483601\n",
      "Iteration 420, loss = 8.59804499\n",
      "Iteration 421, loss = 8.57127339\n",
      "Iteration 422, loss = 8.54452144\n",
      "Iteration 423, loss = 8.51778935\n",
      "Iteration 424, loss = 8.49107734\n",
      "Iteration 425, loss = 8.46438561\n",
      "Iteration 426, loss = 8.43771439\n",
      "Iteration 427, loss = 8.41106388\n",
      "Iteration 428, loss = 8.38443429\n",
      "Iteration 429, loss = 8.35782222\n",
      "Iteration 430, loss = 8.33123076\n",
      "Iteration 431, loss = 8.30466032\n",
      "Iteration 432, loss = 8.27811114\n",
      "Iteration 433, loss = 8.25158344\n",
      "Iteration 434, loss = 8.22507745\n",
      "Iteration 435, loss = 8.19859339\n",
      "Iteration 436, loss = 8.17213148\n",
      "Iteration 437, loss = 8.14569194\n",
      "Iteration 438, loss = 8.11927497\n",
      "Iteration 439, loss = 8.09288080\n",
      "Iteration 440, loss = 8.06650963\n",
      "Iteration 441, loss = 8.04016168\n",
      "Iteration 442, loss = 8.01383714\n",
      "Iteration 443, loss = 7.98753622\n",
      "Iteration 444, loss = 7.96125914\n",
      "Iteration 445, loss = 7.93500608\n",
      "Iteration 446, loss = 7.90877726\n",
      "Iteration 447, loss = 7.88257286\n",
      "Iteration 448, loss = 7.85639310\n",
      "Iteration 449, loss = 7.83023816\n",
      "Iteration 450, loss = 7.80410824\n",
      "Iteration 451, loss = 7.77800354\n",
      "Iteration 452, loss = 7.75192426\n",
      "Iteration 453, loss = 7.72587058\n",
      "Iteration 454, loss = 7.69984269\n",
      "Iteration 455, loss = 7.67384079\n",
      "Iteration 456, loss = 7.64786507\n",
      "Iteration 457, loss = 7.62191571\n",
      "Iteration 458, loss = 7.59599290\n",
      "Iteration 459, loss = 7.57009683\n",
      "Iteration 460, loss = 7.54422769\n",
      "Iteration 461, loss = 7.51838566\n",
      "Iteration 462, loss = 7.49257092\n",
      "Iteration 463, loss = 7.46678366\n",
      "Iteration 464, loss = 7.44102406\n",
      "Iteration 465, loss = 7.41529231\n",
      "Iteration 466, loss = 7.38958857\n",
      "Iteration 467, loss = 7.36391303\n",
      "Iteration 468, loss = 7.33826588\n",
      "Iteration 469, loss = 7.31264728\n",
      "Iteration 470, loss = 7.28705742\n",
      "Iteration 471, loss = 7.26149648\n",
      "Iteration 472, loss = 7.23596462\n",
      "Iteration 473, loss = 7.21046202\n",
      "Iteration 474, loss = 7.18498886\n",
      "Iteration 475, loss = 7.15954530\n",
      "Iteration 476, loss = 7.13413153\n",
      "Iteration 477, loss = 7.10874772\n",
      "Iteration 478, loss = 7.08339402\n",
      "Iteration 479, loss = 7.05807062\n",
      "Iteration 480, loss = 7.03277769\n",
      "Iteration 481, loss = 7.00751539\n",
      "Iteration 482, loss = 6.98228388\n",
      "Iteration 483, loss = 6.95708334\n",
      "Iteration 484, loss = 6.93191393\n",
      "Iteration 485, loss = 6.90677582\n",
      "Iteration 486, loss = 6.88166918\n",
      "Iteration 487, loss = 6.85659415\n",
      "Iteration 488, loss = 6.83155092\n",
      "Iteration 489, loss = 6.80653964\n",
      "Iteration 490, loss = 6.78156047\n",
      "Iteration 491, loss = 6.75661357\n",
      "Iteration 492, loss = 6.73169911\n",
      "Iteration 493, loss = 6.70681723\n",
      "Iteration 494, loss = 6.68196811\n",
      "Iteration 495, loss = 6.65715190\n",
      "Iteration 496, loss = 6.63236875\n",
      "Iteration 497, loss = 6.60761883\n",
      "Iteration 498, loss = 6.58290228\n",
      "Iteration 499, loss = 6.55821927\n",
      "Iteration 500, loss = 6.53356995\n",
      "Iteration 501, loss = 6.50895446\n",
      "Iteration 502, loss = 6.48437297\n",
      "Iteration 503, loss = 6.45982563\n",
      "Iteration 504, loss = 6.43531258\n",
      "Iteration 505, loss = 6.41083398\n",
      "Iteration 506, loss = 6.38638998\n",
      "Iteration 507, loss = 6.36198073\n",
      "Iteration 508, loss = 6.33760638\n",
      "Iteration 509, loss = 6.31326707\n",
      "Iteration 510, loss = 6.28896296\n",
      "Iteration 511, loss = 6.26469418\n",
      "Iteration 512, loss = 6.24046089\n",
      "Iteration 513, loss = 6.21626324\n",
      "Iteration 514, loss = 6.19210136\n",
      "Iteration 515, loss = 6.16797540\n",
      "Iteration 516, loss = 6.14388552\n",
      "Iteration 517, loss = 6.11983184\n",
      "Iteration 518, loss = 6.09581451\n",
      "Iteration 519, loss = 6.07183368\n",
      "Iteration 520, loss = 6.04788948\n",
      "Iteration 521, loss = 6.02398206\n",
      "Iteration 522, loss = 6.00011156\n",
      "Iteration 523, loss = 5.97627812\n",
      "Iteration 524, loss = 5.95248187\n",
      "Iteration 525, loss = 5.92872296\n",
      "Iteration 526, loss = 5.90500153\n",
      "Iteration 527, loss = 5.88131771\n",
      "Iteration 528, loss = 5.85767164\n",
      "Iteration 529, loss = 5.83406346\n",
      "Iteration 530, loss = 5.81049330\n",
      "Iteration 531, loss = 5.78696131\n",
      "Iteration 532, loss = 5.76346760\n",
      "Iteration 533, loss = 5.74001234\n",
      "Iteration 534, loss = 5.71659563\n",
      "Iteration 535, loss = 5.69321763\n",
      "Iteration 536, loss = 5.66987846\n",
      "Iteration 537, loss = 5.64657826\n",
      "Iteration 538, loss = 5.62331715\n",
      "Iteration 539, loss = 5.60009529\n",
      "Iteration 540, loss = 5.57691278\n",
      "Iteration 541, loss = 5.55376978\n",
      "Iteration 542, loss = 5.53066640\n",
      "Iteration 543, loss = 5.50760279\n",
      "Iteration 544, loss = 5.48457907\n",
      "Iteration 545, loss = 5.46159537\n",
      "Iteration 546, loss = 5.43865182\n",
      "Iteration 547, loss = 5.41574855\n",
      "Iteration 548, loss = 5.39288569\n",
      "Iteration 549, loss = 5.37006338\n",
      "Iteration 550, loss = 5.34728174\n",
      "Iteration 551, loss = 5.32454089\n",
      "Iteration 552, loss = 5.30184098\n",
      "Iteration 553, loss = 5.27918212\n",
      "Iteration 554, loss = 5.25656444\n",
      "Iteration 555, loss = 5.23398807\n",
      "Iteration 556, loss = 5.21145315\n",
      "Iteration 557, loss = 5.18895979\n",
      "Iteration 558, loss = 5.16650812\n",
      "Iteration 559, loss = 5.14409828\n",
      "Iteration 560, loss = 5.12173039\n",
      "Iteration 561, loss = 5.09940456\n",
      "Iteration 562, loss = 5.07712094\n",
      "Iteration 563, loss = 5.05487965\n",
      "Iteration 564, loss = 5.03268081\n",
      "Iteration 565, loss = 5.01052455\n",
      "Iteration 566, loss = 4.98841100\n",
      "Iteration 567, loss = 4.96634028\n",
      "Iteration 568, loss = 4.94431251\n",
      "Iteration 569, loss = 4.92232783\n",
      "Iteration 570, loss = 4.90038635\n",
      "Iteration 571, loss = 4.87848821\n",
      "Iteration 572, loss = 4.85663353\n",
      "Iteration 573, loss = 4.83482243\n",
      "Iteration 574, loss = 4.81305504\n",
      "Iteration 575, loss = 4.79133148\n",
      "Iteration 576, loss = 4.76965189\n",
      "Iteration 577, loss = 4.74801638\n",
      "Iteration 578, loss = 4.72642508\n",
      "Iteration 579, loss = 4.70487811\n",
      "Iteration 580, loss = 4.68337561\n",
      "Iteration 581, loss = 4.66191769\n",
      "Iteration 582, loss = 4.64050448\n",
      "Iteration 583, loss = 4.61913610\n",
      "Iteration 584, loss = 4.59781268\n",
      "Iteration 585, loss = 4.57653435\n",
      "Iteration 586, loss = 4.55530123\n",
      "Iteration 587, loss = 4.53411345\n",
      "Iteration 588, loss = 4.51297112\n",
      "Iteration 589, loss = 4.49187438\n",
      "Iteration 590, loss = 4.47082335\n",
      "Iteration 591, loss = 4.44981816\n",
      "Iteration 592, loss = 4.42885893\n",
      "Iteration 593, loss = 4.40794578\n",
      "Iteration 594, loss = 4.38707885\n",
      "Iteration 595, loss = 4.36625825\n",
      "Iteration 596, loss = 4.34548412\n",
      "Iteration 597, loss = 4.32475657\n",
      "Iteration 598, loss = 4.30407574\n",
      "Iteration 599, loss = 4.28344175\n",
      "Iteration 600, loss = 4.26285472\n",
      "Iteration 601, loss = 4.24231479\n",
      "Iteration 602, loss = 4.22182207\n",
      "Iteration 603, loss = 4.20137670\n",
      "Iteration 604, loss = 4.18097879\n",
      "Iteration 605, loss = 4.16062848\n",
      "Iteration 606, loss = 4.14032590\n",
      "Iteration 607, loss = 4.12007116\n",
      "Iteration 608, loss = 4.09986439\n",
      "Iteration 609, loss = 4.07970573\n",
      "Iteration 610, loss = 4.05959529\n",
      "Iteration 611, loss = 4.03953321\n",
      "Iteration 612, loss = 4.01951961\n",
      "Iteration 613, loss = 3.99955461\n",
      "Iteration 614, loss = 3.97963835\n",
      "Iteration 615, loss = 3.95977095\n",
      "Iteration 616, loss = 3.93995254\n",
      "Iteration 617, loss = 3.92018324\n",
      "Iteration 618, loss = 3.90046318\n",
      "Iteration 619, loss = 3.88079249\n",
      "Iteration 620, loss = 3.86117130\n",
      "Iteration 621, loss = 3.84159972\n",
      "Iteration 622, loss = 3.82207790\n",
      "Iteration 623, loss = 3.80260596\n",
      "Iteration 624, loss = 3.78318401\n",
      "Iteration 625, loss = 3.76381220\n",
      "Iteration 626, loss = 3.74449065\n",
      "Iteration 627, loss = 3.72521948\n",
      "Iteration 628, loss = 3.70599882\n",
      "Iteration 629, loss = 3.68682880\n",
      "Iteration 630, loss = 3.66770954\n",
      "Iteration 631, loss = 3.64864118\n",
      "Iteration 632, loss = 3.62962384\n",
      "Iteration 633, loss = 3.61065764\n",
      "Iteration 634, loss = 3.59174271\n",
      "Iteration 635, loss = 3.57287919\n",
      "Iteration 636, loss = 3.55406719\n",
      "Iteration 637, loss = 3.53530684\n",
      "Iteration 638, loss = 3.51659827\n",
      "Iteration 639, loss = 3.49794160\n",
      "Iteration 640, loss = 3.47933697\n",
      "Iteration 641, loss = 3.46078448\n",
      "Iteration 642, loss = 3.44228428\n",
      "Iteration 643, loss = 3.42383649\n",
      "Iteration 644, loss = 3.40544123\n",
      "Iteration 645, loss = 3.38709863\n",
      "Iteration 646, loss = 3.36880881\n",
      "Iteration 647, loss = 3.35057189\n",
      "Iteration 648, loss = 3.33238800\n",
      "Iteration 649, loss = 3.31425727\n",
      "Iteration 650, loss = 3.29617982\n",
      "Iteration 651, loss = 3.27815577\n",
      "Iteration 652, loss = 3.26018524\n",
      "Iteration 653, loss = 3.24226836\n",
      "Iteration 654, loss = 3.22440525\n",
      "Iteration 655, loss = 3.20659603\n",
      "Iteration 656, loss = 3.18884082\n",
      "Iteration 657, loss = 3.17113975\n",
      "Iteration 658, loss = 3.15349293\n",
      "Iteration 659, loss = 3.13590049\n",
      "Iteration 660, loss = 3.11836254\n",
      "Iteration 661, loss = 3.10087920\n",
      "Iteration 662, loss = 3.08345060\n",
      "Iteration 663, loss = 3.06607685\n",
      "Iteration 664, loss = 3.04875807\n",
      "Iteration 665, loss = 3.03149437\n",
      "Iteration 666, loss = 3.01428588\n",
      "Iteration 667, loss = 2.99713270\n",
      "Iteration 668, loss = 2.98003496\n",
      "Iteration 669, loss = 2.96299277\n",
      "Iteration 670, loss = 2.94600624\n",
      "Iteration 671, loss = 2.92907549\n",
      "Iteration 672, loss = 2.91220063\n",
      "Iteration 673, loss = 2.89538177\n",
      "Iteration 674, loss = 2.87861902\n",
      "Iteration 675, loss = 2.86191249\n",
      "Iteration 676, loss = 2.84526230\n",
      "Iteration 677, loss = 2.82866855\n",
      "Iteration 678, loss = 2.81213135\n",
      "Iteration 679, loss = 2.79565080\n",
      "Iteration 680, loss = 2.77922702\n",
      "Iteration 681, loss = 2.76286011\n",
      "Iteration 682, loss = 2.74655017\n",
      "Iteration 683, loss = 2.73029730\n",
      "Iteration 684, loss = 2.71410162\n",
      "Iteration 685, loss = 2.69796322\n",
      "Iteration 686, loss = 2.68188220\n",
      "Iteration 687, loss = 2.66585867\n",
      "Iteration 688, loss = 2.64989271\n",
      "Iteration 689, loss = 2.63398444\n",
      "Iteration 690, loss = 2.61813393\n",
      "Iteration 691, loss = 2.60234130\n",
      "Iteration 692, loss = 2.58660664\n",
      "Iteration 693, loss = 2.57093003\n",
      "Iteration 694, loss = 2.55531157\n",
      "Iteration 695, loss = 2.53975135\n",
      "Iteration 696, loss = 2.52424947\n",
      "Iteration 697, loss = 2.50880600\n",
      "Iteration 698, loss = 2.49342104\n",
      "Iteration 699, loss = 2.47809467\n",
      "Iteration 700, loss = 2.46282697\n",
      "Iteration 701, loss = 2.44761804\n",
      "Iteration 702, loss = 2.43246795\n",
      "Iteration 703, loss = 2.41737678\n",
      "Iteration 704, loss = 2.40234461\n",
      "Iteration 705, loss = 2.38737152\n",
      "Iteration 706, loss = 2.37245759\n",
      "Iteration 707, loss = 2.35760289\n",
      "Iteration 708, loss = 2.34280749\n",
      "Iteration 709, loss = 2.32807147\n",
      "Iteration 710, loss = 2.31339489\n",
      "Iteration 711, loss = 2.29877783\n",
      "Iteration 712, loss = 2.28422036\n",
      "Iteration 713, loss = 2.26972253\n",
      "Iteration 714, loss = 2.25528441\n",
      "Iteration 715, loss = 2.24090607\n",
      "Iteration 716, loss = 2.22658757\n",
      "Iteration 717, loss = 2.21232897\n",
      "Iteration 718, loss = 2.19813032\n",
      "Iteration 719, loss = 2.18399168\n",
      "Iteration 720, loss = 2.16991310\n",
      "Iteration 721, loss = 2.15589465\n",
      "Iteration 722, loss = 2.14193636\n",
      "Iteration 723, loss = 2.12803829\n",
      "Iteration 724, loss = 2.11420048\n",
      "Iteration 725, loss = 2.10042299\n",
      "Iteration 726, loss = 2.08670585\n",
      "Iteration 727, loss = 2.07304912\n",
      "Iteration 728, loss = 2.05945282\n",
      "Iteration 729, loss = 2.04591699\n",
      "Iteration 730, loss = 2.03244168\n",
      "Iteration 731, loss = 2.01902692\n",
      "Iteration 732, loss = 2.00567274\n",
      "Iteration 733, loss = 1.99237917\n",
      "Iteration 734, loss = 1.97914625\n",
      "Iteration 735, loss = 1.96597399\n",
      "Iteration 736, loss = 1.95286243\n",
      "Iteration 737, loss = 1.93981159\n",
      "Iteration 738, loss = 1.92682148\n",
      "Iteration 739, loss = 1.91389213\n",
      "Iteration 740, loss = 1.90102356\n",
      "Iteration 741, loss = 1.88821578\n",
      "Iteration 742, loss = 1.87546881\n",
      "Iteration 743, loss = 1.86278265\n",
      "Iteration 744, loss = 1.85015732\n",
      "Iteration 745, loss = 1.83759283\n",
      "Iteration 746, loss = 1.82508917\n",
      "Iteration 747, loss = 1.81264635\n",
      "Iteration 748, loss = 1.80026438\n",
      "Iteration 749, loss = 1.78794325\n",
      "Iteration 750, loss = 1.77568296\n",
      "Iteration 751, loss = 1.76348350\n",
      "Iteration 752, loss = 1.75134487\n",
      "Iteration 753, loss = 1.73926706\n",
      "Iteration 754, loss = 1.72725006\n",
      "Iteration 755, loss = 1.71529385\n",
      "Iteration 756, loss = 1.70339841\n",
      "Iteration 757, loss = 1.69156374\n",
      "Iteration 758, loss = 1.67978981\n",
      "Iteration 759, loss = 1.66807660\n",
      "Iteration 760, loss = 1.65642408\n",
      "Iteration 761, loss = 1.64483223\n",
      "Iteration 762, loss = 1.63330102\n",
      "Iteration 763, loss = 1.62183042\n",
      "Iteration 764, loss = 1.61042039\n",
      "Iteration 765, loss = 1.59907090\n",
      "Iteration 766, loss = 1.58778192\n",
      "Iteration 767, loss = 1.57655341\n",
      "Iteration 768, loss = 1.56538532\n",
      "Iteration 769, loss = 1.55427760\n",
      "Iteration 770, loss = 1.54323022\n",
      "Iteration 771, loss = 1.53224313\n",
      "Iteration 772, loss = 1.52131628\n",
      "Iteration 773, loss = 1.51044960\n",
      "Iteration 774, loss = 1.49964306\n",
      "Iteration 775, loss = 1.48889658\n",
      "Iteration 776, loss = 1.47821012\n",
      "Iteration 777, loss = 1.46758362\n",
      "Iteration 778, loss = 1.45701700\n",
      "Iteration 779, loss = 1.44651020\n",
      "Iteration 780, loss = 1.43606317\n",
      "Iteration 781, loss = 1.42567581\n",
      "Iteration 782, loss = 1.41534808\n",
      "Iteration 783, loss = 1.40507988\n",
      "Iteration 784, loss = 1.39487114\n",
      "Iteration 785, loss = 1.38472180\n",
      "Iteration 786, loss = 1.37463175\n",
      "Iteration 787, loss = 1.36460093\n",
      "Iteration 788, loss = 1.35462925\n",
      "Iteration 789, loss = 1.34471661\n",
      "Iteration 790, loss = 1.33486294\n",
      "Iteration 791, loss = 1.32506814\n",
      "Iteration 792, loss = 1.31533212\n",
      "Iteration 793, loss = 1.30565478\n",
      "Iteration 794, loss = 1.29603602\n",
      "Iteration 795, loss = 1.28647574\n",
      "Iteration 796, loss = 1.27697385\n",
      "Iteration 797, loss = 1.26753024\n",
      "Iteration 798, loss = 1.25814480\n",
      "Iteration 799, loss = 1.24881743\n",
      "Iteration 800, loss = 1.23954801\n",
      "Iteration 801, loss = 1.23033643\n",
      "Iteration 802, loss = 1.22118259\n",
      "Iteration 803, loss = 1.21208635\n",
      "Iteration 804, loss = 1.20304762\n",
      "Iteration 805, loss = 1.19406625\n",
      "Iteration 806, loss = 1.18514215\n",
      "Iteration 807, loss = 1.17627517\n",
      "Iteration 808, loss = 1.16746519\n",
      "Iteration 809, loss = 1.15871209\n",
      "Iteration 810, loss = 1.15001573\n",
      "Iteration 811, loss = 1.14137599\n",
      "Iteration 812, loss = 1.13279272\n",
      "Iteration 813, loss = 1.12426580\n",
      "Iteration 814, loss = 1.11579509\n",
      "Iteration 815, loss = 1.10738044\n",
      "Iteration 816, loss = 1.09902172\n",
      "Iteration 817, loss = 1.09071879\n",
      "Iteration 818, loss = 1.08247149\n",
      "Iteration 819, loss = 1.07427968\n",
      "Iteration 820, loss = 1.06614321\n",
      "Iteration 821, loss = 1.05806194\n",
      "Iteration 822, loss = 1.05003571\n",
      "Iteration 823, loss = 1.04206436\n",
      "Iteration 824, loss = 1.03414774\n",
      "Iteration 825, loss = 1.02628570\n",
      "Iteration 826, loss = 1.01847807\n",
      "Iteration 827, loss = 1.01072469\n",
      "Iteration 828, loss = 1.00302540\n",
      "Iteration 829, loss = 0.99538004\n",
      "Iteration 830, loss = 0.98778845\n",
      "Iteration 831, loss = 0.98025044\n",
      "Iteration 832, loss = 0.97276586\n",
      "Iteration 833, loss = 0.96533453\n",
      "Iteration 834, loss = 0.95795629\n",
      "Iteration 835, loss = 0.95063095\n",
      "Iteration 836, loss = 0.94335835\n",
      "Iteration 837, loss = 0.93613830\n",
      "Iteration 838, loss = 0.92897063\n",
      "Iteration 839, loss = 0.92185516\n",
      "Iteration 840, loss = 0.91479170\n",
      "Iteration 841, loss = 0.90778008\n",
      "Iteration 842, loss = 0.90082010\n",
      "Iteration 843, loss = 0.89391159\n",
      "Iteration 844, loss = 0.88705436\n",
      "Iteration 845, loss = 0.88024822\n",
      "Iteration 846, loss = 0.87349297\n",
      "Iteration 847, loss = 0.86678844\n",
      "Iteration 848, loss = 0.86013442\n",
      "Iteration 849, loss = 0.85353072\n",
      "Iteration 850, loss = 0.84697715\n",
      "Iteration 851, loss = 0.84047351\n",
      "Iteration 852, loss = 0.83401961\n",
      "Iteration 853, loss = 0.82761524\n",
      "Iteration 854, loss = 0.82126020\n",
      "Iteration 855, loss = 0.81495431\n",
      "Iteration 856, loss = 0.80869734\n",
      "Iteration 857, loss = 0.80248910\n",
      "Iteration 858, loss = 0.79632939\n",
      "Iteration 859, loss = 0.79021800\n",
      "Iteration 860, loss = 0.78415472\n",
      "Iteration 861, loss = 0.77813935\n",
      "Iteration 862, loss = 0.77217167\n",
      "Iteration 863, loss = 0.76625148\n",
      "Iteration 864, loss = 0.76037857\n",
      "Iteration 865, loss = 0.75455272\n",
      "Iteration 866, loss = 0.74877373\n",
      "Iteration 867, loss = 0.74304138\n",
      "Iteration 868, loss = 0.73735545\n",
      "Iteration 869, loss = 0.73171573\n",
      "Iteration 870, loss = 0.72612201\n",
      "Iteration 871, loss = 0.72057407\n",
      "Iteration 872, loss = 0.71507168\n",
      "Iteration 873, loss = 0.70961464\n",
      "Iteration 874, loss = 0.70420272\n",
      "Iteration 875, loss = 0.69883570\n",
      "Iteration 876, loss = 0.69351336\n",
      "Iteration 877, loss = 0.68823549\n",
      "Iteration 878, loss = 0.68300185\n",
      "Iteration 879, loss = 0.67781223\n",
      "Iteration 880, loss = 0.67266640\n",
      "Iteration 881, loss = 0.66756414\n",
      "Iteration 882, loss = 0.66250523\n",
      "Iteration 883, loss = 0.65748943\n",
      "Iteration 884, loss = 0.65251652\n",
      "Iteration 885, loss = 0.64758629\n",
      "Iteration 886, loss = 0.64269849\n",
      "Iteration 887, loss = 0.63785290\n",
      "Iteration 888, loss = 0.63304930\n",
      "Iteration 889, loss = 0.62828746\n",
      "Iteration 890, loss = 0.62356715\n",
      "Iteration 891, loss = 0.61888813\n",
      "Iteration 892, loss = 0.61425019\n",
      "Iteration 893, loss = 0.60965308\n",
      "Iteration 894, loss = 0.60509659\n",
      "Iteration 895, loss = 0.60058047\n",
      "Iteration 896, loss = 0.59610451\n",
      "Iteration 897, loss = 0.59166846\n",
      "Iteration 898, loss = 0.58727210\n",
      "Iteration 899, loss = 0.58291520\n",
      "Iteration 900, loss = 0.57859752\n",
      "Iteration 901, loss = 0.57431883\n",
      "Iteration 902, loss = 0.57007890\n",
      "Iteration 903, loss = 0.56587750\n",
      "Iteration 904, loss = 0.56171439\n",
      "Iteration 905, loss = 0.55758934\n",
      "Iteration 906, loss = 0.55350213\n",
      "Iteration 907, loss = 0.54945251\n",
      "Iteration 908, loss = 0.54544026\n",
      "Iteration 909, loss = 0.54146514\n",
      "Iteration 910, loss = 0.53752692\n",
      "Iteration 911, loss = 0.53362536\n",
      "Iteration 912, loss = 0.52976024\n",
      "Iteration 913, loss = 0.52593131\n",
      "Iteration 914, loss = 0.52213836\n",
      "Iteration 915, loss = 0.51838114\n",
      "Iteration 916, loss = 0.51465942\n",
      "Iteration 917, loss = 0.51097297\n",
      "Iteration 918, loss = 0.50732157\n",
      "Iteration 919, loss = 0.50370496\n",
      "Iteration 920, loss = 0.50012293\n",
      "Iteration 921, loss = 0.49657524\n",
      "Iteration 922, loss = 0.49306166\n",
      "Iteration 923, loss = 0.48958196\n",
      "Iteration 924, loss = 0.48613591\n",
      "Iteration 925, loss = 0.48272327\n",
      "Iteration 926, loss = 0.47934382\n",
      "Iteration 927, loss = 0.47599733\n",
      "Iteration 928, loss = 0.47268356\n",
      "Iteration 929, loss = 0.46940229\n",
      "Iteration 930, loss = 0.46615328\n",
      "Iteration 931, loss = 0.46293631\n",
      "Iteration 932, loss = 0.45975115\n",
      "Iteration 933, loss = 0.45659757\n",
      "Iteration 934, loss = 0.45347534\n",
      "Iteration 935, loss = 0.45038424\n",
      "Iteration 936, loss = 0.44732404\n",
      "Iteration 937, loss = 0.44429450\n",
      "Iteration 938, loss = 0.44129542\n",
      "Iteration 939, loss = 0.43832655\n",
      "Iteration 940, loss = 0.43538767\n",
      "Iteration 941, loss = 0.43247857\n",
      "Iteration 942, loss = 0.42959901\n",
      "Iteration 943, loss = 0.42674877\n",
      "Iteration 944, loss = 0.42392764\n",
      "Iteration 945, loss = 0.42113538\n",
      "Iteration 946, loss = 0.41837177\n",
      "Iteration 947, loss = 0.41563660\n",
      "Iteration 948, loss = 0.41292964\n",
      "Iteration 949, loss = 0.41025067\n",
      "Iteration 950, loss = 0.40759948\n",
      "Iteration 951, loss = 0.40497584\n",
      "Iteration 952, loss = 0.40237953\n",
      "Iteration 953, loss = 0.39981035\n",
      "Iteration 954, loss = 0.39726807\n",
      "Iteration 955, loss = 0.39475247\n",
      "Iteration 956, loss = 0.39226334\n",
      "Iteration 957, loss = 0.38980047\n",
      "Iteration 958, loss = 0.38736365\n",
      "Iteration 959, loss = 0.38495265\n",
      "Iteration 960, loss = 0.38256726\n",
      "Iteration 961, loss = 0.38020729\n",
      "Iteration 962, loss = 0.37787250\n",
      "Iteration 963, loss = 0.37556270\n",
      "Iteration 964, loss = 0.37327768\n",
      "Iteration 965, loss = 0.37101722\n",
      "Iteration 966, loss = 0.36878112\n",
      "Iteration 967, loss = 0.36656917\n",
      "Iteration 968, loss = 0.36438116\n",
      "Iteration 969, loss = 0.36221690\n",
      "Iteration 970, loss = 0.36007617\n",
      "Iteration 971, loss = 0.35795877\n",
      "Iteration 972, loss = 0.35586451\n",
      "Iteration 973, loss = 0.35379317\n",
      "Iteration 974, loss = 0.35174456\n",
      "Iteration 975, loss = 0.34971847\n",
      "Iteration 976, loss = 0.34771472\n",
      "Iteration 977, loss = 0.34573309\n",
      "Iteration 978, loss = 0.34377340\n",
      "Iteration 979, loss = 0.34183545\n",
      "Iteration 980, loss = 0.33991903\n",
      "Iteration 981, loss = 0.33802397\n",
      "Iteration 982, loss = 0.33615006\n",
      "Iteration 983, loss = 0.33429711\n",
      "Iteration 984, loss = 0.33246492\n",
      "Iteration 985, loss = 0.33065332\n",
      "Iteration 986, loss = 0.32886211\n",
      "Iteration 987, loss = 0.32709110\n",
      "Iteration 988, loss = 0.32534011\n",
      "Iteration 989, loss = 0.32360894\n",
      "Iteration 990, loss = 0.32189741\n",
      "Iteration 991, loss = 0.32020534\n",
      "Iteration 992, loss = 0.31853254\n",
      "Iteration 993, loss = 0.31687884\n",
      "Iteration 994, loss = 0.31524404\n",
      "Iteration 995, loss = 0.31362797\n",
      "Iteration 996, loss = 0.31203045\n",
      "Iteration 997, loss = 0.31045131\n",
      "Iteration 998, loss = 0.30889035\n",
      "Iteration 999, loss = 0.30734742\n",
      "Iteration 1000, loss = 0.30582233\n",
      "Iteration 1001, loss = 0.30431490\n",
      "Iteration 1002, loss = 0.30282497\n",
      "Iteration 1003, loss = 0.30135236\n",
      "Iteration 1004, loss = 0.29989690\n",
      "Iteration 1005, loss = 0.29845843\n",
      "Iteration 1006, loss = 0.29703677\n",
      "Iteration 1007, loss = 0.29563175\n",
      "Iteration 1008, loss = 0.29424321\n",
      "Iteration 1009, loss = 0.29287098\n",
      "Iteration 1010, loss = 0.29151490\n",
      "Iteration 1011, loss = 0.29017480\n",
      "Iteration 1012, loss = 0.28885052\n",
      "Iteration 1013, loss = 0.28754191\n",
      "Iteration 1014, loss = 0.28624879\n",
      "Iteration 1015, loss = 0.28497102\n",
      "Iteration 1016, loss = 0.28370842\n",
      "Iteration 1017, loss = 0.28246085\n",
      "Iteration 1018, loss = 0.28122815\n",
      "Iteration 1019, loss = 0.28001017\n",
      "Iteration 1020, loss = 0.27880674\n",
      "Iteration 1021, loss = 0.27761773\n",
      "Iteration 1022, loss = 0.27644296\n",
      "Iteration 1023, loss = 0.27528231\n",
      "Iteration 1024, loss = 0.27413561\n",
      "Iteration 1025, loss = 0.27300271\n",
      "Iteration 1026, loss = 0.27188348\n",
      "Iteration 1027, loss = 0.27077776\n",
      "Iteration 1028, loss = 0.26968541\n",
      "Iteration 1029, loss = 0.26860629\n",
      "Iteration 1030, loss = 0.26754024\n",
      "Iteration 1031, loss = 0.26648714\n",
      "Iteration 1032, loss = 0.26544683\n",
      "Iteration 1033, loss = 0.26441919\n",
      "Iteration 1034, loss = 0.26340406\n",
      "Iteration 1035, loss = 0.26240132\n",
      "Iteration 1036, loss = 0.26141083\n",
      "Iteration 1037, loss = 0.26043245\n",
      "Iteration 1038, loss = 0.25946604\n",
      "Iteration 1039, loss = 0.25851148\n",
      "Iteration 1040, loss = 0.25756863\n",
      "Iteration 1041, loss = 0.25663736\n",
      "Iteration 1042, loss = 0.25571754\n",
      "Iteration 1043, loss = 0.25480904\n",
      "Iteration 1044, loss = 0.25391174\n",
      "Iteration 1045, loss = 0.25302550\n",
      "Iteration 1046, loss = 0.25215021\n",
      "Iteration 1047, loss = 0.25128573\n",
      "Iteration 1048, loss = 0.25043194\n",
      "Iteration 1049, loss = 0.24958873\n",
      "Iteration 1050, loss = 0.24875596\n",
      "Iteration 1051, loss = 0.24793352\n",
      "Iteration 1052, loss = 0.24712129\n",
      "Iteration 1053, loss = 0.24631914\n",
      "Iteration 1054, loss = 0.24552697\n",
      "Iteration 1055, loss = 0.24474466\n",
      "Iteration 1056, loss = 0.24397209\n",
      "Iteration 1057, loss = 0.24320914\n",
      "Iteration 1058, loss = 0.24245571\n",
      "Iteration 1059, loss = 0.24171167\n",
      "Iteration 1060, loss = 0.24097693\n",
      "Iteration 1061, loss = 0.24025136\n",
      "Iteration 1062, loss = 0.23953487\n",
      "Iteration 1063, loss = 0.23882733\n",
      "Iteration 1064, loss = 0.23812865\n",
      "Iteration 1065, loss = 0.23743872\n",
      "Iteration 1066, loss = 0.23675743\n",
      "Iteration 1067, loss = 0.23608468\n",
      "Iteration 1068, loss = 0.23542036\n",
      "Iteration 1069, loss = 0.23476438\n",
      "Iteration 1070, loss = 0.23411663\n",
      "Iteration 1071, loss = 0.23347700\n",
      "Iteration 1072, loss = 0.23284541\n",
      "Iteration 1073, loss = 0.23222175\n",
      "Iteration 1074, loss = 0.23160592\n",
      "Iteration 1075, loss = 0.23099784\n",
      "Iteration 1076, loss = 0.23039739\n",
      "Iteration 1077, loss = 0.22980449\n",
      "Iteration 1078, loss = 0.22921905\n",
      "Iteration 1079, loss = 0.22864097\n",
      "Iteration 1080, loss = 0.22807015\n",
      "Iteration 1081, loss = 0.22750651\n",
      "Iteration 1082, loss = 0.22694997\n",
      "Iteration 1083, loss = 0.22640041\n",
      "Iteration 1084, loss = 0.22585777\n",
      "Iteration 1085, loss = 0.22532195\n",
      "Iteration 1086, loss = 0.22479287\n",
      "Iteration 1087, loss = 0.22427044\n",
      "Iteration 1088, loss = 0.22375457\n",
      "Iteration 1089, loss = 0.22324519\n",
      "Iteration 1090, loss = 0.22274220\n",
      "Iteration 1091, loss = 0.22224553\n",
      "Iteration 1092, loss = 0.22175509\n",
      "Iteration 1093, loss = 0.22127081\n",
      "Iteration 1094, loss = 0.22079260\n",
      "Iteration 1095, loss = 0.22032038\n",
      "Iteration 1096, loss = 0.21985409\n",
      "Iteration 1097, loss = 0.21939363\n",
      "Iteration 1098, loss = 0.21893894\n",
      "Iteration 1099, loss = 0.21848994\n",
      "Iteration 1100, loss = 0.21804655\n",
      "Iteration 1101, loss = 0.21760870\n",
      "Iteration 1102, loss = 0.21717632\n",
      "Iteration 1103, loss = 0.21674934\n",
      "Iteration 1104, loss = 0.21632767\n",
      "Iteration 1105, loss = 0.21591126\n",
      "Iteration 1106, loss = 0.21550003\n",
      "Iteration 1107, loss = 0.21509392\n",
      "Iteration 1108, loss = 0.21469285\n",
      "Iteration 1109, loss = 0.21429676\n",
      "Iteration 1110, loss = 0.21390557\n",
      "Iteration 1111, loss = 0.21351924\n",
      "Iteration 1112, loss = 0.21313768\n",
      "Iteration 1113, loss = 0.21276083\n",
      "Iteration 1114, loss = 0.21238864\n",
      "Iteration 1115, loss = 0.21202103\n",
      "Iteration 1116, loss = 0.21165795\n",
      "Iteration 1117, loss = 0.21129932\n",
      "Iteration 1118, loss = 0.21094511\n",
      "Iteration 1119, loss = 0.21059523\n",
      "Iteration 1120, loss = 0.21024963\n",
      "Iteration 1121, loss = 0.20990826\n",
      "Iteration 1122, loss = 0.20957105\n",
      "Iteration 1123, loss = 0.20923795\n",
      "Iteration 1124, loss = 0.20890890\n",
      "Iteration 1125, loss = 0.20858384\n",
      "Iteration 1126, loss = 0.20826272\n",
      "Iteration 1127, loss = 0.20794549\n",
      "Iteration 1128, loss = 0.20763208\n",
      "Iteration 1129, loss = 0.20732245\n",
      "Iteration 1130, loss = 0.20701654\n",
      "Iteration 1131, loss = 0.20671430\n",
      "Iteration 1132, loss = 0.20641568\n",
      "Iteration 1133, loss = 0.20612063\n",
      "Iteration 1134, loss = 0.20582909\n",
      "Iteration 1135, loss = 0.20554102\n",
      "Iteration 1136, loss = 0.20525637\n",
      "Iteration 1137, loss = 0.20497509\n",
      "Iteration 1138, loss = 0.20469713\n",
      "Iteration 1139, loss = 0.20442244\n",
      "Iteration 1140, loss = 0.20415098\n",
      "Iteration 1141, loss = 0.20388270\n",
      "Iteration 1142, loss = 0.20361755\n",
      "Iteration 1143, loss = 0.20335550\n",
      "Iteration 1144, loss = 0.20309648\n",
      "Iteration 1145, loss = 0.20284047\n",
      "Iteration 1146, loss = 0.20258742\n",
      "Iteration 1147, loss = 0.20233728\n",
      "Iteration 1148, loss = 0.20209001\n",
      "Iteration 1149, loss = 0.20184557\n",
      "Iteration 1150, loss = 0.20160393\n",
      "Iteration 1151, loss = 0.20136503\n",
      "Iteration 1152, loss = 0.20112883\n",
      "Iteration 1153, loss = 0.20089531\n",
      "Iteration 1154, loss = 0.20066441\n",
      "Iteration 1155, loss = 0.20043611\n",
      "Iteration 1156, loss = 0.20021035\n",
      "Iteration 1157, loss = 0.19998711\n",
      "Iteration 1158, loss = 0.19976635\n",
      "Iteration 1159, loss = 0.19954803\n",
      "Iteration 1160, loss = 0.19933211\n",
      "Iteration 1161, loss = 0.19911855\n",
      "Iteration 1162, loss = 0.19890733\n",
      "Iteration 1163, loss = 0.19869841\n",
      "Iteration 1164, loss = 0.19849175\n",
      "Iteration 1165, loss = 0.19828732\n",
      "Iteration 1166, loss = 0.19808509\n",
      "Iteration 1167, loss = 0.19788502\n",
      "Iteration 1168, loss = 0.19768708\n",
      "Iteration 1169, loss = 0.19749123\n",
      "Iteration 1170, loss = 0.19729745\n",
      "Iteration 1171, loss = 0.19710571\n",
      "Iteration 1172, loss = 0.19691597\n",
      "Iteration 1173, loss = 0.19672820\n",
      "Iteration 1174, loss = 0.19654238\n",
      "Iteration 1175, loss = 0.19635847\n",
      "Iteration 1176, loss = 0.19617644\n",
      "Iteration 1177, loss = 0.19599627\n",
      "Iteration 1178, loss = 0.19581793\n",
      "Iteration 1179, loss = 0.19564138\n",
      "Iteration 1180, loss = 0.19546661\n",
      "Iteration 1181, loss = 0.19529358\n",
      "Iteration 1182, loss = 0.19512226\n",
      "Iteration 1183, loss = 0.19495264\n",
      "Iteration 1184, loss = 0.19478468\n",
      "Iteration 1185, loss = 0.19461835\n",
      "Iteration 1186, loss = 0.19445364\n",
      "Iteration 1187, loss = 0.19429052\n",
      "Iteration 1188, loss = 0.19412896\n",
      "Iteration 1189, loss = 0.19396894\n",
      "Iteration 1190, loss = 0.19381043\n",
      "Iteration 1191, loss = 0.19365341\n",
      "Iteration 1192, loss = 0.19349786\n",
      "Iteration 1193, loss = 0.19334375\n",
      "Iteration 1194, loss = 0.19319106\n",
      "Iteration 1195, loss = 0.19303978\n",
      "Iteration 1196, loss = 0.19288986\n",
      "Iteration 1197, loss = 0.19274131\n",
      "Iteration 1198, loss = 0.19259408\n",
      "Iteration 1199, loss = 0.19244817\n",
      "Iteration 1200, loss = 0.19230355\n",
      "Iteration 1201, loss = 0.19216020\n",
      "Iteration 1202, loss = 0.19201810\n",
      "Iteration 1203, loss = 0.19187723\n",
      "Iteration 1204, loss = 0.19173757\n",
      "Iteration 1205, loss = 0.19159910\n",
      "Iteration 1206, loss = 0.19146180\n",
      "Iteration 1207, loss = 0.19132566\n",
      "Iteration 1208, loss = 0.19119065\n",
      "Iteration 1209, loss = 0.19105676\n",
      "Iteration 1210, loss = 0.19092396\n",
      "Iteration 1211, loss = 0.19079225\n",
      "Iteration 1212, loss = 0.19066160\n",
      "Iteration 1213, loss = 0.19053199\n",
      "Iteration 1214, loss = 0.19040341\n",
      "Iteration 1215, loss = 0.19027585\n",
      "Iteration 1216, loss = 0.19014928\n",
      "Iteration 1217, loss = 0.19002368\n",
      "Iteration 1218, loss = 0.18989906\n",
      "Iteration 1219, loss = 0.18977538\n",
      "Iteration 1220, loss = 0.18965263\n",
      "Iteration 1221, loss = 0.18953079\n",
      "Iteration 1222, loss = 0.18940986\n",
      "Iteration 1223, loss = 0.18928982\n",
      "Iteration 1224, loss = 0.18917065\n",
      "Iteration 1225, loss = 0.18905233\n",
      "Iteration 1226, loss = 0.18893486\n",
      "Iteration 1227, loss = 0.18881822\n",
      "Iteration 1228, loss = 0.18870239\n",
      "Iteration 1229, loss = 0.18858737\n",
      "Iteration 1230, loss = 0.18847313\n",
      "Iteration 1231, loss = 0.18835967\n",
      "Iteration 1232, loss = 0.18824697\n",
      "Iteration 1233, loss = 0.18813503\n",
      "Iteration 1234, loss = 0.18802382\n",
      "Iteration 1235, loss = 0.18791333\n",
      "Iteration 1236, loss = 0.18780356\n",
      "Iteration 1237, loss = 0.18769449\n",
      "Iteration 1238, loss = 0.18758611\n",
      "Iteration 1239, loss = 0.18747840\n",
      "Iteration 1240, loss = 0.18737136\n",
      "Iteration 1241, loss = 0.18726498\n",
      "Iteration 1242, loss = 0.18715924\n",
      "Iteration 1243, loss = 0.18705413\n",
      "Iteration 1244, loss = 0.18694965\n",
      "Iteration 1245, loss = 0.18684578\n",
      "Iteration 1246, loss = 0.18674251\n",
      "Iteration 1247, loss = 0.18663983\n",
      "Iteration 1248, loss = 0.18653773\n",
      "Iteration 1249, loss = 0.18643620\n",
      "Iteration 1250, loss = 0.18633523\n",
      "Iteration 1251, loss = 0.18623482\n",
      "Iteration 1252, loss = 0.18613494\n",
      "Iteration 1253, loss = 0.18603560\n",
      "Iteration 1254, loss = 0.18593679\n",
      "Iteration 1255, loss = 0.18583849\n",
      "Iteration 1256, loss = 0.18574069\n",
      "Iteration 1257, loss = 0.18564339\n",
      "Iteration 1258, loss = 0.18554658\n",
      "Iteration 1259, loss = 0.18545025\n",
      "Iteration 1260, loss = 0.18535440\n",
      "Iteration 1261, loss = 0.18525900\n",
      "Iteration 1262, loss = 0.18516407\n",
      "Iteration 1263, loss = 0.18506958\n",
      "Iteration 1264, loss = 0.18497553\n",
      "Iteration 1265, loss = 0.18488191\n",
      "Iteration 1266, loss = 0.18478872\n",
      "Iteration 1267, loss = 0.18469595\n",
      "Iteration 1268, loss = 0.18460358\n",
      "Iteration 1269, loss = 0.18451162\n",
      "Iteration 1270, loss = 0.18442006\n",
      "Iteration 1271, loss = 0.18432888\n",
      "Iteration 1272, loss = 0.18423809\n",
      "Iteration 1273, loss = 0.18414767\n",
      "Iteration 1274, loss = 0.18405762\n",
      "Iteration 1275, loss = 0.18396793\n",
      "Iteration 1276, loss = 0.18387860\n",
      "Iteration 1277, loss = 0.18378962\n",
      "Iteration 1278, loss = 0.18370099\n",
      "Iteration 1279, loss = 0.18361269\n",
      "Iteration 1280, loss = 0.18352472\n",
      "Iteration 1281, loss = 0.18343708\n",
      "Iteration 1282, loss = 0.18334976\n",
      "Iteration 1283, loss = 0.18326275\n",
      "Iteration 1284, loss = 0.18317605\n",
      "Iteration 1285, loss = 0.18308966\n",
      "Iteration 1286, loss = 0.18300356\n",
      "Iteration 1287, loss = 0.18291776\n",
      "Iteration 1288, loss = 0.18283224\n",
      "Iteration 1289, loss = 0.18274701\n",
      "Iteration 1290, loss = 0.18266205\n",
      "Iteration 1291, loss = 0.18257737\n",
      "Iteration 1292, loss = 0.18249295\n",
      "Iteration 1293, loss = 0.18240880\n",
      "Iteration 1294, loss = 0.18232490\n",
      "Iteration 1295, loss = 0.18224126\n",
      "Iteration 1296, loss = 0.18215786\n",
      "Iteration 1297, loss = 0.18207471\n",
      "Iteration 1298, loss = 0.18199180\n",
      "Iteration 1299, loss = 0.18190913\n",
      "Iteration 1300, loss = 0.18182669\n",
      "Iteration 1301, loss = 0.18174447\n",
      "Iteration 1302, loss = 0.18166248\n",
      "Iteration 1303, loss = 0.18158071\n",
      "Iteration 1304, loss = 0.18149915\n",
      "Iteration 1305, loss = 0.18141780\n",
      "Iteration 1306, loss = 0.18133666\n",
      "Iteration 1307, loss = 0.18125573\n",
      "Iteration 1308, loss = 0.18117499\n",
      "Iteration 1309, loss = 0.18109446\n",
      "Iteration 1310, loss = 0.18101411\n",
      "Iteration 1311, loss = 0.18093396\n",
      "Iteration 1312, loss = 0.18085399\n",
      "Iteration 1313, loss = 0.18077420\n",
      "Iteration 1314, loss = 0.18069459\n",
      "Iteration 1315, loss = 0.18061516\n",
      "Iteration 1316, loss = 0.18053590\n",
      "Iteration 1317, loss = 0.18045681\n",
      "Iteration 1318, loss = 0.18037789\n",
      "Iteration 1319, loss = 0.18029913\n",
      "Iteration 1320, loss = 0.18022053\n",
      "Iteration 1321, loss = 0.18014209\n",
      "Iteration 1322, loss = 0.18006380\n",
      "Iteration 1323, loss = 0.17998567\n",
      "Iteration 1324, loss = 0.17990768\n",
      "Iteration 1325, loss = 0.17982984\n",
      "Iteration 1326, loss = 0.17975215\n",
      "Iteration 1327, loss = 0.17967460\n",
      "Iteration 1328, loss = 0.17959718\n",
      "Iteration 1329, loss = 0.17951990\n",
      "Iteration 1330, loss = 0.17944276\n",
      "Iteration 1331, loss = 0.17936574\n",
      "Iteration 1332, loss = 0.17928886\n",
      "Iteration 1333, loss = 0.17921210\n",
      "Iteration 1334, loss = 0.17913547\n",
      "Iteration 1335, loss = 0.17905895\n",
      "Iteration 1336, loss = 0.17898256\n",
      "Iteration 1337, loss = 0.17890629\n",
      "Iteration 1338, loss = 0.17883012\n",
      "Iteration 1339, loss = 0.17875408\n",
      "Iteration 1340, loss = 0.17867814\n",
      "Iteration 1341, loss = 0.17860231\n",
      "Iteration 1342, loss = 0.17852660\n",
      "Iteration 1343, loss = 0.17845098\n",
      "Iteration 1344, loss = 0.17837547\n",
      "Iteration 1345, loss = 0.17830006\n",
      "Iteration 1346, loss = 0.17822475\n",
      "Iteration 1347, loss = 0.17814954\n",
      "Iteration 1348, loss = 0.17807442\n",
      "Iteration 1349, loss = 0.17799940\n",
      "Iteration 1350, loss = 0.17792446\n",
      "Iteration 1351, loss = 0.17784963\n",
      "Iteration 1352, loss = 0.17777487\n",
      "Iteration 1353, loss = 0.17770021\n",
      "Iteration 1354, loss = 0.17762563\n",
      "Iteration 1355, loss = 0.17755114\n",
      "Iteration 1356, loss = 0.17747673\n",
      "Iteration 1357, loss = 0.17740240\n",
      "Iteration 1358, loss = 0.17732815\n",
      "Iteration 1359, loss = 0.17725398\n",
      "Iteration 1360, loss = 0.17717989\n",
      "Iteration 1361, loss = 0.17710587\n",
      "Iteration 1362, loss = 0.17703193\n",
      "Iteration 1363, loss = 0.17695805\n",
      "Iteration 1364, loss = 0.17688425\n",
      "Iteration 1365, loss = 0.17681053\n",
      "Iteration 1366, loss = 0.17673687\n",
      "Iteration 1367, loss = 0.17666327\n",
      "Iteration 1368, loss = 0.17658975\n",
      "Iteration 1369, loss = 0.17651629\n",
      "Iteration 1370, loss = 0.17644289\n",
      "Iteration 1371, loss = 0.17636955\n",
      "Iteration 1372, loss = 0.17629628\n",
      "Iteration 1373, loss = 0.17622307\n",
      "Iteration 1374, loss = 0.17614992\n",
      "Iteration 1375, loss = 0.17607683\n",
      "Iteration 1376, loss = 0.17600379\n",
      "Iteration 1377, loss = 0.17593082\n",
      "Iteration 1378, loss = 0.17585789\n",
      "Iteration 1379, loss = 0.17578502\n",
      "Iteration 1380, loss = 0.17571221\n",
      "Iteration 1381, loss = 0.17563945\n",
      "Iteration 1382, loss = 0.17556674\n",
      "Iteration 1383, loss = 0.17549408\n",
      "Iteration 1384, loss = 0.17542147\n",
      "Iteration 1385, loss = 0.17534891\n",
      "Iteration 1386, loss = 0.17527640\n",
      "Iteration 1387, loss = 0.17520393\n",
      "Iteration 1388, loss = 0.17513152\n",
      "Iteration 1389, loss = 0.17505914\n",
      "Iteration 1390, loss = 0.17498682\n",
      "Iteration 1391, loss = 0.17491454\n",
      "Iteration 1392, loss = 0.17484230\n",
      "Iteration 1393, loss = 0.17477010\n",
      "Iteration 1394, loss = 0.17469795\n",
      "Iteration 1395, loss = 0.17462583\n",
      "Iteration 1396, loss = 0.17455376\n",
      "Iteration 1397, loss = 0.17448173\n",
      "Iteration 1398, loss = 0.17440974\n",
      "Iteration 1399, loss = 0.17433778\n",
      "Iteration 1400, loss = 0.17426587\n",
      "Iteration 1401, loss = 0.17419399\n",
      "Iteration 1402, loss = 0.17412215\n",
      "Iteration 1403, loss = 0.17405034\n",
      "Iteration 1404, loss = 0.17397857\n",
      "Iteration 1405, loss = 0.17390684\n",
      "Iteration 1406, loss = 0.17383514\n",
      "Iteration 1407, loss = 0.17376347\n",
      "Iteration 1408, loss = 0.17369184\n",
      "Iteration 1409, loss = 0.17362024\n",
      "Iteration 1410, loss = 0.17354867\n",
      "Iteration 1411, loss = 0.17347714\n",
      "Iteration 1412, loss = 0.17340563\n",
      "Iteration 1413, loss = 0.17333416\n",
      "Iteration 1414, loss = 0.17326272\n",
      "Iteration 1415, loss = 0.17319130\n",
      "Iteration 1416, loss = 0.17311992\n",
      "Iteration 1417, loss = 0.17304857\n",
      "Iteration 1418, loss = 0.17297724\n",
      "Iteration 1419, loss = 0.17290594\n",
      "Iteration 1420, loss = 0.17283467\n",
      "Iteration 1421, loss = 0.17276343\n",
      "Iteration 1422, loss = 0.17269221\n",
      "Iteration 1423, loss = 0.17262102\n",
      "Iteration 1424, loss = 0.17254986\n",
      "Iteration 1425, loss = 0.17247872\n",
      "Iteration 1426, loss = 0.17240761\n",
      "Iteration 1427, loss = 0.17233653\n",
      "Iteration 1428, loss = 0.17226546\n",
      "Iteration 1429, loss = 0.17219443\n",
      "Iteration 1430, loss = 0.17212341\n",
      "Iteration 1431, loss = 0.17205242\n",
      "Iteration 1432, loss = 0.17198146\n",
      "Iteration 1433, loss = 0.17191052\n",
      "Iteration 1434, loss = 0.17183960\n",
      "Iteration 1435, loss = 0.17176870\n",
      "Iteration 1436, loss = 0.17169782\n",
      "Iteration 1437, loss = 0.17162697\n",
      "Iteration 1438, loss = 0.17155614\n",
      "Iteration 1439, loss = 0.17148533\n",
      "Iteration 1440, loss = 0.17141454\n",
      "Iteration 1441, loss = 0.17134377\n",
      "Iteration 1442, loss = 0.17127303\n",
      "Iteration 1443, loss = 0.17120230\n",
      "Iteration 1444, loss = 0.17113159\n",
      "Iteration 1445, loss = 0.17106091\n",
      "Iteration 1446, loss = 0.17099024\n",
      "Iteration 1447, loss = 0.17091959\n",
      "Iteration 1448, loss = 0.17084897\n",
      "Iteration 1449, loss = 0.17077836\n",
      "Iteration 1450, loss = 0.17070777\n",
      "Iteration 1451, loss = 0.17063720\n",
      "Iteration 1452, loss = 0.17056664\n",
      "Iteration 1453, loss = 0.17049611\n",
      "Iteration 1454, loss = 0.17042559\n",
      "Iteration 1455, loss = 0.17035510\n",
      "Iteration 1456, loss = 0.17028462\n",
      "Iteration 1457, loss = 0.17021415\n",
      "Iteration 1458, loss = 0.17014371\n",
      "Iteration 1459, loss = 0.17007328\n",
      "Iteration 1460, loss = 0.17000287\n",
      "Iteration 1461, loss = 0.16993248\n",
      "Iteration 1462, loss = 0.16986210\n",
      "Iteration 1463, loss = 0.16979174\n",
      "Iteration 1464, loss = 0.16972140\n",
      "Iteration 1465, loss = 0.16965107\n",
      "Iteration 1466, loss = 0.16958076\n",
      "Iteration 1467, loss = 0.16951046\n",
      "Iteration 1468, loss = 0.16944018\n",
      "Iteration 1469, loss = 0.16936992\n",
      "Iteration 1470, loss = 0.16929967\n",
      "Iteration 1471, loss = 0.16922944\n",
      "Iteration 1472, loss = 0.16915923\n",
      "Iteration 1473, loss = 0.16908903\n",
      "Iteration 1474, loss = 0.16901884\n",
      "Iteration 1475, loss = 0.16894867\n",
      "Iteration 1476, loss = 0.16887852\n",
      "Iteration 1477, loss = 0.16880838\n",
      "Iteration 1478, loss = 0.16873825\n",
      "Iteration 1479, loss = 0.16866814\n",
      "Iteration 1480, loss = 0.16859805\n",
      "Iteration 1481, loss = 0.16852797\n",
      "Iteration 1482, loss = 0.16845790\n",
      "Iteration 1483, loss = 0.16838785\n",
      "Iteration 1484, loss = 0.16831782\n",
      "Iteration 1485, loss = 0.16824780\n",
      "Iteration 1486, loss = 0.16817779\n",
      "Iteration 1487, loss = 0.16810779\n",
      "Iteration 1488, loss = 0.16803782\n",
      "Iteration 1489, loss = 0.16796785\n",
      "Iteration 1490, loss = 0.16789790\n",
      "Iteration 1491, loss = 0.16782796\n",
      "Iteration 1492, loss = 0.16775804\n",
      "Iteration 1493, loss = 0.16768813\n",
      "Iteration 1494, loss = 0.16761824\n",
      "Iteration 1495, loss = 0.16754836\n",
      "Iteration 1496, loss = 0.16747849\n",
      "Iteration 1497, loss = 0.16740863\n",
      "Iteration 1498, loss = 0.16733879\n",
      "Iteration 1499, loss = 0.16726897\n",
      "Iteration 1500, loss = 0.16719916\n",
      "Iteration 1501, loss = 0.16712936\n",
      "Iteration 1502, loss = 0.16705957\n",
      "Iteration 1503, loss = 0.16698980\n",
      "Iteration 1504, loss = 0.16692004\n",
      "Iteration 1505, loss = 0.16685030\n",
      "Iteration 1506, loss = 0.16678056\n",
      "Iteration 1507, loss = 0.16671085\n",
      "Iteration 1508, loss = 0.16664114\n",
      "Iteration 1509, loss = 0.16657145\n",
      "Iteration 1510, loss = 0.16650177\n",
      "Iteration 1511, loss = 0.16643211\n",
      "Iteration 1512, loss = 0.16636246\n",
      "Iteration 1513, loss = 0.16629282\n",
      "Iteration 1514, loss = 0.16622319\n",
      "Iteration 1515, loss = 0.16615358\n",
      "Iteration 1516, loss = 0.16608398\n",
      "Iteration 1517, loss = 0.16601440\n",
      "Iteration 1518, loss = 0.16594482\n",
      "Iteration 1519, loss = 0.16587527\n",
      "Iteration 1520, loss = 0.16580572\n",
      "Iteration 1521, loss = 0.16573619\n",
      "Iteration 1522, loss = 0.16566667\n",
      "Iteration 1523, loss = 0.16559716\n",
      "Iteration 1524, loss = 0.16552767\n",
      "Iteration 1525, loss = 0.16545819\n",
      "Iteration 1526, loss = 0.16538872\n",
      "Iteration 1527, loss = 0.16531927\n",
      "Iteration 1528, loss = 0.16524983\n",
      "Iteration 1529, loss = 0.16518040\n",
      "Iteration 1530, loss = 0.16511099\n",
      "Iteration 1531, loss = 0.16504159\n",
      "Iteration 1532, loss = 0.16497220\n",
      "Iteration 1533, loss = 0.16490283\n",
      "Iteration 1534, loss = 0.16483347\n",
      "Iteration 1535, loss = 0.16476412\n",
      "Iteration 1536, loss = 0.16469479\n",
      "Iteration 1537, loss = 0.16462546\n",
      "Iteration 1538, loss = 0.16455616\n",
      "Iteration 1539, loss = 0.16448686\n",
      "Iteration 1540, loss = 0.16441758\n",
      "Iteration 1541, loss = 0.16434831\n",
      "Iteration 1542, loss = 0.16427906\n",
      "Iteration 1543, loss = 0.16420982\n",
      "Iteration 1544, loss = 0.16414059\n",
      "Iteration 1545, loss = 0.16407138\n",
      "Iteration 1546, loss = 0.16400217\n",
      "Iteration 1547, loss = 0.16393299\n",
      "Iteration 1548, loss = 0.16386381\n",
      "Iteration 1549, loss = 0.16379465\n",
      "Iteration 1550, loss = 0.16372550\n",
      "Iteration 1551, loss = 0.16365637\n",
      "Iteration 1552, loss = 0.16358725\n",
      "Iteration 1553, loss = 0.16351814\n",
      "Iteration 1554, loss = 0.16344905\n",
      "Iteration 1555, loss = 0.16337997\n",
      "Iteration 1556, loss = 0.16331091\n",
      "Iteration 1557, loss = 0.16324185\n",
      "Iteration 1558, loss = 0.16317281\n",
      "Iteration 1559, loss = 0.16310379\n",
      "Iteration 1560, loss = 0.16303478\n",
      "Iteration 1561, loss = 0.16296578\n",
      "Iteration 1562, loss = 0.16289680\n",
      "Iteration 1563, loss = 0.16282783\n",
      "Iteration 1564, loss = 0.16275887\n",
      "Iteration 1565, loss = 0.16268993\n",
      "Iteration 1566, loss = 0.16262100\n",
      "Iteration 1567, loss = 0.16255209\n",
      "Iteration 1568, loss = 0.16248319\n",
      "Iteration 1569, loss = 0.16241430\n",
      "Iteration 1570, loss = 0.16234543\n",
      "Iteration 1571, loss = 0.16227657\n",
      "Iteration 1572, loss = 0.16220773\n",
      "Iteration 1573, loss = 0.16213890\n",
      "Iteration 1574, loss = 0.16207008\n",
      "Iteration 1575, loss = 0.16200128\n",
      "Iteration 1576, loss = 0.16193249\n",
      "Iteration 1577, loss = 0.16186372\n",
      "Iteration 1578, loss = 0.16179496\n",
      "Iteration 1579, loss = 0.16172621\n",
      "Iteration 1580, loss = 0.16165748\n",
      "Iteration 1581, loss = 0.16158877\n",
      "Iteration 1582, loss = 0.16152007\n",
      "Iteration 1583, loss = 0.16145138\n",
      "Iteration 1584, loss = 0.16138271\n",
      "Iteration 1585, loss = 0.16131405\n",
      "Iteration 1586, loss = 0.16124541\n",
      "Iteration 1587, loss = 0.16117678\n",
      "Iteration 1588, loss = 0.16110816\n",
      "Iteration 1589, loss = 0.16103957\n",
      "Iteration 1590, loss = 0.16097098\n",
      "Iteration 1591, loss = 0.16090241\n",
      "Iteration 1592, loss = 0.16083386\n",
      "Iteration 1593, loss = 0.16076532\n",
      "Iteration 1594, loss = 0.16069679\n",
      "Iteration 1595, loss = 0.16062828\n",
      "Iteration 1596, loss = 0.16055979\n",
      "Iteration 1597, loss = 0.16049131\n",
      "Iteration 1598, loss = 0.16042284\n",
      "Iteration 1599, loss = 0.16035439\n",
      "Iteration 1600, loss = 0.16028596\n",
      "Iteration 1601, loss = 0.16021754\n",
      "Iteration 1602, loss = 0.16014913\n",
      "Iteration 1603, loss = 0.16008074\n",
      "Iteration 1604, loss = 0.16001237\n",
      "Iteration 1605, loss = 0.15994401\n",
      "Iteration 1606, loss = 0.15987567\n",
      "Iteration 1607, loss = 0.15980734\n",
      "Iteration 1608, loss = 0.15973903\n",
      "Iteration 1609, loss = 0.15967073\n",
      "Iteration 1610, loss = 0.15960245\n",
      "Iteration 1611, loss = 0.15953418\n",
      "Iteration 1612, loss = 0.15946593\n",
      "Iteration 1613, loss = 0.15939770\n",
      "Iteration 1614, loss = 0.15932948\n",
      "Iteration 1615, loss = 0.15926128\n",
      "Iteration 1616, loss = 0.15919309\n",
      "Iteration 1617, loss = 0.15912492\n",
      "Iteration 1618, loss = 0.15905677\n",
      "Iteration 1619, loss = 0.15898863\n",
      "Iteration 1620, loss = 0.15892050\n",
      "Iteration 1621, loss = 0.15885240\n",
      "Iteration 1622, loss = 0.15878431\n",
      "Iteration 1623, loss = 0.15871623\n",
      "Iteration 1624, loss = 0.15864817\n",
      "Iteration 1625, loss = 0.15858013\n",
      "Iteration 1626, loss = 0.15851210\n",
      "Iteration 1627, loss = 0.15844410\n",
      "Iteration 1628, loss = 0.15837610\n",
      "Iteration 1629, loss = 0.15830812\n",
      "Iteration 1630, loss = 0.15824016\n",
      "Iteration 1631, loss = 0.15817222\n",
      "Iteration 1632, loss = 0.15810429\n",
      "Iteration 1633, loss = 0.15803638\n",
      "Iteration 1634, loss = 0.15796849\n",
      "Iteration 1635, loss = 0.15790061\n",
      "Iteration 1636, loss = 0.15783275\n",
      "Iteration 1637, loss = 0.15776491\n",
      "Iteration 1638, loss = 0.15769708\n",
      "Iteration 1639, loss = 0.15762927\n",
      "Iteration 1640, loss = 0.15756148\n",
      "Iteration 1641, loss = 0.15749370\n",
      "Iteration 1642, loss = 0.15742594\n",
      "Iteration 1643, loss = 0.15735820\n",
      "Iteration 1644, loss = 0.15729047\n",
      "Iteration 1645, loss = 0.15722276\n",
      "Iteration 1646, loss = 0.15715507\n",
      "Iteration 1647, loss = 0.15708740\n",
      "Iteration 1648, loss = 0.15701974\n",
      "Iteration 1649, loss = 0.15695211\n",
      "Iteration 1650, loss = 0.15688448\n",
      "Iteration 1651, loss = 0.15681688\n",
      "Iteration 1652, loss = 0.15674929\n",
      "Iteration 1653, loss = 0.15668172\n",
      "Iteration 1654, loss = 0.15661417\n",
      "Iteration 1655, loss = 0.15654664\n",
      "Iteration 1656, loss = 0.15647912\n",
      "Iteration 1657, loss = 0.15641163\n",
      "Iteration 1658, loss = 0.15634414\n",
      "Iteration 1659, loss = 0.15627668\n",
      "Iteration 1660, loss = 0.15620924\n",
      "Iteration 1661, loss = 0.15614181\n",
      "Iteration 1662, loss = 0.15607440\n",
      "Iteration 1663, loss = 0.15600701\n",
      "Iteration 1664, loss = 0.15593964\n",
      "Iteration 1665, loss = 0.15587228\n",
      "Iteration 1666, loss = 0.15580495\n",
      "Iteration 1667, loss = 0.15573763\n",
      "Iteration 1668, loss = 0.15567033\n",
      "Iteration 1669, loss = 0.15560305\n",
      "Iteration 1670, loss = 0.15553578\n",
      "Iteration 1671, loss = 0.15546854\n",
      "Iteration 1672, loss = 0.15540131\n",
      "Iteration 1673, loss = 0.15533410\n",
      "Iteration 1674, loss = 0.15526691\n",
      "Iteration 1675, loss = 0.15519974\n",
      "Iteration 1676, loss = 0.15513259\n",
      "Iteration 1677, loss = 0.15506546\n",
      "Iteration 1678, loss = 0.15499834\n",
      "Iteration 1679, loss = 0.15493125\n",
      "Iteration 1680, loss = 0.15486417\n",
      "Iteration 1681, loss = 0.15479711\n",
      "Iteration 1682, loss = 0.15473007\n",
      "Iteration 1683, loss = 0.15466305\n",
      "Iteration 1684, loss = 0.15459605\n",
      "Iteration 1685, loss = 0.15452907\n",
      "Iteration 1686, loss = 0.15446210\n",
      "Iteration 1687, loss = 0.15439516\n",
      "Iteration 1688, loss = 0.15432823\n",
      "Iteration 1689, loss = 0.15426133\n",
      "Iteration 1690, loss = 0.15419444\n",
      "Iteration 1691, loss = 0.15412757\n",
      "Iteration 1692, loss = 0.15406073\n",
      "Iteration 1693, loss = 0.15399390\n",
      "Iteration 1694, loss = 0.15392709\n",
      "Iteration 1695, loss = 0.15386030\n",
      "Iteration 1696, loss = 0.15379353\n",
      "Iteration 1697, loss = 0.15372678\n",
      "Iteration 1698, loss = 0.15366005\n",
      "Iteration 1699, loss = 0.15359334\n",
      "Iteration 1700, loss = 0.15352665\n",
      "Iteration 1701, loss = 0.15345998\n",
      "Iteration 1702, loss = 0.15339333\n",
      "Iteration 1703, loss = 0.15332670\n",
      "Iteration 1704, loss = 0.15326009\n",
      "Iteration 1705, loss = 0.15319350\n",
      "Iteration 1706, loss = 0.15312693\n",
      "Iteration 1707, loss = 0.15306038\n",
      "Iteration 1708, loss = 0.15299385\n",
      "Iteration 1709, loss = 0.15292734\n",
      "Iteration 1710, loss = 0.15286085\n",
      "Iteration 1711, loss = 0.15279438\n",
      "Iteration 1712, loss = 0.15272793\n",
      "Iteration 1713, loss = 0.15266150\n",
      "Iteration 1714, loss = 0.15259509\n",
      "Iteration 1715, loss = 0.15252871\n",
      "Iteration 1716, loss = 0.15246234\n",
      "Iteration 1717, loss = 0.15239599\n",
      "Iteration 1718, loss = 0.15232967\n",
      "Iteration 1719, loss = 0.15226336\n",
      "Iteration 1720, loss = 0.15219708\n",
      "Iteration 1721, loss = 0.15213081\n",
      "Iteration 1722, loss = 0.15206457\n",
      "Iteration 1723, loss = 0.15199835\n",
      "Iteration 1724, loss = 0.15193215\n",
      "Iteration 1725, loss = 0.15186597\n",
      "Iteration 1726, loss = 0.15179981\n",
      "Iteration 1727, loss = 0.15173368\n",
      "Iteration 1728, loss = 0.15166756\n",
      "Iteration 1729, loss = 0.15160146\n",
      "Iteration 1730, loss = 0.15153539\n",
      "Iteration 1731, loss = 0.15146934\n",
      "Iteration 1732, loss = 0.15140331\n",
      "Iteration 1733, loss = 0.15133730\n",
      "Iteration 1734, loss = 0.15127131\n",
      "Iteration 1735, loss = 0.15120534\n",
      "Iteration 1736, loss = 0.15113940\n",
      "Iteration 1737, loss = 0.15107348\n",
      "Iteration 1738, loss = 0.15100757\n",
      "Iteration 1739, loss = 0.15094169\n",
      "Iteration 1740, loss = 0.15087584\n",
      "Iteration 1741, loss = 0.15081000\n",
      "Iteration 1742, loss = 0.15074418\n",
      "Iteration 1743, loss = 0.15067839\n",
      "Iteration 1744, loss = 0.15061262\n",
      "Iteration 1745, loss = 0.15054687\n",
      "Iteration 1746, loss = 0.15048114\n",
      "Iteration 1747, loss = 0.15041544\n",
      "Iteration 1748, loss = 0.15034976\n",
      "Iteration 1749, loss = 0.15028410\n",
      "Iteration 1750, loss = 0.15021846\n",
      "Iteration 1751, loss = 0.15015284\n",
      "Iteration 1752, loss = 0.15008725\n",
      "Iteration 1753, loss = 0.15002168\n",
      "Iteration 1754, loss = 0.14995613\n",
      "Iteration 1755, loss = 0.14989060\n",
      "Iteration 1756, loss = 0.14982510\n",
      "Iteration 1757, loss = 0.14975962\n",
      "Iteration 1758, loss = 0.14969416\n",
      "Iteration 1759, loss = 0.14962872\n",
      "Iteration 1760, loss = 0.14956331\n",
      "Iteration 1761, loss = 0.14949792\n",
      "Iteration 1762, loss = 0.14943255\n",
      "Iteration 1763, loss = 0.14936721\n",
      "Iteration 1764, loss = 0.14930188\n",
      "Iteration 1765, loss = 0.14923659\n",
      "Iteration 1766, loss = 0.14917131\n",
      "Iteration 1767, loss = 0.14910606\n",
      "Iteration 1768, loss = 0.14904083\n",
      "Iteration 1769, loss = 0.14897562\n",
      "Iteration 1770, loss = 0.14891044\n",
      "Iteration 1771, loss = 0.14884528\n",
      "Iteration 1772, loss = 0.14878014\n",
      "Iteration 1773, loss = 0.14871503\n",
      "Iteration 1774, loss = 0.14864994\n",
      "Iteration 1775, loss = 0.14858487\n",
      "Iteration 1776, loss = 0.14851983\n",
      "Iteration 1777, loss = 0.14845481\n",
      "Iteration 1778, loss = 0.14838981\n",
      "Iteration 1779, loss = 0.14832484\n",
      "Iteration 1780, loss = 0.14825989\n",
      "Iteration 1781, loss = 0.14819496\n",
      "Iteration 1782, loss = 0.14813006\n",
      "Iteration 1783, loss = 0.14806518\n",
      "Iteration 1784, loss = 0.14800033\n",
      "Iteration 1785, loss = 0.14793550\n",
      "Iteration 1786, loss = 0.14787069\n",
      "Iteration 1787, loss = 0.14780591\n",
      "Iteration 1788, loss = 0.14774115\n",
      "Iteration 1789, loss = 0.14767642\n",
      "Iteration 1790, loss = 0.14761171\n",
      "Iteration 1791, loss = 0.14754702\n",
      "Iteration 1792, loss = 0.14748236\n",
      "Iteration 1793, loss = 0.14741773\n",
      "Iteration 1794, loss = 0.14735311\n",
      "Iteration 1795, loss = 0.14728852\n",
      "Iteration 1796, loss = 0.14722396\n",
      "Iteration 1797, loss = 0.14715942\n",
      "Iteration 1798, loss = 0.14709491\n",
      "Iteration 1799, loss = 0.14703042\n",
      "Iteration 1800, loss = 0.14696595\n",
      "Iteration 1801, loss = 0.14690151\n",
      "Iteration 1802, loss = 0.14683709\n",
      "Iteration 1803, loss = 0.14677270\n",
      "Iteration 1804, loss = 0.14670833\n",
      "Iteration 1805, loss = 0.14664399\n",
      "Iteration 1806, loss = 0.14657968\n",
      "Iteration 1807, loss = 0.14651538\n",
      "Iteration 1808, loss = 0.14645112\n",
      "Iteration 1809, loss = 0.14638687\n",
      "Iteration 1810, loss = 0.14632266\n",
      "Iteration 1811, loss = 0.14625847\n",
      "Iteration 1812, loss = 0.14619430\n",
      "Iteration 1813, loss = 0.14613016\n",
      "Iteration 1814, loss = 0.14606604\n",
      "Iteration 1815, loss = 0.14600195\n",
      "Iteration 1816, loss = 0.14593788\n",
      "Iteration 1817, loss = 0.14587384\n",
      "Iteration 1818, loss = 0.14580983\n",
      "Iteration 1819, loss = 0.14574584\n",
      "Iteration 1820, loss = 0.14568188\n",
      "Iteration 1821, loss = 0.14561794\n",
      "Iteration 1822, loss = 0.14555403\n",
      "Iteration 1823, loss = 0.14549014\n",
      "Iteration 1824, loss = 0.14542628\n",
      "Iteration 1825, loss = 0.14536244\n",
      "Iteration 1826, loss = 0.14529863\n",
      "Iteration 1827, loss = 0.14523485\n",
      "Iteration 1828, loss = 0.14517109\n",
      "Iteration 1829, loss = 0.14510736\n",
      "Iteration 1830, loss = 0.14504366\n",
      "Iteration 1831, loss = 0.14497998\n",
      "Iteration 1832, loss = 0.14491632\n",
      "Iteration 1833, loss = 0.14485270\n",
      "Iteration 1834, loss = 0.14478909\n",
      "Iteration 1835, loss = 0.14472552\n",
      "Iteration 1836, loss = 0.14466197\n",
      "Iteration 1837, loss = 0.14459845\n",
      "Iteration 1838, loss = 0.14453495\n",
      "Iteration 1839, loss = 0.14447148\n",
      "Iteration 1840, loss = 0.14440804\n",
      "Iteration 1841, loss = 0.14434463\n",
      "Iteration 1842, loss = 0.14428124\n",
      "Iteration 1843, loss = 0.14421787\n",
      "Iteration 1844, loss = 0.14415454\n",
      "Iteration 1845, loss = 0.14409123\n",
      "Iteration 1846, loss = 0.14402794\n",
      "Iteration 1847, loss = 0.14396469\n",
      "Iteration 1848, loss = 0.14390146\n",
      "Iteration 1849, loss = 0.14383825\n",
      "Iteration 1850, loss = 0.14377508\n",
      "Iteration 1851, loss = 0.14371193\n",
      "Iteration 1852, loss = 0.14364881\n",
      "Iteration 1853, loss = 0.14358571\n",
      "Iteration 1854, loss = 0.14352265\n",
      "Iteration 1855, loss = 0.14345961\n",
      "Iteration 1856, loss = 0.14339659\n",
      "Iteration 1857, loss = 0.14333361\n",
      "Iteration 1858, loss = 0.14327065\n",
      "Iteration 1859, loss = 0.14320772\n",
      "Iteration 1860, loss = 0.14314481\n",
      "Iteration 1861, loss = 0.14308194\n",
      "Iteration 1862, loss = 0.14301909\n",
      "Iteration 1863, loss = 0.14295627\n",
      "Iteration 1864, loss = 0.14289347\n",
      "Iteration 1865, loss = 0.14283071\n",
      "Iteration 1866, loss = 0.14276797\n",
      "Iteration 1867, loss = 0.14270526\n",
      "Iteration 1868, loss = 0.14264258\n",
      "Iteration 1869, loss = 0.14257992\n",
      "Iteration 1870, loss = 0.14251729\n",
      "Iteration 1871, loss = 0.14245469\n",
      "Iteration 1872, loss = 0.14239212\n",
      "Iteration 1873, loss = 0.14232958\n",
      "Iteration 1874, loss = 0.14226706\n",
      "Iteration 1875, loss = 0.14220458\n",
      "Iteration 1876, loss = 0.14214212\n",
      "Iteration 1877, loss = 0.14207969\n",
      "Iteration 1878, loss = 0.14201728\n",
      "Iteration 1879, loss = 0.14195491\n",
      "Iteration 1880, loss = 0.14189256\n",
      "Iteration 1881, loss = 0.14183024\n",
      "Iteration 1882, loss = 0.14176795\n",
      "Iteration 1883, loss = 0.14170569\n",
      "Iteration 1884, loss = 0.14164346\n",
      "Iteration 1885, loss = 0.14158126\n",
      "Iteration 1886, loss = 0.14151908\n",
      "Iteration 1887, loss = 0.14145693\n",
      "Iteration 1888, loss = 0.14139481\n",
      "Iteration 1889, loss = 0.14133272\n",
      "Iteration 1890, loss = 0.14127066\n",
      "Iteration 1891, loss = 0.14120863\n",
      "Iteration 1892, loss = 0.14114663\n",
      "Iteration 1893, loss = 0.14108465\n",
      "Iteration 1894, loss = 0.14102271\n",
      "Iteration 1895, loss = 0.14096079\n",
      "Iteration 1896, loss = 0.14089890\n",
      "Iteration 1897, loss = 0.14083704\n",
      "Iteration 1898, loss = 0.14077521\n",
      "Iteration 1899, loss = 0.14071341\n",
      "Iteration 1900, loss = 0.14065164\n",
      "Iteration 1901, loss = 0.14058990\n",
      "Iteration 1902, loss = 0.14052819\n",
      "Iteration 1903, loss = 0.14046650\n",
      "Iteration 1904, loss = 0.14040485\n",
      "Iteration 1905, loss = 0.14034323\n",
      "Iteration 1906, loss = 0.14028163\n",
      "Iteration 1907, loss = 0.14022006\n",
      "Iteration 1908, loss = 0.14015853\n",
      "Iteration 1909, loss = 0.14009702\n",
      "Iteration 1910, loss = 0.14003554\n",
      "Iteration 1911, loss = 0.13997410\n",
      "Iteration 1912, loss = 0.13991268\n",
      "Iteration 1913, loss = 0.13985129\n",
      "Iteration 1914, loss = 0.13978993\n",
      "Iteration 1915, loss = 0.13972860\n",
      "Iteration 1916, loss = 0.13966731\n",
      "Iteration 1917, loss = 0.13960604\n",
      "Iteration 1918, loss = 0.13954480\n",
      "Iteration 1919, loss = 0.13948359\n",
      "Iteration 1920, loss = 0.13942241\n",
      "Iteration 1921, loss = 0.13936126\n",
      "Iteration 1922, loss = 0.13930015\n",
      "Iteration 1923, loss = 0.13923906\n",
      "Iteration 1924, loss = 0.13917800\n",
      "Iteration 1925, loss = 0.13911697\n",
      "Iteration 1926, loss = 0.13905597\n",
      "Iteration 1927, loss = 0.13899501\n",
      "Iteration 1928, loss = 0.13893407\n",
      "Iteration 1929, loss = 0.13887317\n",
      "Iteration 1930, loss = 0.13881229\n",
      "Iteration 1931, loss = 0.13875144\n",
      "Iteration 1932, loss = 0.13869063\n",
      "Iteration 1933, loss = 0.13862985\n",
      "Iteration 1934, loss = 0.13856909\n",
      "Iteration 1935, loss = 0.13850837\n",
      "Iteration 1936, loss = 0.13844768\n",
      "Iteration 1937, loss = 0.13838702\n",
      "Iteration 1938, loss = 0.13832639\n",
      "Iteration 1939, loss = 0.13826579\n",
      "Iteration 1940, loss = 0.13820522\n",
      "Iteration 1941, loss = 0.13814468\n",
      "Iteration 1942, loss = 0.13808418\n",
      "Iteration 1943, loss = 0.13802370\n",
      "Iteration 1944, loss = 0.13796326\n",
      "Iteration 1945, loss = 0.13790284\n",
      "Iteration 1946, loss = 0.13784246\n",
      "Iteration 1947, loss = 0.13778211\n",
      "Iteration 1948, loss = 0.13772179\n",
      "Iteration 1949, loss = 0.13766150\n",
      "Iteration 1950, loss = 0.13760125\n",
      "Iteration 1951, loss = 0.13754102\n",
      "Iteration 1952, loss = 0.13748083\n",
      "Iteration 1953, loss = 0.13742066\n",
      "Iteration 1954, loss = 0.13736053\n",
      "Iteration 1955, loss = 0.13730043\n",
      "Iteration 1956, loss = 0.13724036\n",
      "Iteration 1957, loss = 0.13718033\n",
      "Iteration 1958, loss = 0.13712032\n",
      "Iteration 1959, loss = 0.13706035\n",
      "Iteration 1960, loss = 0.13700041\n",
      "Iteration 1961, loss = 0.13694050\n",
      "Iteration 1962, loss = 0.13688062\n",
      "Iteration 1963, loss = 0.13682078\n",
      "Iteration 1964, loss = 0.13676096\n",
      "Iteration 1965, loss = 0.13670118\n",
      "Iteration 1966, loss = 0.13664143\n",
      "Iteration 1967, loss = 0.13658171\n",
      "Iteration 1968, loss = 0.13652203\n",
      "Iteration 1969, loss = 0.13646237\n",
      "Iteration 1970, loss = 0.13640275\n",
      "Iteration 1971, loss = 0.13634316\n",
      "Iteration 1972, loss = 0.13628360\n",
      "Iteration 1973, loss = 0.13622408\n",
      "Iteration 1974, loss = 0.13616459\n",
      "Iteration 1975, loss = 0.13610513\n",
      "Iteration 1976, loss = 0.13604570\n",
      "Iteration 1977, loss = 0.13598630\n",
      "Iteration 1978, loss = 0.13592694\n",
      "Iteration 1979, loss = 0.13586761\n",
      "Iteration 1980, loss = 0.13580831\n",
      "Iteration 1981, loss = 0.13574905\n",
      "Iteration 1982, loss = 0.13568982\n",
      "Iteration 1983, loss = 0.13563062\n",
      "Iteration 1984, loss = 0.13557145\n",
      "Iteration 1985, loss = 0.13551232\n",
      "Iteration 1986, loss = 0.13545322\n",
      "Iteration 1987, loss = 0.13539415\n",
      "Iteration 1988, loss = 0.13533511\n",
      "Iteration 1989, loss = 0.13527611\n",
      "Iteration 1990, loss = 0.13521714\n",
      "Iteration 1991, loss = 0.13515820\n",
      "Iteration 1992, loss = 0.13509930\n",
      "Iteration 1993, loss = 0.13504043\n",
      "Iteration 1994, loss = 0.13498159\n",
      "Iteration 1995, loss = 0.13492279\n",
      "Iteration 1996, loss = 0.13486402\n",
      "Iteration 1997, loss = 0.13480528\n",
      "Iteration 1998, loss = 0.13474658\n",
      "Iteration 1999, loss = 0.13468791\n",
      "Iteration 2000, loss = 0.13462927\n",
      "Iteration 2001, loss = 0.13457066\n",
      "Iteration 2002, loss = 0.13451209\n",
      "Iteration 2003, loss = 0.13445356\n",
      "Iteration 2004, loss = 0.13439505\n",
      "Iteration 2005, loss = 0.13433658\n",
      "Iteration 2006, loss = 0.13427815\n",
      "Iteration 2007, loss = 0.13421974\n",
      "Iteration 2008, loss = 0.13416137\n",
      "Iteration 2009, loss = 0.13410304\n",
      "Iteration 2010, loss = 0.13404474\n",
      "Iteration 2011, loss = 0.13398647\n",
      "Iteration 2012, loss = 0.13392824\n",
      "Iteration 2013, loss = 0.13387004\n",
      "Iteration 2014, loss = 0.13381187\n",
      "Iteration 2015, loss = 0.13375374\n",
      "Iteration 2016, loss = 0.13369564\n",
      "Iteration 2017, loss = 0.13363758\n",
      "Iteration 2018, loss = 0.13357955\n",
      "Iteration 2019, loss = 0.13352155\n",
      "Iteration 2020, loss = 0.13346359\n",
      "Iteration 2021, loss = 0.13340566\n",
      "Iteration 2022, loss = 0.13334777\n",
      "Iteration 2023, loss = 0.13328991\n",
      "Iteration 2024, loss = 0.13323209\n",
      "Iteration 2025, loss = 0.13317430\n",
      "Iteration 2026, loss = 0.13311654\n",
      "Iteration 2027, loss = 0.13305882\n",
      "Iteration 2028, loss = 0.13300113\n",
      "Iteration 2029, loss = 0.13294348\n",
      "Iteration 2030, loss = 0.13288586\n",
      "Iteration 2031, loss = 0.13282828\n",
      "Iteration 2032, loss = 0.13277073\n",
      "Iteration 2033, loss = 0.13271322\n",
      "Iteration 2034, loss = 0.13265574\n",
      "Iteration 2035, loss = 0.13259830\n",
      "Iteration 2036, loss = 0.13254089\n",
      "Iteration 2037, loss = 0.13248351\n",
      "Iteration 2038, loss = 0.13242617\n",
      "Iteration 2039, loss = 0.13236887\n",
      "Iteration 2040, loss = 0.13231160\n",
      "Iteration 2041, loss = 0.13225436\n",
      "Iteration 2042, loss = 0.13219716\n",
      "Iteration 2043, loss = 0.13214000\n",
      "Iteration 2044, loss = 0.13208287\n",
      "Iteration 2045, loss = 0.13202578\n",
      "Iteration 2046, loss = 0.13196872\n",
      "Iteration 2047, loss = 0.13191169\n",
      "Iteration 2048, loss = 0.13185470\n",
      "Iteration 2049, loss = 0.13179775\n",
      "Iteration 2050, loss = 0.13174083\n",
      "Iteration 2051, loss = 0.13168395\n",
      "Iteration 2052, loss = 0.13162710\n",
      "Iteration 2053, loss = 0.13157029\n",
      "Iteration 2054, loss = 0.13151351\n",
      "Iteration 2055, loss = 0.13145677\n",
      "Iteration 2056, loss = 0.13140007\n",
      "Iteration 2057, loss = 0.13134340\n",
      "Iteration 2058, loss = 0.13128676\n",
      "Iteration 2059, loss = 0.13123016\n",
      "Iteration 2060, loss = 0.13117360\n",
      "Iteration 2061, loss = 0.13111707\n",
      "Iteration 2062, loss = 0.13106058\n",
      "Iteration 2063, loss = 0.13100413\n",
      "Iteration 2064, loss = 0.13094771\n",
      "Iteration 2065, loss = 0.13089132\n",
      "Iteration 2066, loss = 0.13083497\n",
      "Iteration 2067, loss = 0.13077866\n",
      "Iteration 2068, loss = 0.13072239\n",
      "Iteration 2069, loss = 0.13066615\n",
      "Iteration 2070, loss = 0.13060994\n",
      "Iteration 2071, loss = 0.13055377\n",
      "Iteration 2072, loss = 0.13049764\n",
      "Iteration 2073, loss = 0.13044155\n",
      "Iteration 2074, loss = 0.13038549\n",
      "Iteration 2075, loss = 0.13032946\n",
      "Iteration 2076, loss = 0.13027348\n",
      "Iteration 2077, loss = 0.13021753\n",
      "Iteration 2078, loss = 0.13016161\n",
      "Iteration 2079, loss = 0.13010573\n",
      "Iteration 2080, loss = 0.13004989\n",
      "Iteration 2081, loss = 0.12999409\n",
      "Iteration 2082, loss = 0.12993832\n",
      "Iteration 2083, loss = 0.12988259\n",
      "Iteration 2084, loss = 0.12982689\n",
      "Iteration 2085, loss = 0.12977123\n",
      "Iteration 2086, loss = 0.12971561\n",
      "Iteration 2087, loss = 0.12966003\n",
      "Iteration 2088, loss = 0.12960448\n",
      "Iteration 2089, loss = 0.12954897\n",
      "Iteration 2090, loss = 0.12949349\n",
      "Iteration 2091, loss = 0.12943805\n",
      "Iteration 2092, loss = 0.12938265\n",
      "Iteration 2093, loss = 0.12932729\n",
      "Iteration 2094, loss = 0.12927196\n",
      "Iteration 2095, loss = 0.12921667\n",
      "Iteration 2096, loss = 0.12916141\n",
      "Iteration 2097, loss = 0.12910620\n",
      "Iteration 2098, loss = 0.12905102\n",
      "Iteration 2099, loss = 0.12899587\n",
      "Iteration 2100, loss = 0.12894077\n",
      "Iteration 2101, loss = 0.12888570\n",
      "Iteration 2102, loss = 0.12883067\n",
      "Iteration 2103, loss = 0.12877568\n",
      "Iteration 2104, loss = 0.12872072\n",
      "Iteration 2105, loss = 0.12866580\n",
      "Iteration 2106, loss = 0.12861092\n",
      "Iteration 2107, loss = 0.12855607\n",
      "Iteration 2108, loss = 0.12850127\n",
      "Iteration 2109, loss = 0.12844650\n",
      "Iteration 2110, loss = 0.12839176\n",
      "Iteration 2111, loss = 0.12833707\n",
      "Iteration 2112, loss = 0.12828241\n",
      "Iteration 2113, loss = 0.12822779\n",
      "Iteration 2114, loss = 0.12817321\n",
      "Iteration 2115, loss = 0.12811867\n",
      "Iteration 2116, loss = 0.12806416\n",
      "Iteration 2117, loss = 0.12800969\n",
      "Iteration 2118, loss = 0.12795526\n",
      "Iteration 2119, loss = 0.12790086\n",
      "Iteration 2120, loss = 0.12784651\n",
      "Iteration 2121, loss = 0.12779219\n",
      "Iteration 2122, loss = 0.12773791\n",
      "Iteration 2123, loss = 0.12768367\n",
      "Iteration 2124, loss = 0.12762946\n",
      "Iteration 2125, loss = 0.12757530\n",
      "Iteration 2126, loss = 0.12752117\n",
      "Iteration 2127, loss = 0.12746708\n",
      "Iteration 2128, loss = 0.12741303\n",
      "Iteration 2129, loss = 0.12735901\n",
      "Iteration 2130, loss = 0.12730504\n",
      "Iteration 2131, loss = 0.12725110\n",
      "Iteration 2132, loss = 0.12719720\n",
      "Iteration 2133, loss = 0.12714334\n",
      "Iteration 2134, loss = 0.12708952\n",
      "Iteration 2135, loss = 0.12703573\n",
      "Iteration 2136, loss = 0.12698199\n",
      "Iteration 2137, loss = 0.12692828\n",
      "Iteration 2138, loss = 0.12687461\n",
      "Iteration 2139, loss = 0.12682098\n",
      "Iteration 2140, loss = 0.12676739\n",
      "Iteration 2141, loss = 0.12671383\n",
      "Iteration 2142, loss = 0.12666032\n",
      "Iteration 2143, loss = 0.12660684\n",
      "Iteration 2144, loss = 0.12655340\n",
      "Iteration 2145, loss = 0.12650000\n",
      "Iteration 2146, loss = 0.12644664\n",
      "Iteration 2147, loss = 0.12639332\n",
      "Iteration 2148, loss = 0.12634003\n",
      "Iteration 2149, loss = 0.12628679\n",
      "Iteration 2150, loss = 0.12623358\n",
      "Iteration 2151, loss = 0.12618042\n",
      "Iteration 2152, loss = 0.12612729\n",
      "Iteration 2153, loss = 0.12607420\n",
      "Iteration 2154, loss = 0.12602115\n",
      "Iteration 2155, loss = 0.12596814\n",
      "Iteration 2156, loss = 0.12591516\n",
      "Iteration 2157, loss = 0.12586223\n",
      "Iteration 2158, loss = 0.12580933\n",
      "Iteration 2159, loss = 0.12575648\n",
      "Iteration 2160, loss = 0.12570366\n",
      "Iteration 2161, loss = 0.12565089\n",
      "Iteration 2162, loss = 0.12559815\n",
      "Iteration 2163, loss = 0.12554545\n",
      "Iteration 2164, loss = 0.12549279\n",
      "Iteration 2165, loss = 0.12544017\n",
      "Iteration 2166, loss = 0.12538759\n",
      "Iteration 2167, loss = 0.12533504\n",
      "Iteration 2168, loss = 0.12528254\n",
      "Iteration 2169, loss = 0.12523008\n",
      "Iteration 2170, loss = 0.12517766\n",
      "Iteration 2171, loss = 0.12512527\n",
      "Iteration 2172, loss = 0.12507293\n",
      "Iteration 2173, loss = 0.12502062\n",
      "Iteration 2174, loss = 0.12496835\n",
      "Iteration 2175, loss = 0.12491613\n",
      "Iteration 2176, loss = 0.12486394\n",
      "Iteration 2177, loss = 0.12481179\n",
      "Iteration 2178, loss = 0.12475969\n",
      "Iteration 2179, loss = 0.12470762\n",
      "Iteration 2180, loss = 0.12465559\n",
      "Iteration 2181, loss = 0.12460360\n",
      "Iteration 2182, loss = 0.12455165\n",
      "Iteration 2183, loss = 0.12449975\n",
      "Iteration 2184, loss = 0.12444788\n",
      "Iteration 2185, loss = 0.12439605\n",
      "Iteration 2186, loss = 0.12434426\n",
      "Iteration 2187, loss = 0.12429251\n",
      "Iteration 2188, loss = 0.12424080\n",
      "Iteration 2189, loss = 0.12418913\n",
      "Iteration 2190, loss = 0.12413750\n",
      "Iteration 2191, loss = 0.12408591\n",
      "Iteration 2192, loss = 0.12403436\n",
      "Iteration 2193, loss = 0.12398285\n",
      "Iteration 2194, loss = 0.12393139\n",
      "Iteration 2195, loss = 0.12387996\n",
      "Iteration 2196, loss = 0.12382857\n",
      "Iteration 2197, loss = 0.12377722\n",
      "Iteration 2198, loss = 0.12372591\n",
      "Iteration 2199, loss = 0.12367464\n",
      "Iteration 2200, loss = 0.12362342\n",
      "Iteration 2201, loss = 0.12357223\n",
      "Iteration 2202, loss = 0.12352108\n",
      "Iteration 2203, loss = 0.12346998\n",
      "Iteration 2204, loss = 0.12341891\n",
      "Iteration 2205, loss = 0.12336788\n",
      "Iteration 2206, loss = 0.12331690\n",
      "Iteration 2207, loss = 0.12326595\n",
      "Iteration 2208, loss = 0.12321505\n",
      "Iteration 2209, loss = 0.12316419\n",
      "Iteration 2210, loss = 0.12311336\n",
      "Iteration 2211, loss = 0.12306258\n",
      "Iteration 2212, loss = 0.12301184\n",
      "Iteration 2213, loss = 0.12296114\n",
      "Iteration 2214, loss = 0.12291048\n",
      "Iteration 2215, loss = 0.12285986\n",
      "Iteration 2216, loss = 0.12280928\n",
      "Iteration 2217, loss = 0.12275874\n",
      "Iteration 2218, loss = 0.12270824\n",
      "Iteration 2219, loss = 0.12265778\n",
      "Iteration 2220, loss = 0.12260737\n",
      "Iteration 2221, loss = 0.12255699\n",
      "Iteration 2222, loss = 0.12250666\n",
      "Iteration 2223, loss = 0.12245636\n",
      "Iteration 2224, loss = 0.12240611\n",
      "Iteration 2225, loss = 0.12235590\n",
      "Iteration 2226, loss = 0.12230573\n",
      "Iteration 2227, loss = 0.12225560\n",
      "Iteration 2228, loss = 0.12220551\n",
      "Iteration 2229, loss = 0.12215547\n",
      "Iteration 2230, loss = 0.12210546\n",
      "Iteration 2231, loss = 0.12205549\n",
      "Iteration 2232, loss = 0.12200557\n",
      "Iteration 2233, loss = 0.12195569\n",
      "Iteration 2234, loss = 0.12190585\n",
      "Iteration 2235, loss = 0.12185605\n",
      "Iteration 2236, loss = 0.12180629\n",
      "Iteration 2237, loss = 0.12175657\n",
      "Iteration 2238, loss = 0.12170689\n",
      "Iteration 2239, loss = 0.12165726\n",
      "Iteration 2240, loss = 0.12160767\n",
      "Iteration 2241, loss = 0.12155811\n",
      "Iteration 2242, loss = 0.12150860\n",
      "Iteration 2243, loss = 0.12145913\n",
      "Iteration 2244, loss = 0.12140971\n",
      "Iteration 2245, loss = 0.12136032\n",
      "Iteration 2246, loss = 0.12131097\n",
      "Iteration 2247, loss = 0.12126167\n",
      "Iteration 2248, loss = 0.12121241\n",
      "Iteration 2249, loss = 0.12116319\n",
      "Iteration 2250, loss = 0.12111401\n",
      "Iteration 2251, loss = 0.12106487\n",
      "Iteration 2252, loss = 0.12101578\n",
      "Iteration 2253, loss = 0.12096673\n",
      "Iteration 2254, loss = 0.12091771\n",
      "Iteration 2255, loss = 0.12086874\n",
      "Iteration 2256, loss = 0.12081982\n",
      "Iteration 2257, loss = 0.12077093\n",
      "Iteration 2258, loss = 0.12072208\n",
      "Iteration 2259, loss = 0.12067328\n",
      "Iteration 2260, loss = 0.12062452\n",
      "Iteration 2261, loss = 0.12057580\n",
      "Iteration 2262, loss = 0.12052712\n",
      "Iteration 2263, loss = 0.12047849\n",
      "Iteration 2264, loss = 0.12042990\n",
      "Iteration 2265, loss = 0.12038135\n",
      "Iteration 2266, loss = 0.12033284\n",
      "Iteration 2267, loss = 0.12028437\n",
      "Iteration 2268, loss = 0.12023594\n",
      "Iteration 2269, loss = 0.12018756\n",
      "Iteration 2270, loss = 0.12013922\n",
      "Iteration 2271, loss = 0.12009092\n",
      "Iteration 2272, loss = 0.12004266\n",
      "Iteration 2273, loss = 0.11999445\n",
      "Iteration 2274, loss = 0.11994628\n",
      "Iteration 2275, loss = 0.11989815\n",
      "Iteration 2276, loss = 0.11985006\n",
      "Iteration 2277, loss = 0.11980201\n",
      "Iteration 2278, loss = 0.11975401\n",
      "Iteration 2279, loss = 0.11970605\n",
      "Iteration 2280, loss = 0.11965813\n",
      "Iteration 2281, loss = 0.11961025\n",
      "Iteration 2282, loss = 0.11956242\n",
      "Iteration 2283, loss = 0.11951463\n",
      "Iteration 2284, loss = 0.11946688\n",
      "Iteration 2285, loss = 0.11941917\n",
      "Iteration 2286, loss = 0.11937151\n",
      "Iteration 2287, loss = 0.11932389\n",
      "Iteration 2288, loss = 0.11927631\n",
      "Iteration 2289, loss = 0.11922877\n",
      "Iteration 2290, loss = 0.11918127\n",
      "Iteration 2291, loss = 0.11913382\n",
      "Iteration 2292, loss = 0.11908641\n",
      "Iteration 2293, loss = 0.11903905\n",
      "Iteration 2294, loss = 0.11899172\n",
      "Iteration 2295, loss = 0.11894444\n",
      "Iteration 2296, loss = 0.11889720\n",
      "Iteration 2297, loss = 0.11885001\n",
      "Iteration 2298, loss = 0.11880286\n",
      "Iteration 2299, loss = 0.11875574\n",
      "Iteration 2300, loss = 0.11870868\n",
      "Iteration 2301, loss = 0.11866165\n",
      "Iteration 2302, loss = 0.11861467\n",
      "Iteration 2303, loss = 0.11856773\n",
      "Iteration 2304, loss = 0.11852083\n",
      "Iteration 2305, loss = 0.11847398\n",
      "Iteration 2306, loss = 0.11842717\n",
      "Iteration 2307, loss = 0.11838040\n",
      "Iteration 2308, loss = 0.11833368\n",
      "Iteration 2309, loss = 0.11828700\n",
      "Iteration 2310, loss = 0.11824036\n",
      "Iteration 2311, loss = 0.11819376\n",
      "Iteration 2312, loss = 0.11814721\n",
      "Iteration 2313, loss = 0.11810070\n",
      "Iteration 2314, loss = 0.11805423\n",
      "Iteration 2315, loss = 0.11800781\n",
      "Iteration 2316, loss = 0.11796143\n",
      "Iteration 2317, loss = 0.11791509\n",
      "Iteration 2318, loss = 0.11786880\n",
      "Iteration 2319, loss = 0.11782254\n",
      "Iteration 2320, loss = 0.11777634\n",
      "Iteration 2321, loss = 0.11773017\n",
      "Iteration 2322, loss = 0.11768405\n",
      "Iteration 2323, loss = 0.11763797\n",
      "Iteration 2324, loss = 0.11759194\n",
      "Iteration 2325, loss = 0.11754594\n",
      "Iteration 2326, loss = 0.11749999\n",
      "Iteration 2327, loss = 0.11745409\n",
      "Iteration 2328, loss = 0.11740823\n",
      "Iteration 2329, loss = 0.11736241\n",
      "Iteration 2330, loss = 0.11731663\n",
      "Iteration 2331, loss = 0.11727090\n",
      "Iteration 2332, loss = 0.11722521\n",
      "Iteration 2333, loss = 0.11717957\n",
      "Iteration 2334, loss = 0.11713396\n",
      "Iteration 2335, loss = 0.11708841\n",
      "Iteration 2336, loss = 0.11704289\n",
      "Iteration 2337, loss = 0.11699742\n",
      "Iteration 2338, loss = 0.11695199\n",
      "Iteration 2339, loss = 0.11690661\n",
      "Iteration 2340, loss = 0.11686127\n",
      "Iteration 2341, loss = 0.11681597\n",
      "Iteration 2342, loss = 0.11677071\n",
      "Iteration 2343, loss = 0.11672550\n",
      "Iteration 2344, loss = 0.11668034\n",
      "Iteration 2345, loss = 0.11663521\n",
      "Iteration 2346, loss = 0.11659013\n",
      "Iteration 2347, loss = 0.11654510\n",
      "Iteration 2348, loss = 0.11650011\n",
      "Iteration 2349, loss = 0.11645516\n",
      "Iteration 2350, loss = 0.11641025\n",
      "Iteration 2351, loss = 0.11636539\n",
      "Iteration 2352, loss = 0.11632057\n",
      "Iteration 2353, loss = 0.11627580\n",
      "Iteration 2354, loss = 0.11623107\n",
      "Iteration 2355, loss = 0.11618639\n",
      "Iteration 2356, loss = 0.11614174\n",
      "Iteration 2357, loss = 0.11609714\n",
      "Iteration 2358, loss = 0.11605259\n",
      "Iteration 2359, loss = 0.11600808\n",
      "Iteration 2360, loss = 0.11596361\n",
      "Iteration 2361, loss = 0.11591919\n",
      "Iteration 2362, loss = 0.11587481\n",
      "Iteration 2363, loss = 0.11583048\n",
      "Iteration 2364, loss = 0.11578618\n",
      "Iteration 2365, loss = 0.11574194\n",
      "Iteration 2366, loss = 0.11569773\n",
      "Iteration 2367, loss = 0.11565357\n",
      "Iteration 2368, loss = 0.11560946\n",
      "Iteration 2369, loss = 0.11556539\n",
      "Iteration 2370, loss = 0.11552136\n",
      "Iteration 2371, loss = 0.11547738\n",
      "Iteration 2372, loss = 0.11543344\n",
      "Iteration 2373, loss = 0.11538954\n",
      "Iteration 2374, loss = 0.11534569\n",
      "Iteration 2375, loss = 0.11530188\n",
      "Iteration 2376, loss = 0.11525812\n",
      "Iteration 2377, loss = 0.11521440\n",
      "Iteration 2378, loss = 0.11517073\n",
      "Iteration 2379, loss = 0.11512710\n",
      "Iteration 2380, loss = 0.11508351\n",
      "Iteration 2381, loss = 0.11503997\n",
      "Iteration 2382, loss = 0.11499647\n",
      "Iteration 2383, loss = 0.11495302\n",
      "Iteration 2384, loss = 0.11490961\n",
      "Iteration 2385, loss = 0.11486624\n",
      "Iteration 2386, loss = 0.11482292\n",
      "Iteration 2387, loss = 0.11477964\n",
      "Iteration 2388, loss = 0.11473641\n",
      "Iteration 2389, loss = 0.11469322\n",
      "Iteration 2390, loss = 0.11465008\n",
      "Iteration 2391, loss = 0.11460698\n",
      "Iteration 2392, loss = 0.11456392\n",
      "Iteration 2393, loss = 0.11452091\n",
      "Iteration 2394, loss = 0.11447795\n",
      "Iteration 2395, loss = 0.11443502\n",
      "Iteration 2396, loss = 0.11439215\n",
      "Iteration 2397, loss = 0.11434931\n",
      "Iteration 2398, loss = 0.11430652\n",
      "Iteration 2399, loss = 0.11426378\n",
      "Iteration 2400, loss = 0.11422108\n",
      "Iteration 2401, loss = 0.11417842\n",
      "Iteration 2402, loss = 0.11413581\n",
      "Iteration 2403, loss = 0.11409324\n",
      "Iteration 2404, loss = 0.11405072\n",
      "Iteration 2405, loss = 0.11400824\n",
      "Iteration 2406, loss = 0.11396581\n",
      "Iteration 2407, loss = 0.11392342\n",
      "Iteration 2408, loss = 0.11388108\n",
      "Iteration 2409, loss = 0.11383878\n",
      "Iteration 2410, loss = 0.11379652\n",
      "Iteration 2411, loss = 0.11375431\n",
      "Iteration 2412, loss = 0.11371215\n",
      "Iteration 2413, loss = 0.11367002\n",
      "Iteration 2414, loss = 0.11362795\n",
      "Iteration 2415, loss = 0.11358591\n",
      "Iteration 2416, loss = 0.11354393\n",
      "Iteration 2417, loss = 0.11350198\n",
      "Iteration 2418, loss = 0.11346008\n",
      "Iteration 2419, loss = 0.11341823\n",
      "Iteration 2420, loss = 0.11337642\n",
      "Iteration 2421, loss = 0.11333466\n",
      "Iteration 2422, loss = 0.11329294\n",
      "Iteration 2423, loss = 0.11325126\n",
      "Iteration 2424, loss = 0.11320963\n",
      "Iteration 2425, loss = 0.11316805\n",
      "Iteration 2426, loss = 0.11312650\n",
      "Iteration 2427, loss = 0.11308501\n",
      "Iteration 2428, loss = 0.11304356\n",
      "Iteration 2429, loss = 0.11300215\n",
      "Iteration 2430, loss = 0.11296079\n",
      "Iteration 2431, loss = 0.11291947\n",
      "Iteration 2432, loss = 0.11287820\n",
      "Iteration 2433, loss = 0.11283697\n",
      "Iteration 2434, loss = 0.11279579\n",
      "Iteration 2435, loss = 0.11275465\n",
      "Iteration 2436, loss = 0.11271356\n",
      "Iteration 2437, loss = 0.11267251\n",
      "Iteration 2438, loss = 0.11263150\n",
      "Iteration 2439, loss = 0.11259055\n",
      "Iteration 2440, loss = 0.11254963\n",
      "Iteration 2441, loss = 0.11250876\n",
      "Iteration 2442, loss = 0.11246794\n",
      "Iteration 2443, loss = 0.11242716\n",
      "Iteration 2444, loss = 0.11238642\n",
      "Iteration 2445, loss = 0.11234573\n",
      "Iteration 2446, loss = 0.11230509\n",
      "Iteration 2447, loss = 0.11226449\n",
      "Iteration 2448, loss = 0.11222394\n",
      "Iteration 2449, loss = 0.11218343\n",
      "Iteration 2450, loss = 0.11214296\n",
      "Iteration 2451, loss = 0.11210254\n",
      "Iteration 2452, loss = 0.11206217\n",
      "Iteration 2453, loss = 0.11202184\n",
      "Iteration 2454, loss = 0.11198155\n",
      "Iteration 2455, loss = 0.11194131\n",
      "Iteration 2456, loss = 0.11190111\n",
      "Iteration 2457, loss = 0.11186096\n",
      "Iteration 2458, loss = 0.11182086\n",
      "Iteration 2459, loss = 0.11178080\n",
      "Iteration 2460, loss = 0.11174078\n",
      "Iteration 2461, loss = 0.11170081\n",
      "Iteration 2462, loss = 0.11166089\n",
      "Iteration 2463, loss = 0.11162101\n",
      "Iteration 2464, loss = 0.11158117\n",
      "Iteration 2465, loss = 0.11154138\n",
      "Iteration 2466, loss = 0.11150164\n",
      "Iteration 2467, loss = 0.11146194\n",
      "Iteration 2468, loss = 0.11142228\n",
      "Iteration 2469, loss = 0.11138267\n",
      "Iteration 2470, loss = 0.11134311\n",
      "Iteration 2471, loss = 0.11130359\n",
      "Iteration 2472, loss = 0.11126411\n",
      "Iteration 2473, loss = 0.11122469\n",
      "Iteration 2474, loss = 0.11118530\n",
      "Iteration 2475, loss = 0.11114596\n",
      "Iteration 2476, loss = 0.11110667\n",
      "Iteration 2477, loss = 0.11106742\n",
      "Iteration 2478, loss = 0.11102821\n",
      "Iteration 2479, loss = 0.11098906\n",
      "Iteration 2480, loss = 0.11094994\n",
      "Iteration 2481, loss = 0.11091087\n",
      "Iteration 2482, loss = 0.11087185\n",
      "Iteration 2483, loss = 0.11083287\n",
      "Iteration 2484, loss = 0.11079394\n",
      "Iteration 2485, loss = 0.11075505\n",
      "Iteration 2486, loss = 0.11071621\n",
      "Iteration 2487, loss = 0.11067741\n",
      "Iteration 2488, loss = 0.11063866\n",
      "Iteration 2489, loss = 0.11059995\n",
      "Iteration 2490, loss = 0.11056129\n",
      "Iteration 2491, loss = 0.11052267\n",
      "Iteration 2492, loss = 0.11048410\n",
      "Iteration 2493, loss = 0.11044557\n",
      "Iteration 2494, loss = 0.11040709\n",
      "Iteration 2495, loss = 0.11036865\n",
      "Iteration 2496, loss = 0.11033026\n",
      "Iteration 2497, loss = 0.11029192\n",
      "Iteration 2498, loss = 0.11025362\n",
      "Iteration 2499, loss = 0.11021536\n",
      "Iteration 2500, loss = 0.11017715\n",
      "Iteration 2501, loss = 0.11013899\n",
      "Iteration 2502, loss = 0.11010087\n",
      "Iteration 2503, loss = 0.11006279\n",
      "Iteration 2504, loss = 0.11002476\n",
      "Iteration 2505, loss = 0.10998678\n",
      "Iteration 2506, loss = 0.10994884\n",
      "Iteration 2507, loss = 0.10991095\n",
      "Iteration 2508, loss = 0.10987310\n",
      "Iteration 2509, loss = 0.10983529\n",
      "Iteration 2510, loss = 0.10979754\n",
      "Iteration 2511, loss = 0.10975982\n",
      "Iteration 2512, loss = 0.10972216\n",
      "Iteration 2513, loss = 0.10968453\n",
      "Iteration 2514, loss = 0.10964696\n",
      "Iteration 2515, loss = 0.10960943\n",
      "Iteration 2516, loss = 0.10957194\n",
      "Iteration 2517, loss = 0.10953450\n",
      "Iteration 2518, loss = 0.10949710\n",
      "Iteration 2519, loss = 0.10945975\n",
      "Iteration 2520, loss = 0.10942245\n",
      "Iteration 2521, loss = 0.10938519\n",
      "Iteration 2522, loss = 0.10934797\n",
      "Iteration 2523, loss = 0.10931080\n",
      "Iteration 2524, loss = 0.10927368\n",
      "Iteration 2525, loss = 0.10923660\n",
      "Iteration 2526, loss = 0.10919956\n",
      "Iteration 2527, loss = 0.10916257\n",
      "Iteration 2528, loss = 0.10912563\n",
      "Iteration 2529, loss = 0.10908873\n",
      "Iteration 2530, loss = 0.10905188\n",
      "Iteration 2531, loss = 0.10901507\n",
      "Iteration 2532, loss = 0.10897831\n",
      "Iteration 2533, loss = 0.10894159\n",
      "Iteration 2534, loss = 0.10890492\n",
      "Iteration 2535, loss = 0.10886830\n",
      "Iteration 2536, loss = 0.10883171\n",
      "Iteration 2537, loss = 0.10879518\n",
      "Iteration 2538, loss = 0.10875869\n",
      "Iteration 2539, loss = 0.10872224\n",
      "Iteration 2540, loss = 0.10868584\n",
      "Iteration 2541, loss = 0.10864949\n",
      "Iteration 2542, loss = 0.10861318\n",
      "Iteration 2543, loss = 0.10857691\n",
      "Iteration 2544, loss = 0.10854069\n",
      "Iteration 2545, loss = 0.10850452\n",
      "Iteration 2546, loss = 0.10846839\n",
      "Iteration 2547, loss = 0.10843231\n",
      "Iteration 2548, loss = 0.10839627\n",
      "Iteration 2549, loss = 0.10836028\n",
      "Iteration 2550, loss = 0.10832433\n",
      "Iteration 2551, loss = 0.10828843\n",
      "Iteration 2552, loss = 0.10825257\n",
      "Iteration 2553, loss = 0.10821676\n",
      "Iteration 2554, loss = 0.10818099\n",
      "Iteration 2555, loss = 0.10814527\n",
      "Iteration 2556, loss = 0.10810959\n",
      "Iteration 2557, loss = 0.10807396\n",
      "Iteration 2558, loss = 0.10803838\n",
      "Iteration 2559, loss = 0.10800284\n",
      "Iteration 2560, loss = 0.10796734\n",
      "Iteration 2561, loss = 0.10793189\n",
      "Iteration 2562, loss = 0.10789649\n",
      "Iteration 2563, loss = 0.10786113\n",
      "Iteration 2564, loss = 0.10782581\n",
      "Iteration 2565, loss = 0.10779054\n",
      "Iteration 2566, loss = 0.10775532\n",
      "Iteration 2567, loss = 0.10772014\n",
      "Iteration 2568, loss = 0.10768501\n",
      "Iteration 2569, loss = 0.10764992\n",
      "Iteration 2570, loss = 0.10761487\n",
      "Iteration 2571, loss = 0.10757988\n",
      "Iteration 2572, loss = 0.10754492\n",
      "Iteration 2573, loss = 0.10751002\n",
      "Iteration 2574, loss = 0.10747515\n",
      "Iteration 2575, loss = 0.10744034\n",
      "Iteration 2576, loss = 0.10740556\n",
      "Iteration 2577, loss = 0.10737084\n",
      "Iteration 2578, loss = 0.10733616\n",
      "Iteration 2579, loss = 0.10730152\n",
      "Iteration 2580, loss = 0.10726693\n",
      "Iteration 2581, loss = 0.10723238\n",
      "Iteration 2582, loss = 0.10719788\n",
      "Iteration 2583, loss = 0.10716343\n",
      "Iteration 2584, loss = 0.10712901\n",
      "Iteration 2585, loss = 0.10709465\n",
      "Iteration 2586, loss = 0.10706033\n",
      "Iteration 2587, loss = 0.10702605\n",
      "Iteration 2588, loss = 0.10699182\n",
      "Iteration 2589, loss = 0.10695764\n",
      "Iteration 2590, loss = 0.10692350\n",
      "Iteration 2591, loss = 0.10688940\n",
      "Iteration 2592, loss = 0.10685535\n",
      "Iteration 2593, loss = 0.10682135\n",
      "Iteration 2594, loss = 0.10678739\n",
      "Iteration 2595, loss = 0.10675347\n",
      "Iteration 2596, loss = 0.10671960\n",
      "Iteration 2597, loss = 0.10668578\n",
      "Iteration 2598, loss = 0.10665200\n",
      "Iteration 2599, loss = 0.10661827\n",
      "Iteration 2600, loss = 0.10658458\n",
      "Iteration 2601, loss = 0.10655093\n",
      "Iteration 2602, loss = 0.10651733\n",
      "Iteration 2603, loss = 0.10648378\n",
      "Iteration 2604, loss = 0.10645027\n",
      "Iteration 2605, loss = 0.10641681\n",
      "Iteration 2606, loss = 0.10638339\n",
      "Iteration 2607, loss = 0.10635001\n",
      "Iteration 2608, loss = 0.10631668\n",
      "Iteration 2609, loss = 0.10628340\n",
      "Iteration 2610, loss = 0.10625016\n",
      "Iteration 2611, loss = 0.10621697\n",
      "Iteration 2612, loss = 0.10618382\n",
      "Iteration 2613, loss = 0.10615071\n",
      "Iteration 2614, loss = 0.10611765\n",
      "Iteration 2615, loss = 0.10608464\n",
      "Iteration 2616, loss = 0.10605167\n",
      "Iteration 2617, loss = 0.10601875\n",
      "Iteration 2618, loss = 0.10598587\n",
      "Iteration 2619, loss = 0.10595303\n",
      "Iteration 2620, loss = 0.10592024\n",
      "Iteration 2621, loss = 0.10588750\n",
      "Iteration 2622, loss = 0.10585480\n",
      "Iteration 2623, loss = 0.10582214\n",
      "Iteration 2624, loss = 0.10578953\n",
      "Iteration 2625, loss = 0.10575697\n",
      "Iteration 2626, loss = 0.10572445\n",
      "Iteration 2627, loss = 0.10569197\n",
      "Iteration 2628, loss = 0.10565954\n",
      "Iteration 2629, loss = 0.10562715\n",
      "Iteration 2630, loss = 0.10559481\n",
      "Iteration 2631, loss = 0.10556252\n",
      "Iteration 2632, loss = 0.10553027\n",
      "Iteration 2633, loss = 0.10549806\n",
      "Iteration 2634, loss = 0.10546590\n",
      "Iteration 2635, loss = 0.10543378\n",
      "Iteration 2636, loss = 0.10540171\n",
      "Iteration 2637, loss = 0.10536968\n",
      "Iteration 2638, loss = 0.10533770\n",
      "Iteration 2639, loss = 0.10530576\n",
      "Iteration 2640, loss = 0.10527386\n",
      "Iteration 2641, loss = 0.10524202\n",
      "Iteration 2642, loss = 0.10521021\n",
      "Iteration 2643, loss = 0.10517845\n",
      "Iteration 2644, loss = 0.10514674\n",
      "Iteration 2645, loss = 0.10511507\n",
      "Iteration 2646, loss = 0.10508344\n",
      "Iteration 2647, loss = 0.10505186\n",
      "Iteration 2648, loss = 0.10502032\n",
      "Iteration 2649, loss = 0.10498883\n",
      "Iteration 2650, loss = 0.10495739\n",
      "Iteration 2651, loss = 0.10492598\n",
      "Iteration 2652, loss = 0.10489463\n",
      "Iteration 2653, loss = 0.10486331\n",
      "Iteration 2654, loss = 0.10483204\n",
      "Iteration 2655, loss = 0.10480082\n",
      "Iteration 2656, loss = 0.10476964\n",
      "Iteration 2657, loss = 0.10473851\n",
      "Iteration 2658, loss = 0.10470742\n",
      "Iteration 2659, loss = 0.10467637\n",
      "Iteration 2660, loss = 0.10464537\n",
      "Iteration 2661, loss = 0.10461441\n",
      "Iteration 2662, loss = 0.10458350\n",
      "Iteration 2663, loss = 0.10455263\n",
      "Iteration 2664, loss = 0.10452181\n",
      "Iteration 2665, loss = 0.10449103\n",
      "Iteration 2666, loss = 0.10446029\n",
      "Iteration 2667, loss = 0.10442960\n",
      "Iteration 2668, loss = 0.10439896\n",
      "Iteration 2669, loss = 0.10436836\n",
      "Iteration 2670, loss = 0.10433780\n",
      "Iteration 2671, loss = 0.10430729\n",
      "Iteration 2672, loss = 0.10427682\n",
      "Iteration 2673, loss = 0.10424639\n",
      "Iteration 2674, loss = 0.10421601\n",
      "Iteration 2675, loss = 0.10418568\n",
      "Iteration 2676, loss = 0.10415539\n",
      "Iteration 2677, loss = 0.10412514\n",
      "Iteration 2678, loss = 0.10409494\n",
      "Iteration 2679, loss = 0.10406478\n",
      "Iteration 2680, loss = 0.10403466\n",
      "Iteration 2681, loss = 0.10400459\n",
      "Iteration 2682, loss = 0.10397457\n",
      "Iteration 2683, loss = 0.10394459\n",
      "Iteration 2684, loss = 0.10391465\n",
      "Iteration 2685, loss = 0.10388476\n",
      "Iteration 2686, loss = 0.10385491\n",
      "Iteration 2687, loss = 0.10382510\n",
      "Iteration 2688, loss = 0.10379534\n",
      "Iteration 2689, loss = 0.10376563\n",
      "Iteration 2690, loss = 0.10373595\n",
      "Iteration 2691, loss = 0.10370632\n",
      "Iteration 2692, loss = 0.10367674\n",
      "Iteration 2693, loss = 0.10364720\n",
      "Iteration 2694, loss = 0.10361770\n",
      "Iteration 2695, loss = 0.10358825\n",
      "Iteration 2696, loss = 0.10355884\n",
      "Iteration 2697, loss = 0.10352948\n",
      "Iteration 2698, loss = 0.10350016\n",
      "Iteration 2699, loss = 0.10347088\n",
      "Iteration 2700, loss = 0.10344165\n",
      "Iteration 2701, loss = 0.10341246\n",
      "Iteration 2702, loss = 0.10338331\n",
      "Iteration 2703, loss = 0.10335421\n",
      "Iteration 2704, loss = 0.10332516\n",
      "Iteration 2705, loss = 0.10329614\n",
      "Iteration 2706, loss = 0.10326717\n",
      "Iteration 2707, loss = 0.10323825\n",
      "Iteration 2708, loss = 0.10320937\n",
      "Iteration 2709, loss = 0.10318053\n",
      "Iteration 2710, loss = 0.10315174\n",
      "Iteration 2711, loss = 0.10312299\n",
      "Iteration 2712, loss = 0.10309428\n",
      "Iteration 2713, loss = 0.10306562\n",
      "Iteration 2714, loss = 0.10303700\n",
      "Iteration 2715, loss = 0.10300842\n",
      "Iteration 2716, loss = 0.10297989\n",
      "Iteration 2717, loss = 0.10295140\n",
      "Iteration 2718, loss = 0.10292296\n",
      "Iteration 2719, loss = 0.10289456\n",
      "Iteration 2720, loss = 0.10286620\n",
      "Iteration 2721, loss = 0.10283788\n",
      "Iteration 2722, loss = 0.10280961\n",
      "Iteration 2723, loss = 0.10278139\n",
      "Iteration 2724, loss = 0.10275320\n",
      "Iteration 2725, loss = 0.10272507\n",
      "Iteration 2726, loss = 0.10269697\n",
      "Iteration 2727, loss = 0.10266892\n",
      "Iteration 2728, loss = 0.10264091\n",
      "Iteration 2729, loss = 0.10261294\n",
      "Iteration 2730, loss = 0.10258502\n",
      "Iteration 2731, loss = 0.10255714\n",
      "Iteration 2732, loss = 0.10252931\n",
      "Iteration 2733, loss = 0.10250151\n",
      "Iteration 2734, loss = 0.10247376\n",
      "Iteration 2735, loss = 0.10244606\n",
      "Iteration 2736, loss = 0.10241840\n",
      "Iteration 2737, loss = 0.10239078\n",
      "Iteration 2738, loss = 0.10236320\n",
      "Iteration 2739, loss = 0.10233567\n",
      "Iteration 2740, loss = 0.10230818\n",
      "Iteration 2741, loss = 0.10228074\n",
      "Iteration 2742, loss = 0.10225333\n",
      "Iteration 2743, loss = 0.10222597\n",
      "Iteration 2744, loss = 0.10219866\n",
      "Iteration 2745, loss = 0.10217138\n",
      "Iteration 2746, loss = 0.10214415\n",
      "Iteration 2747, loss = 0.10211697\n",
      "Iteration 2748, loss = 0.10208982\n",
      "Iteration 2749, loss = 0.10206272\n",
      "Iteration 2750, loss = 0.10203566\n",
      "Iteration 2751, loss = 0.10200865\n",
      "Iteration 2752, loss = 0.10198168\n",
      "Iteration 2753, loss = 0.10195475\n",
      "Iteration 2754, loss = 0.10192786\n",
      "Iteration 2755, loss = 0.10190102\n",
      "Iteration 2756, loss = 0.10187422\n",
      "Iteration 2757, loss = 0.10184746\n",
      "Iteration 2758, loss = 0.10182075\n",
      "Iteration 2759, loss = 0.10179408\n",
      "Iteration 2760, loss = 0.10176745\n",
      "Iteration 2761, loss = 0.10174086\n",
      "Iteration 2762, loss = 0.10171432\n",
      "Iteration 2763, loss = 0.10168782\n",
      "Iteration 2764, loss = 0.10166136\n",
      "Iteration 2765, loss = 0.10163495\n",
      "Iteration 2766, loss = 0.10160857\n",
      "Iteration 2767, loss = 0.10158224\n",
      "Iteration 2768, loss = 0.10155596\n",
      "Iteration 2769, loss = 0.10152971\n",
      "Iteration 2770, loss = 0.10150351\n",
      "Iteration 2771, loss = 0.10147735\n",
      "Iteration 2772, loss = 0.10145124\n",
      "Iteration 2773, loss = 0.10142516\n",
      "Iteration 2774, loss = 0.10139913\n",
      "Iteration 2775, loss = 0.10137314\n",
      "Iteration 2776, loss = 0.10134720\n",
      "Iteration 2777, loss = 0.10132129\n",
      "Iteration 2778, loss = 0.10129543\n",
      "Iteration 2779, loss = 0.10126961\n",
      "Iteration 2780, loss = 0.10124383\n",
      "Iteration 2781, loss = 0.10121810\n",
      "Iteration 2782, loss = 0.10119241\n",
      "Iteration 2783, loss = 0.10116676\n",
      "Iteration 2784, loss = 0.10114115\n",
      "Iteration 2785, loss = 0.10111559\n",
      "Iteration 2786, loss = 0.10109006\n",
      "Iteration 2787, loss = 0.10106458\n",
      "Iteration 2788, loss = 0.10103914\n",
      "Iteration 2789, loss = 0.10101375\n",
      "Iteration 2790, loss = 0.10098839\n",
      "Iteration 2791, loss = 0.10096308\n",
      "Iteration 2792, loss = 0.10093781\n",
      "Iteration 2793, loss = 0.10091258\n",
      "Iteration 2794, loss = 0.10088740\n",
      "Iteration 2795, loss = 0.10086225\n",
      "Iteration 2796, loss = 0.10083715\n",
      "Iteration 2797, loss = 0.10081209\n",
      "Iteration 2798, loss = 0.10078707\n",
      "Iteration 2799, loss = 0.10076210\n",
      "Iteration 2800, loss = 0.10073716\n",
      "Iteration 2801, loss = 0.10071227\n",
      "Iteration 2802, loss = 0.10068742\n",
      "Iteration 2803, loss = 0.10066261\n",
      "Iteration 2804, loss = 0.10063785\n",
      "Iteration 2805, loss = 0.10061312\n",
      "Iteration 2806, loss = 0.10058844\n",
      "Iteration 2807, loss = 0.10056380\n",
      "Iteration 2808, loss = 0.10053920\n",
      "Iteration 2809, loss = 0.10051464\n",
      "Iteration 2810, loss = 0.10049012\n",
      "Iteration 2811, loss = 0.10046565\n",
      "Iteration 2812, loss = 0.10044122\n",
      "Iteration 2813, loss = 0.10041682\n",
      "Iteration 2814, loss = 0.10039247\n",
      "Iteration 2815, loss = 0.10036817\n",
      "Iteration 2816, loss = 0.10034390\n",
      "Iteration 2817, loss = 0.10031967\n",
      "Iteration 2818, loss = 0.10029549\n",
      "Iteration 2819, loss = 0.10027135\n",
      "Iteration 2820, loss = 0.10024725\n",
      "Iteration 2821, loss = 0.10022319\n",
      "Iteration 2822, loss = 0.10019917\n",
      "Iteration 2823, loss = 0.10017519\n",
      "Iteration 2824, loss = 0.10015126\n",
      "Iteration 2825, loss = 0.10012736\n",
      "Iteration 2826, loss = 0.10010351\n",
      "Iteration 2827, loss = 0.10007970\n",
      "Iteration 2828, loss = 0.10005592\n",
      "Iteration 2829, loss = 0.10003219\n",
      "Iteration 2830, loss = 0.10000851\n",
      "Iteration 2831, loss = 0.09998486\n",
      "Iteration 2832, loss = 0.09996125\n",
      "Iteration 2833, loss = 0.09993769\n",
      "Iteration 2834, loss = 0.09991416\n",
      "Iteration 2835, loss = 0.09989068\n",
      "Iteration 2836, loss = 0.09986724\n",
      "Iteration 2837, loss = 0.09984384\n",
      "Iteration 2838, loss = 0.09982048\n",
      "Iteration 2839, loss = 0.09979716\n",
      "Iteration 2840, loss = 0.09977388\n",
      "Iteration 2841, loss = 0.09975064\n",
      "Iteration 2842, loss = 0.09972744\n",
      "Iteration 2843, loss = 0.09970429\n",
      "Iteration 2844, loss = 0.09968117\n",
      "Iteration 2845, loss = 0.09965810\n",
      "Iteration 2846, loss = 0.09963506\n",
      "Iteration 2847, loss = 0.09961207\n",
      "Iteration 2848, loss = 0.09958912\n",
      "Iteration 2849, loss = 0.09956620\n",
      "Iteration 2850, loss = 0.09954333\n",
      "Iteration 2851, loss = 0.09952050\n",
      "Iteration 2852, loss = 0.09949771\n",
      "Iteration 2853, loss = 0.09947496\n",
      "Iteration 2854, loss = 0.09945225\n",
      "Iteration 2855, loss = 0.09942958\n",
      "Iteration 2856, loss = 0.09940695\n",
      "Iteration 2857, loss = 0.09938436\n",
      "Iteration 2858, loss = 0.09936182\n",
      "Iteration 2859, loss = 0.09933931\n",
      "Iteration 2860, loss = 0.09931684\n",
      "Iteration 2861, loss = 0.09929441\n",
      "Iteration 2862, loss = 0.09927203\n",
      "Iteration 2863, loss = 0.09924968\n",
      "Iteration 2864, loss = 0.09922737\n",
      "Iteration 2865, loss = 0.09920511\n",
      "Iteration 2866, loss = 0.09918288\n",
      "Iteration 2867, loss = 0.09916069\n",
      "Iteration 2868, loss = 0.09913855\n",
      "Iteration 2869, loss = 0.09911644\n",
      "Iteration 2870, loss = 0.09909438\n",
      "Iteration 2871, loss = 0.09907235\n",
      "Iteration 2872, loss = 0.09905036\n",
      "Iteration 2873, loss = 0.09902842\n",
      "Iteration 2874, loss = 0.09900651\n",
      "Iteration 2875, loss = 0.09898464\n",
      "Iteration 2876, loss = 0.09896282\n",
      "Iteration 2877, loss = 0.09894103\n",
      "Iteration 2878, loss = 0.09891928\n",
      "Iteration 2879, loss = 0.09889758\n",
      "Iteration 2880, loss = 0.09887591\n",
      "Iteration 2881, loss = 0.09885428\n",
      "Iteration 2882, loss = 0.09883269\n",
      "Iteration 2883, loss = 0.09881114\n",
      "Iteration 2884, loss = 0.09878963\n",
      "Iteration 2885, loss = 0.09876816\n",
      "Iteration 2886, loss = 0.09874673\n",
      "Iteration 2887, loss = 0.09872534\n",
      "Iteration 2888, loss = 0.09870399\n",
      "Iteration 2889, loss = 0.09868268\n",
      "Iteration 2890, loss = 0.09866140\n",
      "Iteration 2891, loss = 0.09864017\n",
      "Iteration 2892, loss = 0.09861898\n",
      "Iteration 2893, loss = 0.09859782\n",
      "Iteration 2894, loss = 0.09857671\n",
      "Iteration 2895, loss = 0.09855563\n",
      "Iteration 2896, loss = 0.09853459\n",
      "Iteration 2897, loss = 0.09851360\n",
      "Iteration 2898, loss = 0.09849264\n",
      "Iteration 2899, loss = 0.09847172\n",
      "Iteration 2900, loss = 0.09845084\n",
      "Iteration 2901, loss = 0.09843000\n",
      "Iteration 2902, loss = 0.09840919\n",
      "Iteration 2903, loss = 0.09838843\n",
      "Iteration 2904, loss = 0.09836770\n",
      "Iteration 2905, loss = 0.09834702\n",
      "Iteration 2906, loss = 0.09832637\n",
      "Iteration 2907, loss = 0.09830576\n",
      "Iteration 2908, loss = 0.09828519\n",
      "Iteration 2909, loss = 0.09826466\n",
      "Iteration 2910, loss = 0.09824417\n",
      "Iteration 2911, loss = 0.09822372\n",
      "Iteration 2912, loss = 0.09820330\n",
      "Iteration 2913, loss = 0.09818293\n",
      "Iteration 2914, loss = 0.09816259\n",
      "Iteration 2915, loss = 0.09814229\n",
      "Iteration 2916, loss = 0.09812203\n",
      "Iteration 2917, loss = 0.09810181\n",
      "Iteration 2918, loss = 0.09808163\n",
      "Iteration 2919, loss = 0.09806148\n",
      "Iteration 2920, loss = 0.09804138\n",
      "Iteration 2921, loss = 0.09802131\n",
      "Iteration 2922, loss = 0.09800128\n",
      "Iteration 2923, loss = 0.09798129\n",
      "Iteration 2924, loss = 0.09796134\n",
      "Iteration 2925, loss = 0.09794142\n",
      "Iteration 2926, loss = 0.09792154\n",
      "Iteration 2927, loss = 0.09790171\n",
      "Iteration 2928, loss = 0.09788191\n",
      "Iteration 2929, loss = 0.09786214\n",
      "Iteration 2930, loss = 0.09784242\n",
      "Iteration 2931, loss = 0.09782273\n",
      "Iteration 2932, loss = 0.09780309\n",
      "Iteration 2933, loss = 0.09778348\n",
      "Iteration 2934, loss = 0.09776390\n",
      "Iteration 2935, loss = 0.09774437\n",
      "Iteration 2936, loss = 0.09772487\n",
      "Iteration 2937, loss = 0.09770542\n",
      "Iteration 2938, loss = 0.09768599\n",
      "Iteration 2939, loss = 0.09766661\n",
      "Iteration 2940, loss = 0.09764727\n",
      "Iteration 2941, loss = 0.09762796\n",
      "Iteration 2942, loss = 0.09760869\n",
      "Iteration 2943, loss = 0.09758946\n",
      "Iteration 2944, loss = 0.09757026\n",
      "Iteration 2945, loss = 0.09755110\n",
      "Iteration 2946, loss = 0.09753199\n",
      "Iteration 2947, loss = 0.09751290\n",
      "Iteration 2948, loss = 0.09749386\n",
      "Iteration 2949, loss = 0.09747485\n",
      "Iteration 2950, loss = 0.09745588\n",
      "Iteration 2951, loss = 0.09743695\n",
      "Iteration 2952, loss = 0.09741805\n",
      "Iteration 2953, loss = 0.09739920\n",
      "Iteration 2954, loss = 0.09738037\n",
      "Iteration 2955, loss = 0.09736159\n",
      "Iteration 2956, loss = 0.09734284\n",
      "Iteration 2957, loss = 0.09732414\n",
      "Iteration 2958, loss = 0.09730546\n",
      "Iteration 2959, loss = 0.09728683\n",
      "Iteration 2960, loss = 0.09726823\n",
      "Iteration 2961, loss = 0.09724967\n",
      "Iteration 2962, loss = 0.09723115\n",
      "Iteration 2963, loss = 0.09721266\n",
      "Iteration 2964, loss = 0.09719421\n",
      "Iteration 2965, loss = 0.09717579\n",
      "Iteration 2966, loss = 0.09715742\n",
      "Iteration 2967, loss = 0.09713908\n",
      "Iteration 2968, loss = 0.09712077\n",
      "Iteration 2969, loss = 0.09710251\n",
      "Iteration 2970, loss = 0.09708428\n",
      "Iteration 2971, loss = 0.09706608\n",
      "Iteration 2972, loss = 0.09704793\n",
      "Iteration 2973, loss = 0.09702981\n",
      "Iteration 2974, loss = 0.09701172\n",
      "Iteration 2975, loss = 0.09699368\n",
      "Iteration 2976, loss = 0.09697567\n",
      "Iteration 2977, loss = 0.09695769\n",
      "Iteration 2978, loss = 0.09693975\n",
      "Iteration 2979, loss = 0.09692185\n",
      "Iteration 2980, loss = 0.09690399\n",
      "Iteration 2981, loss = 0.09688616\n",
      "Iteration 2982, loss = 0.09686837\n",
      "Iteration 2983, loss = 0.09685061\n",
      "Iteration 2984, loss = 0.09683289\n",
      "Iteration 2985, loss = 0.09681520\n",
      "Iteration 2986, loss = 0.09679756\n",
      "Iteration 2987, loss = 0.09677994\n",
      "Iteration 2988, loss = 0.09676237\n",
      "Iteration 2989, loss = 0.09674483\n",
      "Iteration 2990, loss = 0.09672732\n",
      "Iteration 2991, loss = 0.09670985\n",
      "Iteration 2992, loss = 0.09669242\n",
      "Iteration 2993, loss = 0.09667502\n",
      "Iteration 2994, loss = 0.09665766\n",
      "Iteration 2995, loss = 0.09664034\n",
      "Iteration 2996, loss = 0.09662305\n",
      "Iteration 2997, loss = 0.09660579\n",
      "Iteration 2998, loss = 0.09658857\n",
      "Iteration 2999, loss = 0.09657139\n",
      "Iteration 3000, loss = 0.09655424\n",
      "Iteration 3001, loss = 0.09653713\n",
      "Iteration 3002, loss = 0.09652006\n",
      "Iteration 3003, loss = 0.09650302\n",
      "Iteration 3004, loss = 0.09648601\n",
      "Iteration 3005, loss = 0.09646904\n",
      "Iteration 3006, loss = 0.09645210\n",
      "Iteration 3007, loss = 0.09643521\n",
      "Iteration 3008, loss = 0.09641834\n",
      "Iteration 3009, loss = 0.09640151\n",
      "Iteration 3010, loss = 0.09638472\n",
      "Iteration 3011, loss = 0.09636796\n",
      "Iteration 3012, loss = 0.09635124\n",
      "Iteration 3013, loss = 0.09633455\n",
      "Iteration 3014, loss = 0.09631789\n",
      "Iteration 3015, loss = 0.09630127\n",
      "Iteration 3016, loss = 0.09628469\n",
      "Iteration 3017, loss = 0.09626814\n",
      "Iteration 3018, loss = 0.09625163\n",
      "Iteration 3019, loss = 0.09623515\n",
      "Iteration 3020, loss = 0.09621870\n",
      "Iteration 3021, loss = 0.09620229\n",
      "Iteration 3022, loss = 0.09618592\n",
      "Iteration 3023, loss = 0.09616958\n",
      "Iteration 3024, loss = 0.09615327\n",
      "Iteration 3025, loss = 0.09613700\n",
      "Iteration 3026, loss = 0.09612076\n",
      "Iteration 3027, loss = 0.09610456\n",
      "Iteration 3028, loss = 0.09608839\n",
      "Iteration 3029, loss = 0.09607226\n",
      "Iteration 3030, loss = 0.09605616\n",
      "Iteration 3031, loss = 0.09604010\n",
      "Iteration 3032, loss = 0.09602407\n",
      "Iteration 3033, loss = 0.09600807\n",
      "Iteration 3034, loss = 0.09599211\n",
      "Iteration 3035, loss = 0.09597618\n",
      "Iteration 3036, loss = 0.09596029\n",
      "Iteration 3037, loss = 0.09594443\n",
      "Iteration 3038, loss = 0.09592860\n",
      "Iteration 3039, loss = 0.09591281\n",
      "Iteration 3040, loss = 0.09589706\n",
      "Iteration 3041, loss = 0.09588133\n",
      "Iteration 3042, loss = 0.09586564\n",
      "Iteration 3043, loss = 0.09584999\n",
      "Iteration 3044, loss = 0.09583437\n",
      "Iteration 3045, loss = 0.09581878\n",
      "Iteration 3046, loss = 0.09580322\n",
      "Iteration 3047, loss = 0.09578770\n",
      "Iteration 3048, loss = 0.09577222\n",
      "Iteration 3049, loss = 0.09575676\n",
      "Iteration 3050, loss = 0.09574134\n",
      "Iteration 3051, loss = 0.09572596\n",
      "Iteration 3052, loss = 0.09571060\n",
      "Iteration 3053, loss = 0.09569529\n",
      "Iteration 3054, loss = 0.09568000\n",
      "Iteration 3055, loss = 0.09566475\n",
      "Iteration 3056, loss = 0.09564953\n",
      "Iteration 3057, loss = 0.09563434\n",
      "Iteration 3058, loss = 0.09561919\n",
      "Iteration 3059, loss = 0.09560407\n",
      "Iteration 3060, loss = 0.09558899\n",
      "Iteration 3061, loss = 0.09557393\n",
      "Iteration 3062, loss = 0.09555891\n",
      "Iteration 3063, loss = 0.09554393\n",
      "Iteration 3064, loss = 0.09552897\n",
      "Iteration 3065, loss = 0.09551405\n",
      "Iteration 3066, loss = 0.09549916\n",
      "Iteration 3067, loss = 0.09548431\n",
      "Iteration 3068, loss = 0.09546949\n",
      "Iteration 3069, loss = 0.09545470\n",
      "Iteration 3070, loss = 0.09543994\n",
      "Iteration 3071, loss = 0.09542522\n",
      "Iteration 3072, loss = 0.09541053\n",
      "Iteration 3073, loss = 0.09539587\n",
      "Iteration 3074, loss = 0.09538124\n",
      "Iteration 3075, loss = 0.09536665\n",
      "Iteration 3076, loss = 0.09535209\n",
      "Iteration 3077, loss = 0.09533756\n",
      "Iteration 3078, loss = 0.09532307\n",
      "Iteration 3079, loss = 0.09530860\n",
      "Iteration 3080, loss = 0.09529417\n",
      "Iteration 3081, loss = 0.09527977\n",
      "Iteration 3082, loss = 0.09526541\n",
      "Iteration 3083, loss = 0.09525107\n",
      "Iteration 3084, loss = 0.09523677\n",
      "Iteration 3085, loss = 0.09522250\n",
      "Iteration 3086, loss = 0.09520827\n",
      "Iteration 3087, loss = 0.09519406\n",
      "Iteration 3088, loss = 0.09517989\n",
      "Iteration 3089, loss = 0.09516575\n",
      "Iteration 3090, loss = 0.09515164\n",
      "Iteration 3091, loss = 0.09513756\n",
      "Iteration 3092, loss = 0.09512351\n",
      "Iteration 3093, loss = 0.09510950\n",
      "Iteration 3094, loss = 0.09509552\n",
      "Iteration 3095, loss = 0.09508157\n",
      "Iteration 3096, loss = 0.09506765\n",
      "Iteration 3097, loss = 0.09505376\n",
      "Iteration 3098, loss = 0.09503991\n",
      "Iteration 3099, loss = 0.09502608\n",
      "Iteration 3100, loss = 0.09501229\n",
      "Iteration 3101, loss = 0.09499853\n",
      "Iteration 3102, loss = 0.09498480\n",
      "Iteration 3103, loss = 0.09497110\n",
      "Iteration 3104, loss = 0.09495744\n",
      "Iteration 3105, loss = 0.09494380\n",
      "Iteration 3106, loss = 0.09493020\n",
      "Iteration 3107, loss = 0.09491663\n",
      "Iteration 3108, loss = 0.09490308\n",
      "Iteration 3109, loss = 0.09488957\n",
      "Iteration 3110, loss = 0.09487610\n",
      "Iteration 3111, loss = 0.09486265\n",
      "Iteration 3112, loss = 0.09484923\n",
      "Iteration 3113, loss = 0.09483584\n",
      "Iteration 3114, loss = 0.09482249\n",
      "Iteration 3115, loss = 0.09480917\n",
      "Iteration 3116, loss = 0.09479587\n",
      "Iteration 3117, loss = 0.09478261\n",
      "Iteration 3118, loss = 0.09476938\n",
      "Iteration 3119, loss = 0.09475618\n",
      "Iteration 3120, loss = 0.09474301\n",
      "Iteration 3121, loss = 0.09472987\n",
      "Iteration 3122, loss = 0.09471676\n",
      "Iteration 3123, loss = 0.09470368\n",
      "Iteration 3124, loss = 0.09469063\n",
      "Iteration 3125, loss = 0.09467762\n",
      "Iteration 3126, loss = 0.09466463\n",
      "Iteration 3127, loss = 0.09465167\n",
      "Iteration 3128, loss = 0.09463875\n",
      "Iteration 3129, loss = 0.09462585\n",
      "Iteration 3130, loss = 0.09461299\n",
      "Iteration 3131, loss = 0.09460015\n",
      "Iteration 3132, loss = 0.09458734\n",
      "Iteration 3133, loss = 0.09457457\n",
      "Iteration 3134, loss = 0.09456182\n",
      "Iteration 3135, loss = 0.09454911\n",
      "Iteration 3136, loss = 0.09453643\n",
      "Iteration 3137, loss = 0.09452377\n",
      "Iteration 3138, loss = 0.09451115\n",
      "Iteration 3139, loss = 0.09449855\n",
      "Iteration 3140, loss = 0.09448599\n",
      "Iteration 3141, loss = 0.09447345\n",
      "Iteration 3142, loss = 0.09446095\n",
      "Iteration 3143, loss = 0.09444847\n",
      "Iteration 3144, loss = 0.09443602\n",
      "Iteration 3145, loss = 0.09442361\n",
      "Iteration 3146, loss = 0.09441122\n",
      "Iteration 3147, loss = 0.09439886\n",
      "Iteration 3148, loss = 0.09438653\n",
      "Iteration 3149, loss = 0.09437424\n",
      "Iteration 3150, loss = 0.09436197\n",
      "Iteration 3151, loss = 0.09434973\n",
      "Iteration 3152, loss = 0.09433752\n",
      "Iteration 3153, loss = 0.09432534\n",
      "Iteration 3154, loss = 0.09431319\n",
      "Iteration 3155, loss = 0.09430106\n",
      "Iteration 3156, loss = 0.09428897\n",
      "Iteration 3157, loss = 0.09427691\n",
      "Iteration 3158, loss = 0.09426487\n",
      "Iteration 3159, loss = 0.09425286\n",
      "Iteration 3160, loss = 0.09424089\n",
      "Iteration 3161, loss = 0.09422894\n",
      "Iteration 3162, loss = 0.09421702\n",
      "Iteration 3163, loss = 0.09420513\n",
      "Iteration 3164, loss = 0.09419327\n",
      "Iteration 3165, loss = 0.09418144\n",
      "Iteration 3166, loss = 0.09416963\n",
      "Iteration 3167, loss = 0.09415786\n",
      "Iteration 3168, loss = 0.09414611\n",
      "Iteration 3169, loss = 0.09413439\n",
      "Iteration 3170, loss = 0.09412270\n",
      "Iteration 3171, loss = 0.09411104\n",
      "Iteration 3172, loss = 0.09409941\n",
      "Iteration 3173, loss = 0.09408780\n",
      "Iteration 3174, loss = 0.09407623\n",
      "Iteration 3175, loss = 0.09406468\n",
      "Iteration 3176, loss = 0.09405316\n",
      "Iteration 3177, loss = 0.09404167\n",
      "Iteration 3178, loss = 0.09403021\n",
      "Iteration 3179, loss = 0.09401877\n",
      "Iteration 3180, loss = 0.09400737\n",
      "Iteration 3181, loss = 0.09399599\n",
      "Iteration 3182, loss = 0.09398464\n",
      "Iteration 3183, loss = 0.09397332\n",
      "Iteration 3184, loss = 0.09396202\n",
      "Iteration 3185, loss = 0.09395076\n",
      "Iteration 3186, loss = 0.09393952\n",
      "Iteration 3187, loss = 0.09392831\n",
      "Iteration 3188, loss = 0.09391713\n",
      "Iteration 3189, loss = 0.09390597\n",
      "Iteration 3190, loss = 0.09389484\n",
      "Iteration 3191, loss = 0.09388374\n",
      "Iteration 3192, loss = 0.09387267\n",
      "Iteration 3193, loss = 0.09386163\n",
      "Iteration 3194, loss = 0.09385061\n",
      "Iteration 3195, loss = 0.09383962\n",
      "Iteration 3196, loss = 0.09382866\n",
      "Iteration 3197, loss = 0.09381772\n",
      "Iteration 3198, loss = 0.09380682\n",
      "Iteration 3199, loss = 0.09379594\n",
      "Iteration 3200, loss = 0.09378508\n",
      "Iteration 3201, loss = 0.09377426\n",
      "Iteration 3202, loss = 0.09376346\n",
      "Iteration 3203, loss = 0.09375269\n",
      "Iteration 3204, loss = 0.09374195\n",
      "Iteration 3205, loss = 0.09373123\n",
      "Iteration 3206, loss = 0.09372054\n",
      "Iteration 3207, loss = 0.09370988\n",
      "Iteration 3208, loss = 0.09369924\n",
      "Iteration 3209, loss = 0.09368863\n",
      "Iteration 3210, loss = 0.09367805\n",
      "Iteration 3211, loss = 0.09366749\n",
      "Iteration 3212, loss = 0.09365696\n",
      "Iteration 3213, loss = 0.09364646\n",
      "Iteration 3214, loss = 0.09363598\n",
      "Iteration 3215, loss = 0.09362554\n",
      "Iteration 3216, loss = 0.09361511\n",
      "Iteration 3217, loss = 0.09360472\n",
      "Iteration 3218, loss = 0.09359435\n",
      "Iteration 3219, loss = 0.09358401\n",
      "Iteration 3220, loss = 0.09357369\n",
      "Iteration 3221, loss = 0.09356340\n",
      "Iteration 3222, loss = 0.09355314\n",
      "Iteration 3223, loss = 0.09354290\n",
      "Iteration 3224, loss = 0.09353269\n",
      "Iteration 3225, loss = 0.09352250\n",
      "Iteration 3226, loss = 0.09351234\n",
      "Iteration 3227, loss = 0.09350221\n",
      "Iteration 3228, loss = 0.09349210\n",
      "Iteration 3229, loss = 0.09348202\n",
      "Iteration 3230, loss = 0.09347197\n",
      "Iteration 3231, loss = 0.09346194\n",
      "Iteration 3232, loss = 0.09345193\n",
      "Iteration 3233, loss = 0.09344196\n",
      "Iteration 3234, loss = 0.09343201\n",
      "Iteration 3235, loss = 0.09342208\n",
      "Iteration 3236, loss = 0.09341218\n",
      "Iteration 3237, loss = 0.09340231\n",
      "Iteration 3238, loss = 0.09339246\n",
      "Iteration 3239, loss = 0.09338263\n",
      "Iteration 3240, loss = 0.09337284\n",
      "Iteration 3241, loss = 0.09336306\n",
      "Iteration 3242, loss = 0.09335332\n",
      "Iteration 3243, loss = 0.09334360\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "pipe.score(..)=0.73\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipe = Pipeline(\n",
    "    [\n",
    "        ('scaler', MinMaxScaler()),\n",
    "        ('mlp', mlp)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipe.fit(X, y)\n",
    "\n",
    "print(f\"pipe.score(..)={pipe.score(X, y):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc) Outliers and the Min-max Scaler vs. the Standard Scaler\n",
    "\n",
    "Explain the fundamental problem with a min-max scaler and outliers. \n",
    "\n",
    "Will a `sklearn.preprocessing.StandardScaler` do better here, in the case of abnormal feature values/outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 23.51270883\n",
      "Iteration 2, loss = 23.47010167\n",
      "Iteration 3, loss = 23.42758225\n",
      "Iteration 4, loss = 23.38515108\n",
      "Iteration 5, loss = 23.34280687\n",
      "Iteration 6, loss = 23.30054517\n",
      "Iteration 7, loss = 23.25835884\n",
      "Iteration 8, loss = 23.21624136\n",
      "Iteration 9, loss = 23.17418905\n",
      "Iteration 10, loss = 23.13220036\n",
      "Iteration 11, loss = 23.09027477\n",
      "Iteration 12, loss = 23.04841214\n",
      "Iteration 13, loss = 23.00661250\n",
      "Iteration 14, loss = 22.96487596\n",
      "Iteration 15, loss = 22.92320260\n",
      "Iteration 16, loss = 22.88159255\n",
      "Iteration 17, loss = 22.84004588\n",
      "Iteration 18, loss = 22.79856267\n",
      "Iteration 19, loss = 22.75714296\n",
      "Iteration 20, loss = 22.71578677\n",
      "Iteration 21, loss = 22.67449410\n",
      "Iteration 22, loss = 22.63326492\n",
      "Iteration 23, loss = 22.59209914\n",
      "Iteration 24, loss = 22.55099669\n",
      "Iteration 25, loss = 22.50995744\n",
      "Iteration 26, loss = 22.46898125\n",
      "Iteration 27, loss = 22.42806792\n",
      "Iteration 28, loss = 22.38721726\n",
      "Iteration 29, loss = 22.34642904\n",
      "Iteration 30, loss = 22.30570299\n",
      "Iteration 31, loss = 22.26503884\n",
      "Iteration 32, loss = 22.22443628\n",
      "Iteration 33, loss = 22.18389497\n",
      "Iteration 34, loss = 22.14341457\n",
      "Iteration 35, loss = 22.10299470\n",
      "Iteration 36, loss = 22.06263498\n",
      "Iteration 37, loss = 22.02230989\n",
      "Iteration 38, loss = 21.98200281\n",
      "Iteration 39, loss = 21.94174972\n",
      "Iteration 40, loss = 21.90155065\n",
      "Iteration 41, loss = 21.86140555\n",
      "Iteration 42, loss = 21.82141284\n",
      "Iteration 43, loss = 21.78151080\n",
      "Iteration 44, loss = 21.74166124\n",
      "Iteration 45, loss = 21.70186396\n",
      "Iteration 46, loss = 21.66211873\n",
      "Iteration 47, loss = 21.62242527\n",
      "Iteration 48, loss = 21.58278330\n",
      "Iteration 49, loss = 21.54319250\n",
      "Iteration 50, loss = 21.50365251\n",
      "Iteration 51, loss = 21.46416298\n",
      "Iteration 52, loss = 21.42472353\n",
      "Iteration 53, loss = 21.38533376\n",
      "Iteration 54, loss = 21.34599326\n",
      "Iteration 55, loss = 21.30670161\n",
      "Iteration 56, loss = 21.26745837\n",
      "Iteration 57, loss = 21.22826310\n",
      "Iteration 58, loss = 21.18911535\n",
      "Iteration 59, loss = 21.15001466\n",
      "Iteration 60, loss = 21.11096057\n",
      "Iteration 61, loss = 21.07195261\n",
      "Iteration 62, loss = 21.03299031\n",
      "Iteration 63, loss = 20.99407318\n",
      "Iteration 64, loss = 20.95520075\n",
      "Iteration 65, loss = 20.91637252\n",
      "Iteration 66, loss = 20.87758803\n",
      "Iteration 67, loss = 20.83884676\n",
      "Iteration 68, loss = 20.80014825\n",
      "Iteration 69, loss = 20.76149199\n",
      "Iteration 70, loss = 20.72288019\n",
      "Iteration 71, loss = 20.68432396\n",
      "Iteration 72, loss = 20.64600363\n",
      "Iteration 73, loss = 20.60776476\n",
      "Iteration 74, loss = 20.56957933\n",
      "Iteration 75, loss = 20.53144499\n",
      "Iteration 76, loss = 20.49335957\n",
      "Iteration 77, loss = 20.45532107\n",
      "Iteration 78, loss = 20.41732768\n",
      "Iteration 79, loss = 20.37937770\n",
      "Iteration 80, loss = 20.34146822\n",
      "Iteration 81, loss = 20.30359747\n",
      "Iteration 82, loss = 20.26576539\n",
      "Iteration 83, loss = 20.22797081\n",
      "Iteration 84, loss = 20.19021261\n",
      "Iteration 85, loss = 20.15248976\n",
      "Iteration 86, loss = 20.11480127\n",
      "Iteration 87, loss = 20.07714624\n",
      "Iteration 88, loss = 20.03952379\n",
      "Iteration 89, loss = 20.00193312\n",
      "Iteration 90, loss = 19.96437344\n",
      "Iteration 91, loss = 19.92684403\n",
      "Iteration 92, loss = 19.88934417\n",
      "Iteration 93, loss = 19.85187322\n",
      "Iteration 94, loss = 19.81443052\n",
      "Iteration 95, loss = 19.77701548\n",
      "Iteration 96, loss = 19.73962748\n",
      "Iteration 97, loss = 19.70229719\n",
      "Iteration 98, loss = 19.66502578\n",
      "Iteration 99, loss = 19.62778365\n",
      "Iteration 100, loss = 19.59056984\n",
      "Iteration 101, loss = 19.55338343\n",
      "Iteration 102, loss = 19.51622356\n",
      "Iteration 103, loss = 19.47908945\n",
      "Iteration 104, loss = 19.44198031\n",
      "Iteration 105, loss = 19.40489543\n",
      "Iteration 106, loss = 19.36783413\n",
      "Iteration 107, loss = 19.33079574\n",
      "Iteration 108, loss = 19.29377965\n",
      "Iteration 109, loss = 19.25684300\n",
      "Iteration 110, loss = 19.22012544\n",
      "Iteration 111, loss = 19.18344477\n",
      "Iteration 112, loss = 19.14679855\n",
      "Iteration 113, loss = 19.11018459\n",
      "Iteration 114, loss = 19.07360084\n",
      "Iteration 115, loss = 19.03699929\n",
      "Iteration 116, loss = 19.00040229\n",
      "Iteration 117, loss = 18.96382466\n",
      "Iteration 118, loss = 18.92726550\n",
      "Iteration 119, loss = 18.89072395\n",
      "Iteration 120, loss = 18.85419923\n",
      "Iteration 121, loss = 18.81769056\n",
      "Iteration 122, loss = 18.78119724\n",
      "Iteration 123, loss = 18.74471860\n",
      "Iteration 124, loss = 18.70825398\n",
      "Iteration 125, loss = 18.67180279\n",
      "Iteration 126, loss = 18.63540889\n",
      "Iteration 127, loss = 18.59906022\n",
      "Iteration 128, loss = 18.56272844\n",
      "Iteration 129, loss = 18.52641248\n",
      "Iteration 130, loss = 18.49011132\n",
      "Iteration 131, loss = 18.45382402\n",
      "Iteration 132, loss = 18.41754970\n",
      "Iteration 133, loss = 18.38128754\n",
      "Iteration 134, loss = 18.34514810\n",
      "Iteration 135, loss = 18.30907653\n",
      "Iteration 136, loss = 18.27310606\n",
      "Iteration 137, loss = 18.23716589\n",
      "Iteration 138, loss = 18.20124927\n",
      "Iteration 139, loss = 18.16535393\n",
      "Iteration 140, loss = 18.12947779\n",
      "Iteration 141, loss = 18.09361895\n",
      "Iteration 142, loss = 18.05779674\n",
      "Iteration 143, loss = 18.02210561\n",
      "Iteration 144, loss = 17.98643608\n",
      "Iteration 145, loss = 17.95081510\n",
      "Iteration 146, loss = 17.91536162\n",
      "Iteration 147, loss = 17.88003152\n",
      "Iteration 148, loss = 17.84473029\n",
      "Iteration 149, loss = 17.80947390\n",
      "Iteration 150, loss = 17.77439903\n",
      "Iteration 151, loss = 17.73951713\n",
      "Iteration 152, loss = 17.70486287\n",
      "Iteration 153, loss = 17.67082833\n",
      "Iteration 154, loss = 17.63704742\n",
      "Iteration 155, loss = 17.60331456\n",
      "Iteration 156, loss = 17.56959340\n",
      "Iteration 157, loss = 17.53594161\n",
      "Iteration 158, loss = 17.50274994\n",
      "Iteration 159, loss = 17.46995981\n",
      "Iteration 160, loss = 17.43718144\n",
      "Iteration 161, loss = 17.40453682\n",
      "Iteration 162, loss = 17.37187549\n",
      "Iteration 163, loss = 17.33919799\n",
      "Iteration 164, loss = 17.30650478\n",
      "Iteration 165, loss = 17.27379625\n",
      "Iteration 166, loss = 17.24107274\n",
      "Iteration 167, loss = 17.20833453\n",
      "Iteration 168, loss = 17.17558186\n",
      "Iteration 169, loss = 17.14281493\n",
      "Iteration 170, loss = 17.11003390\n",
      "Iteration 171, loss = 17.07723890\n",
      "Iteration 172, loss = 17.04428106\n",
      "Iteration 173, loss = 17.01127143\n",
      "Iteration 174, loss = 16.97822170\n",
      "Iteration 175, loss = 16.94513620\n",
      "Iteration 176, loss = 16.91201870\n",
      "Iteration 177, loss = 16.87887247\n",
      "Iteration 178, loss = 16.84570031\n",
      "Iteration 179, loss = 16.81250468\n",
      "Iteration 180, loss = 16.77928769\n",
      "Iteration 181, loss = 16.74605117\n",
      "Iteration 182, loss = 16.71279671\n",
      "Iteration 183, loss = 16.67952567\n",
      "Iteration 184, loss = 16.64623923\n",
      "Iteration 185, loss = 16.61300691\n",
      "Iteration 186, loss = 16.57986805\n",
      "Iteration 187, loss = 16.54672762\n",
      "Iteration 188, loss = 16.51358509\n",
      "Iteration 189, loss = 16.48043994\n",
      "Iteration 190, loss = 16.44729168\n",
      "Iteration 191, loss = 16.41413985\n",
      "Iteration 192, loss = 16.38098401\n",
      "Iteration 193, loss = 16.34782374\n",
      "Iteration 194, loss = 16.31465864\n",
      "Iteration 195, loss = 16.28148834\n",
      "Iteration 196, loss = 16.24831250\n",
      "Iteration 197, loss = 16.21513080\n",
      "Iteration 198, loss = 16.18194292\n",
      "Iteration 199, loss = 16.14874860\n",
      "Iteration 200, loss = 16.11554756\n",
      "Iteration 201, loss = 16.08233957\n",
      "Iteration 202, loss = 16.04912440\n",
      "Iteration 203, loss = 16.01590185\n",
      "Iteration 204, loss = 15.98267173\n",
      "Iteration 205, loss = 15.94943387\n",
      "Iteration 206, loss = 15.91618811\n",
      "Iteration 207, loss = 15.88293431\n",
      "Iteration 208, loss = 15.84967235\n",
      "Iteration 209, loss = 15.81640211\n",
      "Iteration 210, loss = 15.78308840\n",
      "Iteration 211, loss = 15.74972020\n",
      "Iteration 212, loss = 15.71633654\n",
      "Iteration 213, loss = 15.68293803\n",
      "Iteration 214, loss = 15.64952523\n",
      "Iteration 215, loss = 15.61609867\n",
      "Iteration 216, loss = 15.58265879\n",
      "Iteration 217, loss = 15.54920603\n",
      "Iteration 218, loss = 15.51574077\n",
      "Iteration 219, loss = 15.48226337\n",
      "Iteration 220, loss = 15.44877415\n",
      "Iteration 221, loss = 15.41527342\n",
      "Iteration 222, loss = 15.38176146\n",
      "Iteration 223, loss = 15.34823853\n",
      "Iteration 224, loss = 15.31470488\n",
      "Iteration 225, loss = 15.28116073\n",
      "Iteration 226, loss = 15.24760632\n",
      "Iteration 227, loss = 15.21404184\n",
      "Iteration 228, loss = 15.18046750\n",
      "Iteration 229, loss = 15.14688350\n",
      "Iteration 230, loss = 15.11329001\n",
      "Iteration 231, loss = 15.07968722\n",
      "Iteration 232, loss = 15.04607530\n",
      "Iteration 233, loss = 15.01245444\n",
      "Iteration 234, loss = 14.97882479\n",
      "Iteration 235, loss = 14.94518653\n",
      "Iteration 236, loss = 14.91153982\n",
      "Iteration 237, loss = 14.87788483\n",
      "Iteration 238, loss = 14.84422172\n",
      "Iteration 239, loss = 14.81055067\n",
      "Iteration 240, loss = 14.77687183\n",
      "Iteration 241, loss = 14.74318536\n",
      "Iteration 242, loss = 14.70949145\n",
      "Iteration 243, loss = 14.67579025\n",
      "Iteration 244, loss = 14.64208194\n",
      "Iteration 245, loss = 14.60836668\n",
      "Iteration 246, loss = 14.57464465\n",
      "Iteration 247, loss = 14.54091602\n",
      "Iteration 248, loss = 14.50718097\n",
      "Iteration 249, loss = 14.47343967\n",
      "Iteration 250, loss = 14.43969232\n",
      "Iteration 251, loss = 14.40593908\n",
      "Iteration 252, loss = 14.37218015\n",
      "Iteration 253, loss = 14.33841571\n",
      "Iteration 254, loss = 14.30464595\n",
      "Iteration 255, loss = 14.27087106\n",
      "Iteration 256, loss = 14.23708785\n",
      "Iteration 257, loss = 14.20318784\n",
      "Iteration 258, loss = 14.16927014\n",
      "Iteration 259, loss = 14.13533691\n",
      "Iteration 260, loss = 14.10139010\n",
      "Iteration 261, loss = 14.06743145\n",
      "Iteration 262, loss = 14.03346254\n",
      "Iteration 263, loss = 13.99948482\n",
      "Iteration 264, loss = 13.96549957\n",
      "Iteration 265, loss = 13.93150797\n",
      "Iteration 266, loss = 13.89751109\n",
      "Iteration 267, loss = 13.86350990\n",
      "Iteration 268, loss = 13.82942573\n",
      "Iteration 269, loss = 13.79522197\n",
      "Iteration 270, loss = 13.76099262\n",
      "Iteration 271, loss = 13.72674196\n",
      "Iteration 272, loss = 13.69247379\n",
      "Iteration 273, loss = 13.65819147\n",
      "Iteration 274, loss = 13.62389803\n",
      "Iteration 275, loss = 13.58959612\n",
      "Iteration 276, loss = 13.55539030\n",
      "Iteration 277, loss = 13.52123238\n",
      "Iteration 278, loss = 13.48708176\n",
      "Iteration 279, loss = 13.45293916\n",
      "Iteration 280, loss = 13.41880520\n",
      "Iteration 281, loss = 13.38468046\n",
      "Iteration 282, loss = 13.35062058\n",
      "Iteration 283, loss = 13.31662811\n",
      "Iteration 284, loss = 13.28265054\n",
      "Iteration 285, loss = 13.24868779\n",
      "Iteration 286, loss = 13.21473978\n",
      "Iteration 287, loss = 13.18080646\n",
      "Iteration 288, loss = 13.14688779\n",
      "Iteration 289, loss = 13.11298373\n",
      "Iteration 290, loss = 13.07909427\n",
      "Iteration 291, loss = 13.04521941\n",
      "Iteration 292, loss = 13.01135916\n",
      "Iteration 293, loss = 12.97751354\n",
      "Iteration 294, loss = 12.94368259\n",
      "Iteration 295, loss = 12.90986636\n",
      "Iteration 296, loss = 12.87606490\n",
      "Iteration 297, loss = 12.84227830\n",
      "Iteration 298, loss = 12.80850662\n",
      "Iteration 299, loss = 12.77474997\n",
      "Iteration 300, loss = 12.74100846\n",
      "Iteration 301, loss = 12.70728218\n",
      "Iteration 302, loss = 12.67357128\n",
      "Iteration 303, loss = 12.63987587\n",
      "Iteration 304, loss = 12.60619610\n",
      "Iteration 305, loss = 12.57253212\n",
      "Iteration 306, loss = 12.53888408\n",
      "Iteration 307, loss = 12.50525215\n",
      "Iteration 308, loss = 12.47163650\n",
      "Iteration 309, loss = 12.43803731\n",
      "Iteration 310, loss = 12.40445476\n",
      "Iteration 311, loss = 12.37088904\n",
      "Iteration 312, loss = 12.33734036\n",
      "Iteration 313, loss = 12.30380890\n",
      "Iteration 314, loss = 12.27029487\n",
      "Iteration 315, loss = 12.23679850\n",
      "Iteration 316, loss = 12.20331999\n",
      "Iteration 317, loss = 12.16985958\n",
      "Iteration 318, loss = 12.13641747\n",
      "Iteration 319, loss = 12.10299391\n",
      "Iteration 320, loss = 12.06958913\n",
      "Iteration 321, loss = 12.03620336\n",
      "Iteration 322, loss = 12.00283685\n",
      "Iteration 323, loss = 11.96948984\n",
      "Iteration 324, loss = 11.93616258\n",
      "Iteration 325, loss = 11.90285532\n",
      "Iteration 326, loss = 11.86956832\n",
      "Iteration 327, loss = 11.83630183\n",
      "Iteration 328, loss = 11.80305611\n",
      "Iteration 329, loss = 11.76983142\n",
      "Iteration 330, loss = 11.73662802\n",
      "Iteration 331, loss = 11.70344620\n",
      "Iteration 332, loss = 11.67028620\n",
      "Iteration 333, loss = 11.63714831\n",
      "Iteration 334, loss = 11.60403279\n",
      "Iteration 335, loss = 11.57093993\n",
      "Iteration 336, loss = 11.53786999\n",
      "Iteration 337, loss = 11.50482325\n",
      "Iteration 338, loss = 11.47180000\n",
      "Iteration 339, loss = 11.43880052\n",
      "Iteration 340, loss = 11.40582508\n",
      "Iteration 341, loss = 11.37287398\n",
      "Iteration 342, loss = 11.33992961\n",
      "Iteration 343, loss = 11.30698215\n",
      "Iteration 344, loss = 11.27405457\n",
      "Iteration 345, loss = 11.24114794\n",
      "Iteration 346, loss = 11.20826320\n",
      "Iteration 347, loss = 11.17540122\n",
      "Iteration 348, loss = 11.14256280\n",
      "Iteration 349, loss = 11.10974865\n",
      "Iteration 350, loss = 11.07695945\n",
      "Iteration 351, loss = 11.04419579\n",
      "Iteration 352, loss = 11.01145825\n",
      "Iteration 353, loss = 10.97874737\n",
      "Iteration 354, loss = 10.94606365\n",
      "Iteration 355, loss = 10.91340755\n",
      "Iteration 356, loss = 10.88077953\n",
      "Iteration 357, loss = 10.84818001\n",
      "Iteration 358, loss = 10.81554767\n",
      "Iteration 359, loss = 10.78289868\n",
      "Iteration 360, loss = 10.75024725\n",
      "Iteration 361, loss = 10.71761563\n",
      "Iteration 362, loss = 10.68500557\n",
      "Iteration 363, loss = 10.65241867\n",
      "Iteration 364, loss = 10.61985636\n",
      "Iteration 365, loss = 10.58731990\n",
      "Iteration 366, loss = 10.55481047\n",
      "Iteration 367, loss = 10.52231046\n",
      "Iteration 368, loss = 10.48979287\n",
      "Iteration 369, loss = 10.45729474\n",
      "Iteration 370, loss = 10.42475591\n",
      "Iteration 371, loss = 10.39223956\n",
      "Iteration 372, loss = 10.35974848\n",
      "Iteration 373, loss = 10.32728503\n",
      "Iteration 374, loss = 10.29485123\n",
      "Iteration 375, loss = 10.26244884\n",
      "Iteration 376, loss = 10.23007934\n",
      "Iteration 377, loss = 10.19774403\n",
      "Iteration 378, loss = 10.16544405\n",
      "Iteration 379, loss = 10.13322203\n",
      "Iteration 380, loss = 10.10108115\n",
      "Iteration 381, loss = 10.06898184\n",
      "Iteration 382, loss = 10.03692441\n",
      "Iteration 383, loss = 10.00490909\n",
      "Iteration 384, loss = 9.97293613\n",
      "Iteration 385, loss = 9.94100574\n",
      "Iteration 386, loss = 9.90911811\n",
      "Iteration 387, loss = 9.87727342\n",
      "Iteration 388, loss = 9.84547184\n",
      "Iteration 389, loss = 9.81370913\n",
      "Iteration 390, loss = 9.78192707\n",
      "Iteration 391, loss = 9.75018528\n",
      "Iteration 392, loss = 9.71848468\n",
      "Iteration 393, loss = 9.68682608\n",
      "Iteration 394, loss = 9.65521020\n",
      "Iteration 395, loss = 9.62364056\n",
      "Iteration 396, loss = 9.59216514\n",
      "Iteration 397, loss = 9.56073334\n",
      "Iteration 398, loss = 9.52934567\n",
      "Iteration 399, loss = 9.49800261\n",
      "Iteration 400, loss = 9.46670458\n",
      "Iteration 401, loss = 9.43545198\n",
      "Iteration 402, loss = 9.40424517\n",
      "Iteration 403, loss = 9.37308448\n",
      "Iteration 404, loss = 9.34197024\n",
      "Iteration 405, loss = 9.31090274\n",
      "Iteration 406, loss = 9.27988226\n",
      "Iteration 407, loss = 9.24890908\n",
      "Iteration 408, loss = 9.21798346\n",
      "Iteration 409, loss = 9.18710563\n",
      "Iteration 410, loss = 9.15627586\n",
      "Iteration 411, loss = 9.12549436\n",
      "Iteration 412, loss = 9.09476137\n",
      "Iteration 413, loss = 9.06407712\n",
      "Iteration 414, loss = 9.03344183\n",
      "Iteration 415, loss = 9.00285572\n",
      "Iteration 416, loss = 8.97231900\n",
      "Iteration 417, loss = 8.94183188\n",
      "Iteration 418, loss = 8.91139458\n",
      "Iteration 419, loss = 8.88100732\n",
      "Iteration 420, loss = 8.85067029\n",
      "Iteration 421, loss = 8.82038370\n",
      "Iteration 422, loss = 8.79014778\n",
      "Iteration 423, loss = 8.75996271\n",
      "Iteration 424, loss = 8.72982871\n",
      "Iteration 425, loss = 8.69974599\n",
      "Iteration 426, loss = 8.66971474\n",
      "Iteration 427, loss = 8.63973518\n",
      "Iteration 428, loss = 8.60980751\n",
      "Iteration 429, loss = 8.57993193\n",
      "Iteration 430, loss = 8.55010864\n",
      "Iteration 431, loss = 8.52033785\n",
      "Iteration 432, loss = 8.49061977\n",
      "Iteration 433, loss = 8.46095458\n",
      "Iteration 434, loss = 8.43134250\n",
      "Iteration 435, loss = 8.40178373\n",
      "Iteration 436, loss = 8.37227846\n",
      "Iteration 437, loss = 8.34282690\n",
      "Iteration 438, loss = 8.31342924\n",
      "Iteration 439, loss = 8.28408568\n",
      "Iteration 440, loss = 8.25479642\n",
      "Iteration 441, loss = 8.22556167\n",
      "Iteration 442, loss = 8.19638160\n",
      "Iteration 443, loss = 8.16725642\n",
      "Iteration 444, loss = 8.13818632\n",
      "Iteration 445, loss = 8.10917150\n",
      "Iteration 446, loss = 8.08021214\n",
      "Iteration 447, loss = 8.05130844\n",
      "Iteration 448, loss = 8.02246059\n",
      "Iteration 449, loss = 7.99366878\n",
      "Iteration 450, loss = 7.96493318\n",
      "Iteration 451, loss = 7.93625400\n",
      "Iteration 452, loss = 7.90763141\n",
      "Iteration 453, loss = 7.87906560\n",
      "Iteration 454, loss = 7.85055674\n",
      "Iteration 455, loss = 7.82210503\n",
      "Iteration 456, loss = 7.79371064\n",
      "Iteration 457, loss = 7.76537375\n",
      "Iteration 458, loss = 7.73709453\n",
      "Iteration 459, loss = 7.70887316\n",
      "Iteration 460, loss = 7.68070981\n",
      "Iteration 461, loss = 7.65260466\n",
      "Iteration 462, loss = 7.62455787\n",
      "Iteration 463, loss = 7.59656962\n",
      "Iteration 464, loss = 7.56864007\n",
      "Iteration 465, loss = 7.54076938\n",
      "Iteration 466, loss = 7.51292243\n",
      "Iteration 467, loss = 7.48508240\n",
      "Iteration 468, loss = 7.45729380\n",
      "Iteration 469, loss = 7.42955764\n",
      "Iteration 470, loss = 7.40187485\n",
      "Iteration 471, loss = 7.37424629\n",
      "Iteration 472, loss = 7.34667271\n",
      "Iteration 473, loss = 7.31915484\n",
      "Iteration 474, loss = 7.29169334\n",
      "Iteration 475, loss = 7.26428879\n",
      "Iteration 476, loss = 7.23694175\n",
      "Iteration 477, loss = 7.20965274\n",
      "Iteration 478, loss = 7.18242222\n",
      "Iteration 479, loss = 7.15525063\n",
      "Iteration 480, loss = 7.12813837\n",
      "Iteration 481, loss = 7.10108582\n",
      "Iteration 482, loss = 7.07409334\n",
      "Iteration 483, loss = 7.04716123\n",
      "Iteration 484, loss = 7.02028982\n",
      "Iteration 485, loss = 6.99347938\n",
      "Iteration 486, loss = 6.96673019\n",
      "Iteration 487, loss = 6.94004248\n",
      "Iteration 488, loss = 6.91341651\n",
      "Iteration 489, loss = 6.88685249\n",
      "Iteration 490, loss = 6.86035064\n",
      "Iteration 491, loss = 6.83391115\n",
      "Iteration 492, loss = 6.80753421\n",
      "Iteration 493, loss = 6.78122001\n",
      "Iteration 494, loss = 6.75496871\n",
      "Iteration 495, loss = 6.72878048\n",
      "Iteration 496, loss = 6.70265547\n",
      "Iteration 497, loss = 6.67659384\n",
      "Iteration 498, loss = 6.65059572\n",
      "Iteration 499, loss = 6.62466125\n",
      "Iteration 500, loss = 6.59879056\n",
      "Iteration 501, loss = 6.57298377\n",
      "Iteration 502, loss = 6.54724102\n",
      "Iteration 503, loss = 6.52156241\n",
      "Iteration 504, loss = 6.49594805\n",
      "Iteration 505, loss = 6.47039805\n",
      "Iteration 506, loss = 6.44491252\n",
      "Iteration 507, loss = 6.41949156\n",
      "Iteration 508, loss = 6.39413526\n",
      "Iteration 509, loss = 6.36884371\n",
      "Iteration 510, loss = 6.34361700\n",
      "Iteration 511, loss = 6.31845523\n",
      "Iteration 512, loss = 6.29335847\n",
      "Iteration 513, loss = 6.26832680\n",
      "Iteration 514, loss = 6.24336030\n",
      "Iteration 515, loss = 6.21845905\n",
      "Iteration 516, loss = 6.19362311\n",
      "Iteration 517, loss = 6.16885256\n",
      "Iteration 518, loss = 6.14414747\n",
      "Iteration 519, loss = 6.11950789\n",
      "Iteration 520, loss = 6.09493389\n",
      "Iteration 521, loss = 6.07042553\n",
      "Iteration 522, loss = 6.04598287\n",
      "Iteration 523, loss = 6.02160595\n",
      "Iteration 524, loss = 5.99729484\n",
      "Iteration 525, loss = 5.97304957\n",
      "Iteration 526, loss = 5.94887021\n",
      "Iteration 527, loss = 5.92475680\n",
      "Iteration 528, loss = 5.90070937\n",
      "Iteration 529, loss = 5.87672797\n",
      "Iteration 530, loss = 5.85281265\n",
      "Iteration 531, loss = 5.82896342\n",
      "Iteration 532, loss = 5.80518034\n",
      "Iteration 533, loss = 5.78146343\n",
      "Iteration 534, loss = 5.75781273\n",
      "Iteration 535, loss = 5.73422826\n",
      "Iteration 536, loss = 5.71071005\n",
      "Iteration 537, loss = 5.68725812\n",
      "Iteration 538, loss = 5.66387249\n",
      "Iteration 539, loss = 5.64055320\n",
      "Iteration 540, loss = 5.61730025\n",
      "Iteration 541, loss = 5.59411366\n",
      "Iteration 542, loss = 5.57099345\n",
      "Iteration 543, loss = 5.54793963\n",
      "Iteration 544, loss = 5.52495221\n",
      "Iteration 545, loss = 5.50203120\n",
      "Iteration 546, loss = 5.47917660\n",
      "Iteration 547, loss = 5.45638842\n",
      "Iteration 548, loss = 5.43366667\n",
      "Iteration 549, loss = 5.41101135\n",
      "Iteration 550, loss = 5.38842245\n",
      "Iteration 551, loss = 5.36589997\n",
      "Iteration 552, loss = 5.34344391\n",
      "Iteration 553, loss = 5.32105427\n",
      "Iteration 554, loss = 5.29873103\n",
      "Iteration 555, loss = 5.27647418\n",
      "Iteration 556, loss = 5.25428372\n",
      "Iteration 557, loss = 5.23215963\n",
      "Iteration 558, loss = 5.21010189\n",
      "Iteration 559, loss = 5.18811050\n",
      "Iteration 560, loss = 5.16618542\n",
      "Iteration 561, loss = 5.14432665\n",
      "Iteration 562, loss = 5.12252986\n",
      "Iteration 563, loss = 5.10076569\n",
      "Iteration 564, loss = 5.07906197\n",
      "Iteration 565, loss = 5.05741920\n",
      "Iteration 566, loss = 5.03583783\n",
      "Iteration 567, loss = 5.01431824\n",
      "Iteration 568, loss = 4.99286078\n",
      "Iteration 569, loss = 4.97146577\n",
      "Iteration 570, loss = 4.95013348\n",
      "Iteration 571, loss = 4.92886415\n",
      "Iteration 572, loss = 4.90765800\n",
      "Iteration 573, loss = 4.88651520\n",
      "Iteration 574, loss = 4.86543592\n",
      "Iteration 575, loss = 4.84442031\n",
      "Iteration 576, loss = 4.82346847\n",
      "Iteration 577, loss = 4.80258053\n",
      "Iteration 578, loss = 4.78175655\n",
      "Iteration 579, loss = 4.76099663\n",
      "Iteration 580, loss = 4.74030081\n",
      "Iteration 581, loss = 4.71966915\n",
      "Iteration 582, loss = 4.69910167\n",
      "Iteration 583, loss = 4.67859842\n",
      "Iteration 584, loss = 4.65815940\n",
      "Iteration 585, loss = 4.63778464\n",
      "Iteration 586, loss = 4.61747412\n",
      "Iteration 587, loss = 4.59722784\n",
      "Iteration 588, loss = 4.57704580\n",
      "Iteration 589, loss = 4.55692798\n",
      "Iteration 590, loss = 4.53687435\n",
      "Iteration 591, loss = 4.51688489\n",
      "Iteration 592, loss = 4.49695957\n",
      "Iteration 593, loss = 4.47709836\n",
      "Iteration 594, loss = 4.45730120\n",
      "Iteration 595, loss = 4.43756807\n",
      "Iteration 596, loss = 4.41789891\n",
      "Iteration 597, loss = 4.39829368\n",
      "Iteration 598, loss = 4.37875232\n",
      "Iteration 599, loss = 4.35927478\n",
      "Iteration 600, loss = 4.33986101\n",
      "Iteration 601, loss = 4.32051093\n",
      "Iteration 602, loss = 4.30122450\n",
      "Iteration 603, loss = 4.28200164\n",
      "Iteration 604, loss = 4.26284229\n",
      "Iteration 605, loss = 4.24374639\n",
      "Iteration 606, loss = 4.22471386\n",
      "Iteration 607, loss = 4.20574463\n",
      "Iteration 608, loss = 4.18683864\n",
      "Iteration 609, loss = 4.16799580\n",
      "Iteration 610, loss = 4.14921604\n",
      "Iteration 611, loss = 4.13049928\n",
      "Iteration 612, loss = 4.11184545\n",
      "Iteration 613, loss = 4.09325446\n",
      "Iteration 614, loss = 4.07472623\n",
      "Iteration 615, loss = 4.05626069\n",
      "Iteration 616, loss = 4.03785775\n",
      "Iteration 617, loss = 4.01951732\n",
      "Iteration 618, loss = 4.00123931\n",
      "Iteration 619, loss = 3.98302365\n",
      "Iteration 620, loss = 3.96487024\n",
      "Iteration 621, loss = 3.94677900\n",
      "Iteration 622, loss = 3.92874983\n",
      "Iteration 623, loss = 3.91078265\n",
      "Iteration 624, loss = 3.89287736\n",
      "Iteration 625, loss = 3.87503387\n",
      "Iteration 626, loss = 3.85725210\n",
      "Iteration 627, loss = 3.83953193\n",
      "Iteration 628, loss = 3.82187328\n",
      "Iteration 629, loss = 3.80427606\n",
      "Iteration 630, loss = 3.78674017\n",
      "Iteration 631, loss = 3.76926550\n",
      "Iteration 632, loss = 3.75185197\n",
      "Iteration 633, loss = 3.73449947\n",
      "Iteration 634, loss = 3.71720790\n",
      "Iteration 635, loss = 3.69997717\n",
      "Iteration 636, loss = 3.68280717\n",
      "Iteration 637, loss = 3.66569780\n",
      "Iteration 638, loss = 3.64864896\n",
      "Iteration 639, loss = 3.63166055\n",
      "Iteration 640, loss = 3.61473246\n",
      "Iteration 641, loss = 3.59786460\n",
      "Iteration 642, loss = 3.58105684\n",
      "Iteration 643, loss = 3.56430910\n",
      "Iteration 644, loss = 3.54762126\n",
      "Iteration 645, loss = 3.53099322\n",
      "Iteration 646, loss = 3.51442487\n",
      "Iteration 647, loss = 3.49791611\n",
      "Iteration 648, loss = 3.48146682\n",
      "Iteration 649, loss = 3.46507690\n",
      "Iteration 650, loss = 3.44874624\n",
      "Iteration 651, loss = 3.43247472\n",
      "Iteration 652, loss = 3.41626225\n",
      "Iteration 653, loss = 3.40010871\n",
      "Iteration 654, loss = 3.38401398\n",
      "Iteration 655, loss = 3.36797796\n",
      "Iteration 656, loss = 3.35200054\n",
      "Iteration 657, loss = 3.33608160\n",
      "Iteration 658, loss = 3.32022104\n",
      "Iteration 659, loss = 3.30441873\n",
      "Iteration 660, loss = 3.28867457\n",
      "Iteration 661, loss = 3.27298844\n",
      "Iteration 662, loss = 3.25736023\n",
      "Iteration 663, loss = 3.24178982\n",
      "Iteration 664, loss = 3.22627710\n",
      "Iteration 665, loss = 3.21082195\n",
      "Iteration 666, loss = 3.19542427\n",
      "Iteration 667, loss = 3.18008392\n",
      "Iteration 668, loss = 3.16480080\n",
      "Iteration 669, loss = 3.14957480\n",
      "Iteration 670, loss = 3.13440579\n",
      "Iteration 671, loss = 3.11929365\n",
      "Iteration 672, loss = 3.10423828\n",
      "Iteration 673, loss = 3.08923954\n",
      "Iteration 674, loss = 3.07429733\n",
      "Iteration 675, loss = 3.05941153\n",
      "Iteration 676, loss = 3.04458202\n",
      "Iteration 677, loss = 3.02980868\n",
      "Iteration 678, loss = 3.01509139\n",
      "Iteration 679, loss = 3.00035282\n",
      "Iteration 680, loss = 2.98564497\n",
      "Iteration 681, loss = 2.97098086\n",
      "Iteration 682, loss = 2.95636156\n",
      "Iteration 683, loss = 2.94178803\n",
      "Iteration 684, loss = 2.92726114\n",
      "Iteration 685, loss = 2.91278165\n",
      "Iteration 686, loss = 2.89835023\n",
      "Iteration 687, loss = 2.88396748\n",
      "Iteration 688, loss = 2.86963392\n",
      "Iteration 689, loss = 2.85535003\n",
      "Iteration 690, loss = 2.84104458\n",
      "Iteration 691, loss = 2.82663553\n",
      "Iteration 692, loss = 2.81225029\n",
      "Iteration 693, loss = 2.79789191\n",
      "Iteration 694, loss = 2.78356313\n",
      "Iteration 695, loss = 2.76926644\n",
      "Iteration 696, loss = 2.75500405\n",
      "Iteration 697, loss = 2.74077796\n",
      "Iteration 698, loss = 2.72658997\n",
      "Iteration 699, loss = 2.71242526\n",
      "Iteration 700, loss = 2.69809511\n",
      "Iteration 701, loss = 2.68378114\n",
      "Iteration 702, loss = 2.66948753\n",
      "Iteration 703, loss = 2.65521806\n",
      "Iteration 704, loss = 2.64097612\n",
      "Iteration 705, loss = 2.62676477\n",
      "Iteration 706, loss = 2.61258674\n",
      "Iteration 707, loss = 2.59844451\n",
      "Iteration 708, loss = 2.58434029\n",
      "Iteration 709, loss = 2.57027606\n",
      "Iteration 710, loss = 2.55625359\n",
      "Iteration 711, loss = 2.54227449\n",
      "Iteration 712, loss = 2.52834018\n",
      "Iteration 713, loss = 2.51445193\n",
      "Iteration 714, loss = 2.50061090\n",
      "Iteration 715, loss = 2.48681809\n",
      "Iteration 716, loss = 2.47307441\n",
      "Iteration 717, loss = 2.45938068\n",
      "Iteration 718, loss = 2.44573761\n",
      "Iteration 719, loss = 2.43214585\n",
      "Iteration 720, loss = 2.41860595\n",
      "Iteration 721, loss = 2.40511843\n",
      "Iteration 722, loss = 2.39168371\n",
      "Iteration 723, loss = 2.37830220\n",
      "Iteration 724, loss = 2.36497423\n",
      "Iteration 725, loss = 2.35170009\n",
      "Iteration 726, loss = 2.33848005\n",
      "Iteration 727, loss = 2.32531433\n",
      "Iteration 728, loss = 2.31220312\n",
      "Iteration 729, loss = 2.29914659\n",
      "Iteration 730, loss = 2.28614486\n",
      "Iteration 731, loss = 2.27319805\n",
      "Iteration 732, loss = 2.26030626\n",
      "Iteration 733, loss = 2.24746956\n",
      "Iteration 734, loss = 2.23468800\n",
      "Iteration 735, loss = 2.22196162\n",
      "Iteration 736, loss = 2.20929046\n",
      "Iteration 737, loss = 2.19667452\n",
      "Iteration 738, loss = 2.18411381\n",
      "Iteration 739, loss = 2.17160831\n",
      "Iteration 740, loss = 2.15915801\n",
      "Iteration 741, loss = 2.14676289\n",
      "Iteration 742, loss = 2.13442291\n",
      "Iteration 743, loss = 2.12213802\n",
      "Iteration 744, loss = 2.10990818\n",
      "Iteration 745, loss = 2.09773334\n",
      "Iteration 746, loss = 2.08561343\n",
      "Iteration 747, loss = 2.07354840\n",
      "Iteration 748, loss = 2.06153816\n",
      "Iteration 749, loss = 2.04958266\n",
      "Iteration 750, loss = 2.03768181\n",
      "Iteration 751, loss = 2.02583553\n",
      "Iteration 752, loss = 2.01404375\n",
      "Iteration 753, loss = 2.00230636\n",
      "Iteration 754, loss = 1.99062329\n",
      "Iteration 755, loss = 1.97899445\n",
      "Iteration 756, loss = 1.96741973\n",
      "Iteration 757, loss = 1.95589905\n",
      "Iteration 758, loss = 1.94443230\n",
      "Iteration 759, loss = 1.93301939\n",
      "Iteration 760, loss = 1.92166021\n",
      "Iteration 761, loss = 1.91035466\n",
      "Iteration 762, loss = 1.89896807\n",
      "Iteration 763, loss = 1.88743753\n",
      "Iteration 764, loss = 1.87593076\n",
      "Iteration 765, loss = 1.86445140\n",
      "Iteration 766, loss = 1.85300271\n",
      "Iteration 767, loss = 1.84158762\n",
      "Iteration 768, loss = 1.83020871\n",
      "Iteration 769, loss = 1.81880312\n",
      "Iteration 770, loss = 1.80707489\n",
      "Iteration 771, loss = 1.79534547\n",
      "Iteration 772, loss = 1.78362227\n",
      "Iteration 773, loss = 1.77191192\n",
      "Iteration 774, loss = 1.76022031\n",
      "Iteration 775, loss = 1.74855272\n",
      "Iteration 776, loss = 1.73691386\n",
      "Iteration 777, loss = 1.72530791\n",
      "Iteration 778, loss = 1.71373858\n",
      "Iteration 779, loss = 1.70220920\n",
      "Iteration 780, loss = 1.69072268\n",
      "Iteration 781, loss = 1.67928164\n",
      "Iteration 782, loss = 1.66788836\n",
      "Iteration 783, loss = 1.65654489\n",
      "Iteration 784, loss = 1.64525299\n",
      "Iteration 785, loss = 1.63401425\n",
      "Iteration 786, loss = 1.62283004\n",
      "Iteration 787, loss = 1.61170155\n",
      "Iteration 788, loss = 1.60062984\n",
      "Iteration 789, loss = 1.58961580\n",
      "Iteration 790, loss = 1.57866022\n",
      "Iteration 791, loss = 1.56776375\n",
      "Iteration 792, loss = 1.55692697\n",
      "Iteration 793, loss = 1.54615036\n",
      "Iteration 794, loss = 1.53543430\n",
      "Iteration 795, loss = 1.52477911\n",
      "Iteration 796, loss = 1.51418507\n",
      "Iteration 797, loss = 1.50365236\n",
      "Iteration 798, loss = 1.49318114\n",
      "Iteration 799, loss = 1.48277150\n",
      "Iteration 800, loss = 1.47242352\n",
      "Iteration 801, loss = 1.46213722\n",
      "Iteration 802, loss = 1.45191258\n",
      "Iteration 803, loss = 1.44174958\n",
      "Iteration 804, loss = 1.43164814\n",
      "Iteration 805, loss = 1.42160819\n",
      "Iteration 806, loss = 1.41162962\n",
      "Iteration 807, loss = 1.40171229\n",
      "Iteration 808, loss = 1.39185608\n",
      "Iteration 809, loss = 1.38206082\n",
      "Iteration 810, loss = 1.37232635\n",
      "Iteration 811, loss = 1.36265248\n",
      "Iteration 812, loss = 1.35303902\n",
      "Iteration 813, loss = 1.34348579\n",
      "Iteration 814, loss = 1.33399256\n",
      "Iteration 815, loss = 1.32455913\n",
      "Iteration 816, loss = 1.31518527\n",
      "Iteration 817, loss = 1.30587077\n",
      "Iteration 818, loss = 1.29661538\n",
      "Iteration 819, loss = 1.28741889\n",
      "Iteration 820, loss = 1.27828104\n",
      "Iteration 821, loss = 1.26920160\n",
      "Iteration 822, loss = 1.26018033\n",
      "Iteration 823, loss = 1.25121697\n",
      "Iteration 824, loss = 1.24231130\n",
      "Iteration 825, loss = 1.23346304\n",
      "Iteration 826, loss = 1.22467196\n",
      "Iteration 827, loss = 1.21593780\n",
      "Iteration 828, loss = 1.20726031\n",
      "Iteration 829, loss = 1.19863925\n",
      "Iteration 830, loss = 1.19007434\n",
      "Iteration 831, loss = 1.18156535\n",
      "Iteration 832, loss = 1.17311201\n",
      "Iteration 833, loss = 1.16471407\n",
      "Iteration 834, loss = 1.15637128\n",
      "Iteration 835, loss = 1.14808339\n",
      "Iteration 836, loss = 1.13985013\n",
      "Iteration 837, loss = 1.13167126\n",
      "Iteration 838, loss = 1.12354652\n",
      "Iteration 839, loss = 1.11547566\n",
      "Iteration 840, loss = 1.10745842\n",
      "Iteration 841, loss = 1.09949456\n",
      "Iteration 842, loss = 1.09158382\n",
      "Iteration 843, loss = 1.08372594\n",
      "Iteration 844, loss = 1.07592069\n",
      "Iteration 845, loss = 1.06816780\n",
      "Iteration 846, loss = 1.06046702\n",
      "Iteration 847, loss = 1.05281811\n",
      "Iteration 848, loss = 1.04522082\n",
      "Iteration 849, loss = 1.03767490\n",
      "Iteration 850, loss = 1.03012768\n",
      "Iteration 851, loss = 1.02259762\n",
      "Iteration 852, loss = 1.01511011\n",
      "Iteration 853, loss = 1.00766592\n",
      "Iteration 854, loss = 1.00026573\n",
      "Iteration 855, loss = 0.99291013\n",
      "Iteration 856, loss = 0.98559961\n",
      "Iteration 857, loss = 0.97833461\n",
      "Iteration 858, loss = 0.97111547\n",
      "Iteration 859, loss = 0.96394251\n",
      "Iteration 860, loss = 0.95681596\n",
      "Iteration 861, loss = 0.94971915\n",
      "Iteration 862, loss = 0.94263007\n",
      "Iteration 863, loss = 0.93558307\n",
      "Iteration 864, loss = 0.92857893\n",
      "Iteration 865, loss = 0.92161834\n",
      "Iteration 866, loss = 0.91470188\n",
      "Iteration 867, loss = 0.90783006\n",
      "Iteration 868, loss = 0.90100329\n",
      "Iteration 869, loss = 0.89422193\n",
      "Iteration 870, loss = 0.88748627\n",
      "Iteration 871, loss = 0.88079656\n",
      "Iteration 872, loss = 0.87415297\n",
      "Iteration 873, loss = 0.86755565\n",
      "Iteration 874, loss = 0.86100469\n",
      "Iteration 875, loss = 0.85450016\n",
      "Iteration 876, loss = 0.84804209\n",
      "Iteration 877, loss = 0.84163047\n",
      "Iteration 878, loss = 0.83526529\n",
      "Iteration 879, loss = 0.82894649\n",
      "Iteration 880, loss = 0.82267400\n",
      "Iteration 881, loss = 0.81644772\n",
      "Iteration 882, loss = 0.81026756\n",
      "Iteration 883, loss = 0.80413339\n",
      "Iteration 884, loss = 0.79804507\n",
      "Iteration 885, loss = 0.79200244\n",
      "Iteration 886, loss = 0.78600536\n",
      "Iteration 887, loss = 0.78005366\n",
      "Iteration 888, loss = 0.77414714\n",
      "Iteration 889, loss = 0.76828563\n",
      "Iteration 890, loss = 0.76246893\n",
      "Iteration 891, loss = 0.75669684\n",
      "Iteration 892, loss = 0.75096916\n",
      "Iteration 893, loss = 0.74528567\n",
      "Iteration 894, loss = 0.73964617\n",
      "Iteration 895, loss = 0.73405043\n",
      "Iteration 896, loss = 0.72849824\n",
      "Iteration 897, loss = 0.72298936\n",
      "Iteration 898, loss = 0.71752357\n",
      "Iteration 899, loss = 0.71210064\n",
      "Iteration 900, loss = 0.70672035\n",
      "Iteration 901, loss = 0.70138245\n",
      "Iteration 902, loss = 0.69608672\n",
      "Iteration 903, loss = 0.69083291\n",
      "Iteration 904, loss = 0.68562080\n",
      "Iteration 905, loss = 0.68045014\n",
      "Iteration 906, loss = 0.67532070\n",
      "Iteration 907, loss = 0.67023224\n",
      "Iteration 908, loss = 0.66518451\n",
      "Iteration 909, loss = 0.66017730\n",
      "Iteration 910, loss = 0.65521034\n",
      "Iteration 911, loss = 0.65028341\n",
      "Iteration 912, loss = 0.64539626\n",
      "Iteration 913, loss = 0.64054866\n",
      "Iteration 914, loss = 0.63574037\n",
      "Iteration 915, loss = 0.63097114\n",
      "Iteration 916, loss = 0.62624075\n",
      "Iteration 917, loss = 0.62154895\n",
      "Iteration 918, loss = 0.61689550\n",
      "Iteration 919, loss = 0.61228017\n",
      "Iteration 920, loss = 0.60770273\n",
      "Iteration 921, loss = 0.60316293\n",
      "Iteration 922, loss = 0.59866055\n",
      "Iteration 923, loss = 0.59419534\n",
      "Iteration 924, loss = 0.58976707\n",
      "Iteration 925, loss = 0.58537551\n",
      "Iteration 926, loss = 0.58102042\n",
      "Iteration 927, loss = 0.57670158\n",
      "Iteration 928, loss = 0.57241874\n",
      "Iteration 929, loss = 0.56817169\n",
      "Iteration 930, loss = 0.56396019\n",
      "Iteration 931, loss = 0.55978400\n",
      "Iteration 932, loss = 0.55564291\n",
      "Iteration 933, loss = 0.55153668\n",
      "Iteration 934, loss = 0.54746509\n",
      "Iteration 935, loss = 0.54342790\n",
      "Iteration 936, loss = 0.53942490\n",
      "Iteration 937, loss = 0.53545586\n",
      "Iteration 938, loss = 0.53152055\n",
      "Iteration 939, loss = 0.52761874\n",
      "Iteration 940, loss = 0.52375023\n",
      "Iteration 941, loss = 0.51991478\n",
      "Iteration 942, loss = 0.51611217\n",
      "Iteration 943, loss = 0.51234219\n",
      "Iteration 944, loss = 0.50860461\n",
      "Iteration 945, loss = 0.50489921\n",
      "Iteration 946, loss = 0.50122579\n",
      "Iteration 947, loss = 0.49758411\n",
      "Iteration 948, loss = 0.49397396\n",
      "Iteration 949, loss = 0.49039513\n",
      "Iteration 950, loss = 0.48684741\n",
      "Iteration 951, loss = 0.48333057\n",
      "Iteration 952, loss = 0.47984441\n",
      "Iteration 953, loss = 0.47638871\n",
      "Iteration 954, loss = 0.47296327\n",
      "Iteration 955, loss = 0.46956787\n",
      "Iteration 956, loss = 0.46620230\n",
      "Iteration 957, loss = 0.46286635\n",
      "Iteration 958, loss = 0.45955982\n",
      "Iteration 959, loss = 0.45628250\n",
      "Iteration 960, loss = 0.45303417\n",
      "Iteration 961, loss = 0.44981465\n",
      "Iteration 962, loss = 0.44662371\n",
      "Iteration 963, loss = 0.44346117\n",
      "Iteration 964, loss = 0.44032681\n",
      "Iteration 965, loss = 0.43722043\n",
      "Iteration 966, loss = 0.43413559\n",
      "Iteration 967, loss = 0.43103861\n",
      "Iteration 968, loss = 0.42796552\n",
      "Iteration 969, loss = 0.42491665\n",
      "Iteration 970, loss = 0.42189229\n",
      "Iteration 971, loss = 0.41889268\n",
      "Iteration 972, loss = 0.41591800\n",
      "Iteration 973, loss = 0.41296839\n",
      "Iteration 974, loss = 0.41004398\n",
      "Iteration 975, loss = 0.40714483\n",
      "Iteration 976, loss = 0.40427102\n",
      "Iteration 977, loss = 0.40142255\n",
      "Iteration 978, loss = 0.39859943\n",
      "Iteration 979, loss = 0.39580165\n",
      "Iteration 980, loss = 0.39302917\n",
      "Iteration 981, loss = 0.39026820\n",
      "Iteration 982, loss = 0.38747985\n",
      "Iteration 983, loss = 0.38471086\n",
      "Iteration 984, loss = 0.38196197\n",
      "Iteration 985, loss = 0.37923383\n",
      "Iteration 986, loss = 0.37652698\n",
      "Iteration 987, loss = 0.37384190\n",
      "Iteration 988, loss = 0.37117899\n",
      "Iteration 989, loss = 0.36853861\n",
      "Iteration 990, loss = 0.36592102\n",
      "Iteration 991, loss = 0.36332647\n",
      "Iteration 992, loss = 0.36075514\n",
      "Iteration 993, loss = 0.35820718\n",
      "Iteration 994, loss = 0.35568270\n",
      "Iteration 995, loss = 0.35318178\n",
      "Iteration 996, loss = 0.35070445\n",
      "Iteration 997, loss = 0.34825075\n",
      "Iteration 998, loss = 0.34582067\n",
      "Iteration 999, loss = 0.34341417\n",
      "Iteration 1000, loss = 0.34103123\n",
      "Iteration 1001, loss = 0.33867177\n",
      "Iteration 1002, loss = 0.33633572\n",
      "Iteration 1003, loss = 0.33402300\n",
      "Iteration 1004, loss = 0.33173349\n",
      "Iteration 1005, loss = 0.32946708\n",
      "Iteration 1006, loss = 0.32722366\n",
      "Iteration 1007, loss = 0.32500309\n",
      "Iteration 1008, loss = 0.32280523\n",
      "Iteration 1009, loss = 0.32062995\n",
      "Iteration 1010, loss = 0.31847708\n",
      "Iteration 1011, loss = 0.31634648\n",
      "Iteration 1012, loss = 0.31423798\n",
      "Iteration 1013, loss = 0.31215143\n",
      "Iteration 1014, loss = 0.31008665\n",
      "Iteration 1015, loss = 0.30804347\n",
      "Iteration 1016, loss = 0.30602173\n",
      "Iteration 1017, loss = 0.30402126\n",
      "Iteration 1018, loss = 0.30204186\n",
      "Iteration 1019, loss = 0.30008338\n",
      "Iteration 1020, loss = 0.29814563\n",
      "Iteration 1021, loss = 0.29622843\n",
      "Iteration 1022, loss = 0.29433160\n",
      "Iteration 1023, loss = 0.29245497\n",
      "Iteration 1024, loss = 0.29059836\n",
      "Iteration 1025, loss = 0.28876158\n",
      "Iteration 1026, loss = 0.28694446\n",
      "Iteration 1027, loss = 0.28514682\n",
      "Iteration 1028, loss = 0.28336847\n",
      "Iteration 1029, loss = 0.28160924\n",
      "Iteration 1030, loss = 0.27986896\n",
      "Iteration 1031, loss = 0.27814744\n",
      "Iteration 1032, loss = 0.27644451\n",
      "Iteration 1033, loss = 0.27475999\n",
      "Iteration 1034, loss = 0.27309371\n",
      "Iteration 1035, loss = 0.27144550\n",
      "Iteration 1036, loss = 0.26981517\n",
      "Iteration 1037, loss = 0.26820256\n",
      "Iteration 1038, loss = 0.26660750\n",
      "Iteration 1039, loss = 0.26502981\n",
      "Iteration 1040, loss = 0.26346933\n",
      "Iteration 1041, loss = 0.26192589\n",
      "Iteration 1042, loss = 0.26039932\n",
      "Iteration 1043, loss = 0.25888946\n",
      "Iteration 1044, loss = 0.25739614\n",
      "Iteration 1045, loss = 0.25591920\n",
      "Iteration 1046, loss = 0.25445847\n",
      "Iteration 1047, loss = 0.25301380\n",
      "Iteration 1048, loss = 0.25158502\n",
      "Iteration 1049, loss = 0.25017198\n",
      "Iteration 1050, loss = 0.24877451\n",
      "Iteration 1051, loss = 0.24739246\n",
      "Iteration 1052, loss = 0.24602568\n",
      "Iteration 1053, loss = 0.24467401\n",
      "Iteration 1054, loss = 0.24333730\n",
      "Iteration 1055, loss = 0.24201539\n",
      "Iteration 1056, loss = 0.24070814\n",
      "Iteration 1057, loss = 0.23941539\n",
      "Iteration 1058, loss = 0.23813701\n",
      "Iteration 1059, loss = 0.23687283\n",
      "Iteration 1060, loss = 0.23562271\n",
      "Iteration 1061, loss = 0.23438652\n",
      "Iteration 1062, loss = 0.23316411\n",
      "Iteration 1063, loss = 0.23195532\n",
      "Iteration 1064, loss = 0.23076003\n",
      "Iteration 1065, loss = 0.22957810\n",
      "Iteration 1066, loss = 0.22840938\n",
      "Iteration 1067, loss = 0.22725373\n",
      "Iteration 1068, loss = 0.22611103\n",
      "Iteration 1069, loss = 0.22498112\n",
      "Iteration 1070, loss = 0.22386389\n",
      "Iteration 1071, loss = 0.22275920\n",
      "Iteration 1072, loss = 0.22166691\n",
      "Iteration 1073, loss = 0.22058690\n",
      "Iteration 1074, loss = 0.21951903\n",
      "Iteration 1075, loss = 0.21846318\n",
      "Iteration 1076, loss = 0.21741922\n",
      "Iteration 1077, loss = 0.21638702\n",
      "Iteration 1078, loss = 0.21536645\n",
      "Iteration 1079, loss = 0.21435740\n",
      "Iteration 1080, loss = 0.21335974\n",
      "Iteration 1081, loss = 0.21237335\n",
      "Iteration 1082, loss = 0.21139811\n",
      "Iteration 1083, loss = 0.21043389\n",
      "Iteration 1084, loss = 0.20948058\n",
      "Iteration 1085, loss = 0.20853806\n",
      "Iteration 1086, loss = 0.20760622\n",
      "Iteration 1087, loss = 0.20668493\n",
      "Iteration 1088, loss = 0.20577409\n",
      "Iteration 1089, loss = 0.20487357\n",
      "Iteration 1090, loss = 0.20398328\n",
      "Iteration 1091, loss = 0.20310309\n",
      "Iteration 1092, loss = 0.20223290\n",
      "Iteration 1093, loss = 0.20137259\n",
      "Iteration 1094, loss = 0.20052206\n",
      "Iteration 1095, loss = 0.19968120\n",
      "Iteration 1096, loss = 0.19884991\n",
      "Iteration 1097, loss = 0.19802807\n",
      "Iteration 1098, loss = 0.19721559\n",
      "Iteration 1099, loss = 0.19641235\n",
      "Iteration 1100, loss = 0.19561827\n",
      "Iteration 1101, loss = 0.19483323\n",
      "Iteration 1102, loss = 0.19405713\n",
      "Iteration 1103, loss = 0.19328988\n",
      "Iteration 1104, loss = 0.19253138\n",
      "Iteration 1105, loss = 0.19178152\n",
      "Iteration 1106, loss = 0.19104022\n",
      "Iteration 1107, loss = 0.19030737\n",
      "Iteration 1108, loss = 0.18958288\n",
      "Iteration 1109, loss = 0.18886666\n",
      "Iteration 1110, loss = 0.18815861\n",
      "Iteration 1111, loss = 0.18745864\n",
      "Iteration 1112, loss = 0.18676665\n",
      "Iteration 1113, loss = 0.18608257\n",
      "Iteration 1114, loss = 0.18540629\n",
      "Iteration 1115, loss = 0.18473774\n",
      "Iteration 1116, loss = 0.18407681\n",
      "Iteration 1117, loss = 0.18342343\n",
      "Iteration 1118, loss = 0.18277750\n",
      "Iteration 1119, loss = 0.18213895\n",
      "Iteration 1120, loss = 0.18150769\n",
      "Iteration 1121, loss = 0.18088362\n",
      "Iteration 1122, loss = 0.18026668\n",
      "Iteration 1123, loss = 0.17965678\n",
      "Iteration 1124, loss = 0.17905383\n",
      "Iteration 1125, loss = 0.17845776\n",
      "Iteration 1126, loss = 0.17786849\n",
      "Iteration 1127, loss = 0.17728593\n",
      "Iteration 1128, loss = 0.17671001\n",
      "Iteration 1129, loss = 0.17614066\n",
      "Iteration 1130, loss = 0.17557779\n",
      "Iteration 1131, loss = 0.17502132\n",
      "Iteration 1132, loss = 0.17447120\n",
      "Iteration 1133, loss = 0.17392733\n",
      "Iteration 1134, loss = 0.17338965\n",
      "Iteration 1135, loss = 0.17285808\n",
      "Iteration 1136, loss = 0.17233255\n",
      "Iteration 1137, loss = 0.17181299\n",
      "Iteration 1138, loss = 0.17129934\n",
      "Iteration 1139, loss = 0.17079151\n",
      "Iteration 1140, loss = 0.17028944\n",
      "Iteration 1141, loss = 0.16979306\n",
      "Iteration 1142, loss = 0.16930231\n",
      "Iteration 1143, loss = 0.16881711\n",
      "Iteration 1144, loss = 0.16833741\n",
      "Iteration 1145, loss = 0.16786313\n",
      "Iteration 1146, loss = 0.16739421\n",
      "Iteration 1147, loss = 0.16693058\n",
      "Iteration 1148, loss = 0.16647219\n",
      "Iteration 1149, loss = 0.16601897\n",
      "Iteration 1150, loss = 0.16557085\n",
      "Iteration 1151, loss = 0.16512778\n",
      "Iteration 1152, loss = 0.16468969\n",
      "Iteration 1153, loss = 0.16425653\n",
      "Iteration 1154, loss = 0.16382823\n",
      "Iteration 1155, loss = 0.16340474\n",
      "Iteration 1156, loss = 0.16298599\n",
      "Iteration 1157, loss = 0.16257193\n",
      "Iteration 1158, loss = 0.16216250\n",
      "Iteration 1159, loss = 0.16175765\n",
      "Iteration 1160, loss = 0.16135732\n",
      "Iteration 1161, loss = 0.16096145\n",
      "Iteration 1162, loss = 0.16056999\n",
      "Iteration 1163, loss = 0.16018289\n",
      "Iteration 1164, loss = 0.15980008\n",
      "Iteration 1165, loss = 0.15942153\n",
      "Iteration 1166, loss = 0.15904717\n",
      "Iteration 1167, loss = 0.15867696\n",
      "Iteration 1168, loss = 0.15831084\n",
      "Iteration 1169, loss = 0.15794876\n",
      "Iteration 1170, loss = 0.15759067\n",
      "Iteration 1171, loss = 0.15723653\n",
      "Iteration 1172, loss = 0.15688628\n",
      "Iteration 1173, loss = 0.15653987\n",
      "Iteration 1174, loss = 0.15619727\n",
      "Iteration 1175, loss = 0.15585841\n",
      "Iteration 1176, loss = 0.15552325\n",
      "Iteration 1177, loss = 0.15519175\n",
      "Iteration 1178, loss = 0.15486387\n",
      "Iteration 1179, loss = 0.15453954\n",
      "Iteration 1180, loss = 0.15421874\n",
      "Iteration 1181, loss = 0.15390142\n",
      "Iteration 1182, loss = 0.15358752\n",
      "Iteration 1183, loss = 0.15327702\n",
      "Iteration 1184, loss = 0.15296986\n",
      "Iteration 1185, loss = 0.15266601\n",
      "Iteration 1186, loss = 0.15236542\n",
      "Iteration 1187, loss = 0.15206804\n",
      "Iteration 1188, loss = 0.15177385\n",
      "Iteration 1189, loss = 0.15148280\n",
      "Iteration 1190, loss = 0.15119485\n",
      "Iteration 1191, loss = 0.15090996\n",
      "Iteration 1192, loss = 0.15062809\n",
      "Iteration 1193, loss = 0.15034920\n",
      "Iteration 1194, loss = 0.15007326\n",
      "Iteration 1195, loss = 0.14980022\n",
      "Iteration 1196, loss = 0.14953005\n",
      "Iteration 1197, loss = 0.14926271\n",
      "Iteration 1198, loss = 0.14899818\n",
      "Iteration 1199, loss = 0.14873640\n",
      "Iteration 1200, loss = 0.14847734\n",
      "Iteration 1201, loss = 0.14822097\n",
      "Iteration 1202, loss = 0.14796726\n",
      "Iteration 1203, loss = 0.14771617\n",
      "Iteration 1204, loss = 0.14746767\n",
      "Iteration 1205, loss = 0.14722172\n",
      "Iteration 1206, loss = 0.14697829\n",
      "Iteration 1207, loss = 0.14673734\n",
      "Iteration 1208, loss = 0.14649885\n",
      "Iteration 1209, loss = 0.14626279\n",
      "Iteration 1210, loss = 0.14602911\n",
      "Iteration 1211, loss = 0.14579780\n",
      "Iteration 1212, loss = 0.14556881\n",
      "Iteration 1213, loss = 0.14534212\n",
      "Iteration 1214, loss = 0.14511770\n",
      "Iteration 1215, loss = 0.14489553\n",
      "Iteration 1216, loss = 0.14467556\n",
      "Iteration 1217, loss = 0.14445777\n",
      "Iteration 1218, loss = 0.14424213\n",
      "Iteration 1219, loss = 0.14402862\n",
      "Iteration 1220, loss = 0.14381720\n",
      "Iteration 1221, loss = 0.14360785\n",
      "Iteration 1222, loss = 0.14340054\n",
      "Iteration 1223, loss = 0.14319525\n",
      "Iteration 1224, loss = 0.14299194\n",
      "Iteration 1225, loss = 0.14279060\n",
      "Iteration 1226, loss = 0.14259119\n",
      "Iteration 1227, loss = 0.14239369\n",
      "Iteration 1228, loss = 0.14219807\n",
      "Iteration 1229, loss = 0.14200431\n",
      "Iteration 1230, loss = 0.14181239\n",
      "Iteration 1231, loss = 0.14162227\n",
      "Iteration 1232, loss = 0.14143395\n",
      "Iteration 1233, loss = 0.14124738\n",
      "Iteration 1234, loss = 0.14106255\n",
      "Iteration 1235, loss = 0.14087944\n",
      "Iteration 1236, loss = 0.14069802\n",
      "Iteration 1237, loss = 0.14051827\n",
      "Iteration 1238, loss = 0.14034017\n",
      "Iteration 1239, loss = 0.14016370\n",
      "Iteration 1240, loss = 0.13998882\n",
      "Iteration 1241, loss = 0.13981553\n",
      "Iteration 1242, loss = 0.13964380\n",
      "Iteration 1243, loss = 0.13947361\n",
      "Iteration 1244, loss = 0.13930494\n",
      "Iteration 1245, loss = 0.13913777\n",
      "Iteration 1246, loss = 0.13897208\n",
      "Iteration 1247, loss = 0.13880784\n",
      "Iteration 1248, loss = 0.13864505\n",
      "Iteration 1249, loss = 0.13848367\n",
      "Iteration 1250, loss = 0.13832369\n",
      "Iteration 1251, loss = 0.13816510\n",
      "Iteration 1252, loss = 0.13800787\n",
      "Iteration 1253, loss = 0.13785198\n",
      "Iteration 1254, loss = 0.13769742\n",
      "Iteration 1255, loss = 0.13754416\n",
      "Iteration 1256, loss = 0.13739220\n",
      "Iteration 1257, loss = 0.13724151\n",
      "Iteration 1258, loss = 0.13709207\n",
      "Iteration 1259, loss = 0.13694388\n",
      "Iteration 1260, loss = 0.13679690\n",
      "Iteration 1261, loss = 0.13665113\n",
      "Iteration 1262, loss = 0.13650655\n",
      "Iteration 1263, loss = 0.13636314\n",
      "Iteration 1264, loss = 0.13622089\n",
      "Iteration 1265, loss = 0.13607977\n",
      "Iteration 1266, loss = 0.13593979\n",
      "Iteration 1267, loss = 0.13580091\n",
      "Iteration 1268, loss = 0.13566313\n",
      "Iteration 1269, loss = 0.13552642\n",
      "Iteration 1270, loss = 0.13539078\n",
      "Iteration 1271, loss = 0.13525619\n",
      "Iteration 1272, loss = 0.13512264\n",
      "Iteration 1273, loss = 0.13499011\n",
      "Iteration 1274, loss = 0.13485859\n",
      "Iteration 1275, loss = 0.13472806\n",
      "Iteration 1276, loss = 0.13459851\n",
      "Iteration 1277, loss = 0.13446992\n",
      "Iteration 1278, loss = 0.13434229\n",
      "Iteration 1279, loss = 0.13421561\n",
      "Iteration 1280, loss = 0.13408984\n",
      "Iteration 1281, loss = 0.13396500\n",
      "Iteration 1282, loss = 0.13384105\n",
      "Iteration 1283, loss = 0.13371800\n",
      "Iteration 1284, loss = 0.13359582\n",
      "Iteration 1285, loss = 0.13347451\n",
      "Iteration 1286, loss = 0.13335405\n",
      "Iteration 1287, loss = 0.13323443\n",
      "Iteration 1288, loss = 0.13311565\n",
      "Iteration 1289, loss = 0.13299768\n",
      "Iteration 1290, loss = 0.13288052\n",
      "Iteration 1291, loss = 0.13276416\n",
      "Iteration 1292, loss = 0.13264858\n",
      "Iteration 1293, loss = 0.13253377\n",
      "Iteration 1294, loss = 0.13241973\n",
      "Iteration 1295, loss = 0.13230645\n",
      "Iteration 1296, loss = 0.13219390\n",
      "Iteration 1297, loss = 0.13208209\n",
      "Iteration 1298, loss = 0.13197100\n",
      "Iteration 1299, loss = 0.13186063\n",
      "Iteration 1300, loss = 0.13175096\n",
      "Iteration 1301, loss = 0.13164198\n",
      "Iteration 1302, loss = 0.13153368\n",
      "Iteration 1303, loss = 0.13142606\n",
      "Iteration 1304, loss = 0.13131910\n",
      "Iteration 1305, loss = 0.13121280\n",
      "Iteration 1306, loss = 0.13110715\n",
      "Iteration 1307, loss = 0.13100213\n",
      "Iteration 1308, loss = 0.13089774\n",
      "Iteration 1309, loss = 0.13079397\n",
      "Iteration 1310, loss = 0.13069082\n",
      "Iteration 1311, loss = 0.13058826\n",
      "Iteration 1312, loss = 0.13048631\n",
      "Iteration 1313, loss = 0.13038494\n",
      "Iteration 1314, loss = 0.13028414\n",
      "Iteration 1315, loss = 0.13018392\n",
      "Iteration 1316, loss = 0.13008427\n",
      "Iteration 1317, loss = 0.12998516\n",
      "Iteration 1318, loss = 0.12988661\n",
      "Iteration 1319, loss = 0.12978859\n",
      "Iteration 1320, loss = 0.12969111\n",
      "Iteration 1321, loss = 0.12959415\n",
      "Iteration 1322, loss = 0.12949771\n",
      "Iteration 1323, loss = 0.12940178\n",
      "Iteration 1324, loss = 0.12930636\n",
      "Iteration 1325, loss = 0.12921143\n",
      "Iteration 1326, loss = 0.12911699\n",
      "Iteration 1327, loss = 0.12902304\n",
      "Iteration 1328, loss = 0.12892956\n",
      "Iteration 1329, loss = 0.12883656\n",
      "Iteration 1330, loss = 0.12874401\n",
      "Iteration 1331, loss = 0.12865193\n",
      "Iteration 1332, loss = 0.12856030\n",
      "Iteration 1333, loss = 0.12846911\n",
      "Iteration 1334, loss = 0.12837836\n",
      "Iteration 1335, loss = 0.12828804\n",
      "Iteration 1336, loss = 0.12819815\n",
      "Iteration 1337, loss = 0.12810869\n",
      "Iteration 1338, loss = 0.12801963\n",
      "Iteration 1339, loss = 0.12793099\n",
      "Iteration 1340, loss = 0.12784276\n",
      "Iteration 1341, loss = 0.12775492\n",
      "Iteration 1342, loss = 0.12766747\n",
      "Iteration 1343, loss = 0.12758042\n",
      "Iteration 1344, loss = 0.12749374\n",
      "Iteration 1345, loss = 0.12740745\n",
      "Iteration 1346, loss = 0.12732153\n",
      "Iteration 1347, loss = 0.12723597\n",
      "Iteration 1348, loss = 0.12715078\n",
      "Iteration 1349, loss = 0.12706595\n",
      "Iteration 1350, loss = 0.12698147\n",
      "Iteration 1351, loss = 0.12689734\n",
      "Iteration 1352, loss = 0.12681355\n",
      "Iteration 1353, loss = 0.12673010\n",
      "Iteration 1354, loss = 0.12664698\n",
      "Iteration 1355, loss = 0.12656420\n",
      "Iteration 1356, loss = 0.12648174\n",
      "Iteration 1357, loss = 0.12639961\n",
      "Iteration 1358, loss = 0.12631779\n",
      "Iteration 1359, loss = 0.12623628\n",
      "Iteration 1360, loss = 0.12615508\n",
      "Iteration 1361, loss = 0.12607419\n",
      "Iteration 1362, loss = 0.12599360\n",
      "Iteration 1363, loss = 0.12591330\n",
      "Iteration 1364, loss = 0.12583330\n",
      "Iteration 1365, loss = 0.12575359\n",
      "Iteration 1366, loss = 0.12567416\n",
      "Iteration 1367, loss = 0.12559501\n",
      "Iteration 1368, loss = 0.12551614\n",
      "Iteration 1369, loss = 0.12543755\n",
      "Iteration 1370, loss = 0.12535923\n",
      "Iteration 1371, loss = 0.12528117\n",
      "Iteration 1372, loss = 0.12520338\n",
      "Iteration 1373, loss = 0.12512585\n",
      "Iteration 1374, loss = 0.12504857\n",
      "Iteration 1375, loss = 0.12497155\n",
      "Iteration 1376, loss = 0.12489478\n",
      "Iteration 1377, loss = 0.12481826\n",
      "Iteration 1378, loss = 0.12474198\n",
      "Iteration 1379, loss = 0.12466594\n",
      "Iteration 1380, loss = 0.12459014\n",
      "Iteration 1381, loss = 0.12451457\n",
      "Iteration 1382, loss = 0.12443924\n",
      "Iteration 1383, loss = 0.12436414\n",
      "Iteration 1384, loss = 0.12428926\n",
      "Iteration 1385, loss = 0.12421460\n",
      "Iteration 1386, loss = 0.12414017\n",
      "Iteration 1387, loss = 0.12406595\n",
      "Iteration 1388, loss = 0.12399195\n",
      "Iteration 1389, loss = 0.12391816\n",
      "Iteration 1390, loss = 0.12384458\n",
      "Iteration 1391, loss = 0.12377121\n",
      "Iteration 1392, loss = 0.12369805\n",
      "Iteration 1393, loss = 0.12362508\n",
      "Iteration 1394, loss = 0.12355232\n",
      "Iteration 1395, loss = 0.12347975\n",
      "Iteration 1396, loss = 0.12340738\n",
      "Iteration 1397, loss = 0.12333520\n",
      "Iteration 1398, loss = 0.12326321\n",
      "Iteration 1399, loss = 0.12319141\n",
      "Iteration 1400, loss = 0.12311979\n",
      "Iteration 1401, loss = 0.12304836\n",
      "Iteration 1402, loss = 0.12297711\n",
      "Iteration 1403, loss = 0.12290603\n",
      "Iteration 1404, loss = 0.12283514\n",
      "Iteration 1405, loss = 0.12276442\n",
      "Iteration 1406, loss = 0.12269387\n",
      "Iteration 1407, loss = 0.12262350\n",
      "Iteration 1408, loss = 0.12255329\n",
      "Iteration 1409, loss = 0.12248325\n",
      "Iteration 1410, loss = 0.12241338\n",
      "Iteration 1411, loss = 0.12234367\n",
      "Iteration 1412, loss = 0.12227412\n",
      "Iteration 1413, loss = 0.12220473\n",
      "Iteration 1414, loss = 0.12213549\n",
      "Iteration 1415, loss = 0.12206642\n",
      "Iteration 1416, loss = 0.12199750\n",
      "Iteration 1417, loss = 0.12192873\n",
      "Iteration 1418, loss = 0.12186011\n",
      "Iteration 1419, loss = 0.12179164\n",
      "Iteration 1420, loss = 0.12172332\n",
      "Iteration 1421, loss = 0.12165514\n",
      "Iteration 1422, loss = 0.12158711\n",
      "Iteration 1423, loss = 0.12151922\n",
      "Iteration 1424, loss = 0.12145147\n",
      "Iteration 1425, loss = 0.12138387\n",
      "Iteration 1426, loss = 0.12131640\n",
      "Iteration 1427, loss = 0.12124906\n",
      "Iteration 1428, loss = 0.12118187\n",
      "Iteration 1429, loss = 0.12111480\n",
      "Iteration 1430, loss = 0.12104787\n",
      "Iteration 1431, loss = 0.12098107\n",
      "Iteration 1432, loss = 0.12091440\n",
      "Iteration 1433, loss = 0.12084786\n",
      "Iteration 1434, loss = 0.12078145\n",
      "Iteration 1435, loss = 0.12071516\n",
      "Iteration 1436, loss = 0.12064900\n",
      "Iteration 1437, loss = 0.12058296\n",
      "Iteration 1438, loss = 0.12051704\n",
      "Iteration 1439, loss = 0.12045125\n",
      "Iteration 1440, loss = 0.12038557\n",
      "Iteration 1441, loss = 0.12032001\n",
      "Iteration 1442, loss = 0.12025457\n",
      "Iteration 1443, loss = 0.12018925\n",
      "Iteration 1444, loss = 0.12012404\n",
      "Iteration 1445, loss = 0.12005895\n",
      "Iteration 1446, loss = 0.11999397\n",
      "Iteration 1447, loss = 0.11992910\n",
      "Iteration 1448, loss = 0.11986434\n",
      "Iteration 1449, loss = 0.11979969\n",
      "Iteration 1450, loss = 0.11973515\n",
      "Iteration 1451, loss = 0.11967072\n",
      "Iteration 1452, loss = 0.11960640\n",
      "Iteration 1453, loss = 0.11954218\n",
      "Iteration 1454, loss = 0.11947807\n",
      "Iteration 1455, loss = 0.11941406\n",
      "Iteration 1456, loss = 0.11935016\n",
      "Iteration 1457, loss = 0.11928635\n",
      "Iteration 1458, loss = 0.11922265\n",
      "Iteration 1459, loss = 0.11915905\n",
      "Iteration 1460, loss = 0.11909555\n",
      "Iteration 1461, loss = 0.11903215\n",
      "Iteration 1462, loss = 0.11896885\n",
      "Iteration 1463, loss = 0.11890564\n",
      "Iteration 1464, loss = 0.11884254\n",
      "Iteration 1465, loss = 0.11877952\n",
      "Iteration 1466, loss = 0.11871660\n",
      "Iteration 1467, loss = 0.11865378\n",
      "Iteration 1468, loss = 0.11859105\n",
      "Iteration 1469, loss = 0.11852841\n",
      "Iteration 1470, loss = 0.11846587\n",
      "Iteration 1471, loss = 0.11840279\n",
      "Iteration 1472, loss = 0.11833868\n",
      "Iteration 1473, loss = 0.11827450\n",
      "Iteration 1474, loss = 0.11821028\n",
      "Iteration 1475, loss = 0.11814603\n",
      "Iteration 1476, loss = 0.11808176\n",
      "Iteration 1477, loss = 0.11801748\n",
      "Iteration 1478, loss = 0.11795321\n",
      "Iteration 1479, loss = 0.11788895\n",
      "Iteration 1480, loss = 0.11782472\n",
      "Iteration 1481, loss = 0.11776051\n",
      "Iteration 1482, loss = 0.11769633\n",
      "Iteration 1483, loss = 0.11763219\n",
      "Iteration 1484, loss = 0.11756810\n",
      "Iteration 1485, loss = 0.11750406\n",
      "Iteration 1486, loss = 0.11744007\n",
      "Iteration 1487, loss = 0.11737613\n",
      "Iteration 1488, loss = 0.11731226\n",
      "Iteration 1489, loss = 0.11724845\n",
      "Iteration 1490, loss = 0.11718470\n",
      "Iteration 1491, loss = 0.11712102\n",
      "Iteration 1492, loss = 0.11705741\n",
      "Iteration 1493, loss = 0.11699387\n",
      "Iteration 1494, loss = 0.11693040\n",
      "Iteration 1495, loss = 0.11686700\n",
      "Iteration 1496, loss = 0.11680368\n",
      "Iteration 1497, loss = 0.11674043\n",
      "Iteration 1498, loss = 0.11667726\n",
      "Iteration 1499, loss = 0.11661416\n",
      "Iteration 1500, loss = 0.11655115\n",
      "Iteration 1501, loss = 0.11648820\n",
      "Iteration 1502, loss = 0.11642534\n",
      "Iteration 1503, loss = 0.11636256\n",
      "Iteration 1504, loss = 0.11629985\n",
      "Iteration 1505, loss = 0.11623722\n",
      "Iteration 1506, loss = 0.11617467\n",
      "Iteration 1507, loss = 0.11611220\n",
      "Iteration 1508, loss = 0.11604981\n",
      "Iteration 1509, loss = 0.11598749\n",
      "Iteration 1510, loss = 0.11592526\n",
      "Iteration 1511, loss = 0.11586310\n",
      "Iteration 1512, loss = 0.11580102\n",
      "Iteration 1513, loss = 0.11573902\n",
      "Iteration 1514, loss = 0.11567709\n",
      "Iteration 1515, loss = 0.11561525\n",
      "Iteration 1516, loss = 0.11555348\n",
      "Iteration 1517, loss = 0.11549179\n",
      "Iteration 1518, loss = 0.11543017\n",
      "Iteration 1519, loss = 0.11536863\n",
      "Iteration 1520, loss = 0.11530717\n",
      "Iteration 1521, loss = 0.11524579\n",
      "Iteration 1522, loss = 0.11518448\n",
      "Iteration 1523, loss = 0.11512325\n",
      "Iteration 1524, loss = 0.11506209\n",
      "Iteration 1525, loss = 0.11500101\n",
      "Iteration 1526, loss = 0.11494000\n",
      "Iteration 1527, loss = 0.11487907\n",
      "Iteration 1528, loss = 0.11481821\n",
      "Iteration 1529, loss = 0.11475743\n",
      "Iteration 1530, loss = 0.11469672\n",
      "Iteration 1531, loss = 0.11463608\n",
      "Iteration 1532, loss = 0.11457552\n",
      "Iteration 1533, loss = 0.11451503\n",
      "Iteration 1534, loss = 0.11445461\n",
      "Iteration 1535, loss = 0.11439427\n",
      "Iteration 1536, loss = 0.11433400\n",
      "Iteration 1537, loss = 0.11427380\n",
      "Iteration 1538, loss = 0.11421367\n",
      "Iteration 1539, loss = 0.11415361\n",
      "Iteration 1540, loss = 0.11409363\n",
      "Iteration 1541, loss = 0.11403371\n",
      "Iteration 1542, loss = 0.11397387\n",
      "Iteration 1543, loss = 0.11391410\n",
      "Iteration 1544, loss = 0.11385440\n",
      "Iteration 1545, loss = 0.11379477\n",
      "Iteration 1546, loss = 0.11373520\n",
      "Iteration 1547, loss = 0.11367571\n",
      "Iteration 1548, loss = 0.11361629\n",
      "Iteration 1549, loss = 0.11355693\n",
      "Iteration 1550, loss = 0.11349765\n",
      "Iteration 1551, loss = 0.11343843\n",
      "Iteration 1552, loss = 0.11337929\n",
      "Iteration 1553, loss = 0.11332021\n",
      "Iteration 1554, loss = 0.11326120\n",
      "Iteration 1555, loss = 0.11320226\n",
      "Iteration 1556, loss = 0.11314338\n",
      "Iteration 1557, loss = 0.11308457\n",
      "Iteration 1558, loss = 0.11302584\n",
      "Iteration 1559, loss = 0.11296716\n",
      "Iteration 1560, loss = 0.11290856\n",
      "Iteration 1561, loss = 0.11285002\n",
      "Iteration 1562, loss = 0.11279155\n",
      "Iteration 1563, loss = 0.11273314\n",
      "Iteration 1564, loss = 0.11267480\n",
      "Iteration 1565, loss = 0.11261653\n",
      "Iteration 1566, loss = 0.11255832\n",
      "Iteration 1567, loss = 0.11250018\n",
      "Iteration 1568, loss = 0.11244211\n",
      "Iteration 1569, loss = 0.11238410\n",
      "Iteration 1570, loss = 0.11232616\n",
      "Iteration 1571, loss = 0.11226828\n",
      "Iteration 1572, loss = 0.11221046\n",
      "Iteration 1573, loss = 0.11215271\n",
      "Iteration 1574, loss = 0.11209503\n",
      "Iteration 1575, loss = 0.11203741\n",
      "Iteration 1576, loss = 0.11197986\n",
      "Iteration 1577, loss = 0.11192237\n",
      "Iteration 1578, loss = 0.11186494\n",
      "Iteration 1579, loss = 0.11180758\n",
      "Iteration 1580, loss = 0.11175028\n",
      "Iteration 1581, loss = 0.11169304\n",
      "Iteration 1582, loss = 0.11163587\n",
      "Iteration 1583, loss = 0.11157877\n",
      "Iteration 1584, loss = 0.11152172\n",
      "Iteration 1585, loss = 0.11146474\n",
      "Iteration 1586, loss = 0.11140783\n",
      "Iteration 1587, loss = 0.11135097\n",
      "Iteration 1588, loss = 0.11129418\n",
      "Iteration 1589, loss = 0.11123745\n",
      "Iteration 1590, loss = 0.11118079\n",
      "Iteration 1591, loss = 0.11112419\n",
      "Iteration 1592, loss = 0.11106765\n",
      "Iteration 1593, loss = 0.11101117\n",
      "Iteration 1594, loss = 0.11095475\n",
      "Iteration 1595, loss = 0.11089840\n",
      "Iteration 1596, loss = 0.11084211\n",
      "Iteration 1597, loss = 0.11078588\n",
      "Iteration 1598, loss = 0.11072971\n",
      "Iteration 1599, loss = 0.11067361\n",
      "Iteration 1600, loss = 0.11061757\n",
      "Iteration 1601, loss = 0.11056158\n",
      "Iteration 1602, loss = 0.11050567\n",
      "Iteration 1603, loss = 0.11044981\n",
      "Iteration 1604, loss = 0.11039401\n",
      "Iteration 1605, loss = 0.11033827\n",
      "Iteration 1606, loss = 0.11028260\n",
      "Iteration 1607, loss = 0.11022699\n",
      "Iteration 1608, loss = 0.11017144\n",
      "Iteration 1609, loss = 0.11011594\n",
      "Iteration 1610, loss = 0.11006051\n",
      "Iteration 1611, loss = 0.11000515\n",
      "Iteration 1612, loss = 0.10994984\n",
      "Iteration 1613, loss = 0.10989459\n",
      "Iteration 1614, loss = 0.10983940\n",
      "Iteration 1615, loss = 0.10978428\n",
      "Iteration 1616, loss = 0.10972921\n",
      "Iteration 1617, loss = 0.10967420\n",
      "Iteration 1618, loss = 0.10961926\n",
      "Iteration 1619, loss = 0.10956437\n",
      "Iteration 1620, loss = 0.10950955\n",
      "Iteration 1621, loss = 0.10945478\n",
      "Iteration 1622, loss = 0.10940008\n",
      "Iteration 1623, loss = 0.10934543\n",
      "Iteration 1624, loss = 0.10929085\n",
      "Iteration 1625, loss = 0.10923633\n",
      "Iteration 1626, loss = 0.10918186\n",
      "Iteration 1627, loss = 0.10912746\n",
      "Iteration 1628, loss = 0.10907311\n",
      "Iteration 1629, loss = 0.10901882\n",
      "Iteration 1630, loss = 0.10896460\n",
      "Iteration 1631, loss = 0.10891043\n",
      "Iteration 1632, loss = 0.10885632\n",
      "Iteration 1633, loss = 0.10880228\n",
      "Iteration 1634, loss = 0.10874829\n",
      "Iteration 1635, loss = 0.10869436\n",
      "Iteration 1636, loss = 0.10864049\n",
      "Iteration 1637, loss = 0.10858668\n",
      "Iteration 1638, loss = 0.10853293\n",
      "Iteration 1639, loss = 0.10847924\n",
      "Iteration 1640, loss = 0.10842560\n",
      "Iteration 1641, loss = 0.10837203\n",
      "Iteration 1642, loss = 0.10831851\n",
      "Iteration 1643, loss = 0.10826506\n",
      "Iteration 1644, loss = 0.10821166\n",
      "Iteration 1645, loss = 0.10815832\n",
      "Iteration 1646, loss = 0.10810504\n",
      "Iteration 1647, loss = 0.10805182\n",
      "Iteration 1648, loss = 0.10799866\n",
      "Iteration 1649, loss = 0.10794555\n",
      "Iteration 1650, loss = 0.10789251\n",
      "Iteration 1651, loss = 0.10783952\n",
      "Iteration 1652, loss = 0.10778659\n",
      "Iteration 1653, loss = 0.10773372\n",
      "Iteration 1654, loss = 0.10768091\n",
      "Iteration 1655, loss = 0.10762816\n",
      "Iteration 1656, loss = 0.10757546\n",
      "Iteration 1657, loss = 0.10752283\n",
      "Iteration 1658, loss = 0.10747025\n",
      "Iteration 1659, loss = 0.10741773\n",
      "Iteration 1660, loss = 0.10736527\n",
      "Iteration 1661, loss = 0.10731287\n",
      "Iteration 1662, loss = 0.10726052\n",
      "Iteration 1663, loss = 0.10720823\n",
      "Iteration 1664, loss = 0.10715600\n",
      "Iteration 1665, loss = 0.10710383\n",
      "Iteration 1666, loss = 0.10705172\n",
      "Iteration 1667, loss = 0.10699967\n",
      "Iteration 1668, loss = 0.10694767\n",
      "Iteration 1669, loss = 0.10689573\n",
      "Iteration 1670, loss = 0.10684385\n",
      "Iteration 1671, loss = 0.10679202\n",
      "Iteration 1672, loss = 0.10674026\n",
      "Iteration 1673, loss = 0.10668855\n",
      "Iteration 1674, loss = 0.10663690\n",
      "Iteration 1675, loss = 0.10658531\n",
      "Iteration 1676, loss = 0.10653377\n",
      "Iteration 1677, loss = 0.10648229\n",
      "Iteration 1678, loss = 0.10643087\n",
      "Iteration 1679, loss = 0.10637951\n",
      "Iteration 1680, loss = 0.10632821\n",
      "Iteration 1681, loss = 0.10627696\n",
      "Iteration 1682, loss = 0.10622577\n",
      "Iteration 1683, loss = 0.10617464\n",
      "Iteration 1684, loss = 0.10612356\n",
      "Iteration 1685, loss = 0.10607254\n",
      "Iteration 1686, loss = 0.10602158\n",
      "Iteration 1687, loss = 0.10597068\n",
      "Iteration 1688, loss = 0.10591983\n",
      "Iteration 1689, loss = 0.10586905\n",
      "Iteration 1690, loss = 0.10581831\n",
      "Iteration 1691, loss = 0.10576764\n",
      "Iteration 1692, loss = 0.10571702\n",
      "Iteration 1693, loss = 0.10566646\n",
      "Iteration 1694, loss = 0.10561596\n",
      "Iteration 1695, loss = 0.10556552\n",
      "Iteration 1696, loss = 0.10551513\n",
      "Iteration 1697, loss = 0.10546480\n",
      "Iteration 1698, loss = 0.10541452\n",
      "Iteration 1699, loss = 0.10536430\n",
      "Iteration 1700, loss = 0.10531414\n",
      "Iteration 1701, loss = 0.10526404\n",
      "Iteration 1702, loss = 0.10521399\n",
      "Iteration 1703, loss = 0.10516400\n",
      "Iteration 1704, loss = 0.10511407\n",
      "Iteration 1705, loss = 0.10506419\n",
      "Iteration 1706, loss = 0.10501437\n",
      "Iteration 1707, loss = 0.10496461\n",
      "Iteration 1708, loss = 0.10491490\n",
      "Iteration 1709, loss = 0.10486526\n",
      "Iteration 1710, loss = 0.10481566\n",
      "Iteration 1711, loss = 0.10476613\n",
      "Iteration 1712, loss = 0.10471665\n",
      "Iteration 1713, loss = 0.10466723\n",
      "Iteration 1714, loss = 0.10461786\n",
      "Iteration 1715, loss = 0.10456855\n",
      "Iteration 1716, loss = 0.10451930\n",
      "Iteration 1717, loss = 0.10447010\n",
      "Iteration 1718, loss = 0.10442096\n",
      "Iteration 1719, loss = 0.10437188\n",
      "Iteration 1720, loss = 0.10432285\n",
      "Iteration 1721, loss = 0.10427388\n",
      "Iteration 1722, loss = 0.10422497\n",
      "Iteration 1723, loss = 0.10417611\n",
      "Iteration 1724, loss = 0.10412731\n",
      "Iteration 1725, loss = 0.10407856\n",
      "Iteration 1726, loss = 0.10402987\n",
      "Iteration 1727, loss = 0.10398124\n",
      "Iteration 1728, loss = 0.10393267\n",
      "Iteration 1729, loss = 0.10388415\n",
      "Iteration 1730, loss = 0.10383568\n",
      "Iteration 1731, loss = 0.10378727\n",
      "Iteration 1732, loss = 0.10373892\n",
      "Iteration 1733, loss = 0.10369063\n",
      "Iteration 1734, loss = 0.10364239\n",
      "Iteration 1735, loss = 0.10359421\n",
      "Iteration 1736, loss = 0.10354608\n",
      "Iteration 1737, loss = 0.10349801\n",
      "Iteration 1738, loss = 0.10344999\n",
      "Iteration 1739, loss = 0.10340204\n",
      "Iteration 1740, loss = 0.10335413\n",
      "Iteration 1741, loss = 0.10330629\n",
      "Iteration 1742, loss = 0.10325850\n",
      "Iteration 1743, loss = 0.10321076\n",
      "Iteration 1744, loss = 0.10316308\n",
      "Iteration 1745, loss = 0.10311546\n",
      "Iteration 1746, loss = 0.10306789\n",
      "Iteration 1747, loss = 0.10302038\n",
      "Iteration 1748, loss = 0.10297293\n",
      "Iteration 1749, loss = 0.10292553\n",
      "Iteration 1750, loss = 0.10287818\n",
      "Iteration 1751, loss = 0.10283089\n",
      "Iteration 1752, loss = 0.10278366\n",
      "Iteration 1753, loss = 0.10273648\n",
      "Iteration 1754, loss = 0.10268936\n",
      "Iteration 1755, loss = 0.10264230\n",
      "Iteration 1756, loss = 0.10259529\n",
      "Iteration 1757, loss = 0.10254833\n",
      "Iteration 1758, loss = 0.10250144\n",
      "Iteration 1759, loss = 0.10245459\n",
      "Iteration 1760, loss = 0.10240781\n",
      "Iteration 1761, loss = 0.10236107\n",
      "Iteration 1762, loss = 0.10231440\n",
      "Iteration 1763, loss = 0.10226778\n",
      "Iteration 1764, loss = 0.10222121\n",
      "Iteration 1765, loss = 0.10217470\n",
      "Iteration 1766, loss = 0.10212825\n",
      "Iteration 1767, loss = 0.10208185\n",
      "Iteration 1768, loss = 0.10203550\n",
      "Iteration 1769, loss = 0.10198922\n",
      "Iteration 1770, loss = 0.10194298\n",
      "Iteration 1771, loss = 0.10189680\n",
      "Iteration 1772, loss = 0.10185068\n",
      "Iteration 1773, loss = 0.10180461\n",
      "Iteration 1774, loss = 0.10175860\n",
      "Iteration 1775, loss = 0.10171265\n",
      "Iteration 1776, loss = 0.10166674\n",
      "Iteration 1777, loss = 0.10162090\n",
      "Iteration 1778, loss = 0.10157511\n",
      "Iteration 1779, loss = 0.10152937\n",
      "Iteration 1780, loss = 0.10148369\n",
      "Iteration 1781, loss = 0.10143806\n",
      "Iteration 1782, loss = 0.10139249\n",
      "Iteration 1783, loss = 0.10134698\n",
      "Iteration 1784, loss = 0.10130151\n",
      "Iteration 1785, loss = 0.10125611\n",
      "Iteration 1786, loss = 0.10121076\n",
      "Iteration 1787, loss = 0.10116546\n",
      "Iteration 1788, loss = 0.10112022\n",
      "Iteration 1789, loss = 0.10107503\n",
      "Iteration 1790, loss = 0.10102990\n",
      "Iteration 1791, loss = 0.10098483\n",
      "Iteration 1792, loss = 0.10093980\n",
      "Iteration 1793, loss = 0.10089484\n",
      "Iteration 1794, loss = 0.10084993\n",
      "Iteration 1795, loss = 0.10080507\n",
      "Iteration 1796, loss = 0.10076027\n",
      "Iteration 1797, loss = 0.10071552\n",
      "Iteration 1798, loss = 0.10067082\n",
      "Iteration 1799, loss = 0.10062619\n",
      "Iteration 1800, loss = 0.10058160\n",
      "Iteration 1801, loss = 0.10053707\n",
      "Iteration 1802, loss = 0.10049260\n",
      "Iteration 1803, loss = 0.10044818\n",
      "Iteration 1804, loss = 0.10040381\n",
      "Iteration 1805, loss = 0.10035950\n",
      "Iteration 1806, loss = 0.10031525\n",
      "Iteration 1807, loss = 0.10027104\n",
      "Iteration 1808, loss = 0.10022690\n",
      "Iteration 1809, loss = 0.10018280\n",
      "Iteration 1810, loss = 0.10013876\n",
      "Iteration 1811, loss = 0.10009478\n",
      "Iteration 1812, loss = 0.10005085\n",
      "Iteration 1813, loss = 0.10000697\n",
      "Iteration 1814, loss = 0.09996405\n",
      "Iteration 1815, loss = 0.09992170\n",
      "Iteration 1816, loss = 0.09987951\n",
      "Iteration 1817, loss = 0.09983747\n",
      "Iteration 1818, loss = 0.09979557\n",
      "Iteration 1819, loss = 0.09975381\n",
      "Iteration 1820, loss = 0.09971217\n",
      "Iteration 1821, loss = 0.09967064\n",
      "Iteration 1822, loss = 0.09962923\n",
      "Iteration 1823, loss = 0.09958792\n",
      "Iteration 1824, loss = 0.09954671\n",
      "Iteration 1825, loss = 0.09950559\n",
      "Iteration 1826, loss = 0.09946456\n",
      "Iteration 1827, loss = 0.09942361\n",
      "Iteration 1828, loss = 0.09938275\n",
      "Iteration 1829, loss = 0.09934196\n",
      "Iteration 1830, loss = 0.09930126\n",
      "Iteration 1831, loss = 0.09926062\n",
      "Iteration 1832, loss = 0.09922005\n",
      "Iteration 1833, loss = 0.09917955\n",
      "Iteration 1834, loss = 0.09913912\n",
      "Iteration 1835, loss = 0.09909875\n",
      "Iteration 1836, loss = 0.09905845\n",
      "Iteration 1837, loss = 0.09901821\n",
      "Iteration 1838, loss = 0.09897802\n",
      "Iteration 1839, loss = 0.09893790\n",
      "Iteration 1840, loss = 0.09889783\n",
      "Iteration 1841, loss = 0.09885783\n",
      "Iteration 1842, loss = 0.09881787\n",
      "Iteration 1843, loss = 0.09877798\n",
      "Iteration 1844, loss = 0.09873813\n",
      "Iteration 1845, loss = 0.09869835\n",
      "Iteration 1846, loss = 0.09865861\n",
      "Iteration 1847, loss = 0.09861893\n",
      "Iteration 1848, loss = 0.09857930\n",
      "Iteration 1849, loss = 0.09853973\n",
      "Iteration 1850, loss = 0.09850020\n",
      "Iteration 1851, loss = 0.09846073\n",
      "Iteration 1852, loss = 0.09842130\n",
      "Iteration 1853, loss = 0.09838193\n",
      "Iteration 1854, loss = 0.09834261\n",
      "Iteration 1855, loss = 0.09830334\n",
      "Iteration 1856, loss = 0.09826412\n",
      "Iteration 1857, loss = 0.09822495\n",
      "Iteration 1858, loss = 0.09818583\n",
      "Iteration 1859, loss = 0.09814676\n",
      "Iteration 1860, loss = 0.09810774\n",
      "Iteration 1861, loss = 0.09806876\n",
      "Iteration 1862, loss = 0.09802984\n",
      "Iteration 1863, loss = 0.09799097\n",
      "Iteration 1864, loss = 0.09795214\n",
      "Iteration 1865, loss = 0.09791337\n",
      "Iteration 1866, loss = 0.09787464\n",
      "Iteration 1867, loss = 0.09783596\n",
      "Iteration 1868, loss = 0.09779733\n",
      "Iteration 1869, loss = 0.09775874\n",
      "Iteration 1870, loss = 0.09772021\n",
      "Iteration 1871, loss = 0.09768173\n",
      "Iteration 1872, loss = 0.09764329\n",
      "Iteration 1873, loss = 0.09760490\n",
      "Iteration 1874, loss = 0.09756656\n",
      "Iteration 1875, loss = 0.09752827\n",
      "Iteration 1876, loss = 0.09749002\n",
      "Iteration 1877, loss = 0.09745182\n",
      "Iteration 1878, loss = 0.09741368\n",
      "Iteration 1879, loss = 0.09737557\n",
      "Iteration 1880, loss = 0.09733752\n",
      "Iteration 1881, loss = 0.09729952\n",
      "Iteration 1882, loss = 0.09726156\n",
      "Iteration 1883, loss = 0.09722365\n",
      "Iteration 1884, loss = 0.09718579\n",
      "Iteration 1885, loss = 0.09714797\n",
      "Iteration 1886, loss = 0.09711021\n",
      "Iteration 1887, loss = 0.09707249\n",
      "Iteration 1888, loss = 0.09703482\n",
      "Iteration 1889, loss = 0.09699719\n",
      "Iteration 1890, loss = 0.09695962\n",
      "Iteration 1891, loss = 0.09692209\n",
      "Iteration 1892, loss = 0.09688461\n",
      "Iteration 1893, loss = 0.09684717\n",
      "Iteration 1894, loss = 0.09680979\n",
      "Iteration 1895, loss = 0.09677245\n",
      "Iteration 1896, loss = 0.09673516\n",
      "Iteration 1897, loss = 0.09669791\n",
      "Iteration 1898, loss = 0.09666071\n",
      "Iteration 1899, loss = 0.09662357\n",
      "Iteration 1900, loss = 0.09658646\n",
      "Iteration 1901, loss = 0.09654941\n",
      "Iteration 1902, loss = 0.09651240\n",
      "Iteration 1903, loss = 0.09647544\n",
      "Iteration 1904, loss = 0.09643852\n",
      "Iteration 1905, loss = 0.09640166\n",
      "Iteration 1906, loss = 0.09636484\n",
      "Iteration 1907, loss = 0.09632807\n",
      "Iteration 1908, loss = 0.09629134\n",
      "Iteration 1909, loss = 0.09625466\n",
      "Iteration 1910, loss = 0.09621803\n",
      "Iteration 1911, loss = 0.09618145\n",
      "Iteration 1912, loss = 0.09614491\n",
      "Iteration 1913, loss = 0.09610842\n",
      "Iteration 1914, loss = 0.09607197\n",
      "Iteration 1915, loss = 0.09603558\n",
      "Iteration 1916, loss = 0.09599923\n",
      "Iteration 1917, loss = 0.09596292\n",
      "Iteration 1918, loss = 0.09592667\n",
      "Iteration 1919, loss = 0.09589046\n",
      "Iteration 1920, loss = 0.09585429\n",
      "Iteration 1921, loss = 0.09581818\n",
      "Iteration 1922, loss = 0.09578211\n",
      "Iteration 1923, loss = 0.09574609\n",
      "Iteration 1924, loss = 0.09571011\n",
      "Iteration 1925, loss = 0.09567418\n",
      "Iteration 1926, loss = 0.09563830\n",
      "Iteration 1927, loss = 0.09560246\n",
      "Iteration 1928, loss = 0.09556667\n",
      "Iteration 1929, loss = 0.09553093\n",
      "Iteration 1930, loss = 0.09549523\n",
      "Iteration 1931, loss = 0.09545958\n",
      "Iteration 1932, loss = 0.09542397\n",
      "Iteration 1933, loss = 0.09538842\n",
      "Iteration 1934, loss = 0.09535290\n",
      "Iteration 1935, loss = 0.09531744\n",
      "Iteration 1936, loss = 0.09528202\n",
      "Iteration 1937, loss = 0.09524665\n",
      "Iteration 1938, loss = 0.09521132\n",
      "Iteration 1939, loss = 0.09517604\n",
      "Iteration 1940, loss = 0.09514081\n",
      "Iteration 1941, loss = 0.09510562\n",
      "Iteration 1942, loss = 0.09507048\n",
      "Iteration 1943, loss = 0.09503538\n",
      "Iteration 1944, loss = 0.09500033\n",
      "Iteration 1945, loss = 0.09496533\n",
      "Iteration 1946, loss = 0.09493037\n",
      "Iteration 1947, loss = 0.09489546\n",
      "Iteration 1948, loss = 0.09486060\n",
      "Iteration 1949, loss = 0.09482578\n",
      "Iteration 1950, loss = 0.09479101\n",
      "Iteration 1951, loss = 0.09475628\n",
      "Iteration 1952, loss = 0.09472160\n",
      "Iteration 1953, loss = 0.09468696\n",
      "Iteration 1954, loss = 0.09465237\n",
      "Iteration 1955, loss = 0.09461783\n",
      "Iteration 1956, loss = 0.09458333\n",
      "Iteration 1957, loss = 0.09454888\n",
      "Iteration 1958, loss = 0.09451448\n",
      "Iteration 1959, loss = 0.09448232\n",
      "Iteration 1960, loss = 0.09445192\n",
      "Iteration 1961, loss = 0.09442178\n",
      "Iteration 1962, loss = 0.09439188\n",
      "Iteration 1963, loss = 0.09436220\n",
      "Iteration 1964, loss = 0.09433272\n",
      "Iteration 1965, loss = 0.09430343\n",
      "Iteration 1966, loss = 0.09427431\n",
      "Iteration 1967, loss = 0.09424534\n",
      "Iteration 1968, loss = 0.09421652\n",
      "Iteration 1969, loss = 0.09418784\n",
      "Iteration 1970, loss = 0.09415928\n",
      "Iteration 1971, loss = 0.09413084\n",
      "Iteration 1972, loss = 0.09410250\n",
      "Iteration 1973, loss = 0.09407426\n",
      "Iteration 1974, loss = 0.09404612\n",
      "Iteration 1975, loss = 0.09401807\n",
      "Iteration 1976, loss = 0.09399010\n",
      "Iteration 1977, loss = 0.09396221\n",
      "Iteration 1978, loss = 0.09393440\n",
      "Iteration 1979, loss = 0.09390665\n",
      "Iteration 1980, loss = 0.09387898\n",
      "Iteration 1981, loss = 0.09385136\n",
      "Iteration 1982, loss = 0.09382381\n",
      "Iteration 1983, loss = 0.09379632\n",
      "Iteration 1984, loss = 0.09376888\n",
      "Iteration 1985, loss = 0.09374150\n",
      "Iteration 1986, loss = 0.09371417\n",
      "Iteration 1987, loss = 0.09368690\n",
      "Iteration 1988, loss = 0.09365967\n",
      "Iteration 1989, loss = 0.09363249\n",
      "Iteration 1990, loss = 0.09360536\n",
      "Iteration 1991, loss = 0.09357827\n",
      "Iteration 1992, loss = 0.09355123\n",
      "Iteration 1993, loss = 0.09352423\n",
      "Iteration 1994, loss = 0.09349728\n",
      "Iteration 1995, loss = 0.09347037\n",
      "Iteration 1996, loss = 0.09344350\n",
      "Iteration 1997, loss = 0.09341667\n",
      "Iteration 1998, loss = 0.09338988\n",
      "Iteration 1999, loss = 0.09336313\n",
      "Iteration 2000, loss = 0.09333642\n",
      "Iteration 2001, loss = 0.09330975\n",
      "Iteration 2002, loss = 0.09328313\n",
      "Iteration 2003, loss = 0.09325653\n",
      "Iteration 2004, loss = 0.09322998\n",
      "Iteration 2005, loss = 0.09320347\n",
      "Iteration 2006, loss = 0.09317699\n",
      "Iteration 2007, loss = 0.09315055\n",
      "Iteration 2008, loss = 0.09312415\n",
      "Iteration 2009, loss = 0.09309779\n",
      "Iteration 2010, loss = 0.09307146\n",
      "Iteration 2011, loss = 0.09304517\n",
      "Iteration 2012, loss = 0.09301891\n",
      "Iteration 2013, loss = 0.09299270\n",
      "Iteration 2014, loss = 0.09296652\n",
      "Iteration 2015, loss = 0.09294037\n",
      "Iteration 2016, loss = 0.09291426\n",
      "Iteration 2017, loss = 0.09288819\n",
      "Iteration 2018, loss = 0.09286216\n",
      "Iteration 2019, loss = 0.09283616\n",
      "Iteration 2020, loss = 0.09281019\n",
      "Iteration 2021, loss = 0.09278426\n",
      "Iteration 2022, loss = 0.09275837\n",
      "Iteration 2023, loss = 0.09273251\n",
      "Iteration 2024, loss = 0.09270669\n",
      "Iteration 2025, loss = 0.09268091\n",
      "Iteration 2026, loss = 0.09265516\n",
      "Iteration 2027, loss = 0.09262944\n",
      "Iteration 2028, loss = 0.09260376\n",
      "Iteration 2029, loss = 0.09257812\n",
      "Iteration 2030, loss = 0.09255251\n",
      "Iteration 2031, loss = 0.09252694\n",
      "Iteration 2032, loss = 0.09250140\n",
      "Iteration 2033, loss = 0.09247590\n",
      "Iteration 2034, loss = 0.09245043\n",
      "Iteration 2035, loss = 0.09242500\n",
      "Iteration 2036, loss = 0.09239960\n",
      "Iteration 2037, loss = 0.09237424\n",
      "Iteration 2038, loss = 0.09234891\n",
      "Iteration 2039, loss = 0.09232362\n",
      "Iteration 2040, loss = 0.09229836\n",
      "Iteration 2041, loss = 0.09227314\n",
      "Iteration 2042, loss = 0.09224795\n",
      "Iteration 2043, loss = 0.09222280\n",
      "Iteration 2044, loss = 0.09219768\n",
      "Iteration 2045, loss = 0.09217161\n",
      "Iteration 2046, loss = 0.09214483\n",
      "Iteration 2047, loss = 0.09211792\n",
      "Iteration 2048, loss = 0.09209091\n",
      "Iteration 2049, loss = 0.09206382\n",
      "Iteration 2050, loss = 0.09203665\n",
      "Iteration 2051, loss = 0.09200942\n",
      "Iteration 2052, loss = 0.09198213\n",
      "Iteration 2053, loss = 0.09195480\n",
      "Iteration 2054, loss = 0.09192743\n",
      "Iteration 2055, loss = 0.09190003\n",
      "Iteration 2056, loss = 0.09187261\n",
      "Iteration 2057, loss = 0.09184517\n",
      "Iteration 2058, loss = 0.09181772\n",
      "Iteration 2059, loss = 0.09179026\n",
      "Iteration 2060, loss = 0.09176280\n",
      "Iteration 2061, loss = 0.09173534\n",
      "Iteration 2062, loss = 0.09170788\n",
      "Iteration 2063, loss = 0.09168043\n",
      "Iteration 2064, loss = 0.09165300\n",
      "Iteration 2065, loss = 0.09162557\n",
      "Iteration 2066, loss = 0.09159816\n",
      "Iteration 2067, loss = 0.09157077\n",
      "Iteration 2068, loss = 0.09154340\n",
      "Iteration 2069, loss = 0.09151698\n",
      "Iteration 2070, loss = 0.09149108\n",
      "Iteration 2071, loss = 0.09146531\n",
      "Iteration 2072, loss = 0.09143967\n",
      "Iteration 2073, loss = 0.09141413\n",
      "Iteration 2074, loss = 0.09138870\n",
      "Iteration 2075, loss = 0.09136337\n",
      "Iteration 2076, loss = 0.09133813\n",
      "Iteration 2077, loss = 0.09131297\n",
      "Iteration 2078, loss = 0.09128788\n",
      "Iteration 2079, loss = 0.09126287\n",
      "Iteration 2080, loss = 0.09123793\n",
      "Iteration 2081, loss = 0.09121306\n",
      "Iteration 2082, loss = 0.09118824\n",
      "Iteration 2083, loss = 0.09116348\n",
      "Iteration 2084, loss = 0.09113878\n",
      "Iteration 2085, loss = 0.09111412\n",
      "Iteration 2086, loss = 0.09108952\n",
      "Iteration 2087, loss = 0.09106496\n",
      "Iteration 2088, loss = 0.09104045\n",
      "Iteration 2089, loss = 0.09101598\n",
      "Iteration 2090, loss = 0.09099156\n",
      "Iteration 2091, loss = 0.09096717\n",
      "Iteration 2092, loss = 0.09094282\n",
      "Iteration 2093, loss = 0.09091851\n",
      "Iteration 2094, loss = 0.09089424\n",
      "Iteration 2095, loss = 0.09087000\n",
      "Iteration 2096, loss = 0.09084579\n",
      "Iteration 2097, loss = 0.09082162\n",
      "Iteration 2098, loss = 0.09079749\n",
      "Iteration 2099, loss = 0.09077338\n",
      "Iteration 2100, loss = 0.09074931\n",
      "Iteration 2101, loss = 0.09072527\n",
      "Iteration 2102, loss = 0.09070126\n",
      "Iteration 2103, loss = 0.09067728\n",
      "Iteration 2104, loss = 0.09065333\n",
      "Iteration 2105, loss = 0.09062941\n",
      "Iteration 2106, loss = 0.09060552\n",
      "Iteration 2107, loss = 0.09058166\n",
      "Iteration 2108, loss = 0.09055782\n",
      "Iteration 2109, loss = 0.09053402\n",
      "Iteration 2110, loss = 0.09051024\n",
      "Iteration 2111, loss = 0.09048649\n",
      "Iteration 2112, loss = 0.09046277\n",
      "Iteration 2113, loss = 0.09043908\n",
      "Iteration 2114, loss = 0.09041541\n",
      "Iteration 2115, loss = 0.09039178\n",
      "Iteration 2116, loss = 0.09036817\n",
      "Iteration 2117, loss = 0.09034458\n",
      "Iteration 2118, loss = 0.09032102\n",
      "Iteration 2119, loss = 0.09029749\n",
      "Iteration 2120, loss = 0.09027399\n",
      "Iteration 2121, loss = 0.09025051\n",
      "Iteration 2122, loss = 0.09022706\n",
      "Iteration 2123, loss = 0.09020364\n",
      "Iteration 2124, loss = 0.09018024\n",
      "Iteration 2125, loss = 0.09015687\n",
      "Iteration 2126, loss = 0.09013353\n",
      "Iteration 2127, loss = 0.09011021\n",
      "Iteration 2128, loss = 0.09008692\n",
      "Iteration 2129, loss = 0.09006365\n",
      "Iteration 2130, loss = 0.09004042\n",
      "Iteration 2131, loss = 0.09001720\n",
      "Iteration 2132, loss = 0.08999401\n",
      "Iteration 2133, loss = 0.08997085\n",
      "Iteration 2134, loss = 0.08994772\n",
      "Iteration 2135, loss = 0.08992461\n",
      "Iteration 2136, loss = 0.08990152\n",
      "Iteration 2137, loss = 0.08987847\n",
      "Iteration 2138, loss = 0.08985544\n",
      "Iteration 2139, loss = 0.08983243\n",
      "Iteration 2140, loss = 0.08980945\n",
      "Iteration 2141, loss = 0.08978649\n",
      "Iteration 2142, loss = 0.08976357\n",
      "Iteration 2143, loss = 0.08974066\n",
      "Iteration 2144, loss = 0.08971779\n",
      "Iteration 2145, loss = 0.08969493\n",
      "Iteration 2146, loss = 0.08967211\n",
      "Iteration 2147, loss = 0.08964931\n",
      "Iteration 2148, loss = 0.08962653\n",
      "Iteration 2149, loss = 0.08960378\n",
      "Iteration 2150, loss = 0.08958106\n",
      "Iteration 2151, loss = 0.08955836\n",
      "Iteration 2152, loss = 0.08953569\n",
      "Iteration 2153, loss = 0.08951304\n",
      "Iteration 2154, loss = 0.08949042\n",
      "Iteration 2155, loss = 0.08946782\n",
      "Iteration 2156, loss = 0.08944525\n",
      "Iteration 2157, loss = 0.08942270\n",
      "Iteration 2158, loss = 0.08940196\n",
      "Iteration 2159, loss = 0.08938544\n",
      "Iteration 2160, loss = 0.08936927\n",
      "Iteration 2161, loss = 0.08935344\n",
      "Iteration 2162, loss = 0.08933789\n",
      "Iteration 2163, loss = 0.08932261\n",
      "Iteration 2164, loss = 0.08930757\n",
      "Iteration 2165, loss = 0.08929275\n",
      "Iteration 2166, loss = 0.08927812\n",
      "Iteration 2167, loss = 0.08926368\n",
      "Iteration 2168, loss = 0.08924939\n",
      "Iteration 2169, loss = 0.08923526\n",
      "Iteration 2170, loss = 0.08922125\n",
      "Iteration 2171, loss = 0.08920737\n",
      "Iteration 2172, loss = 0.08919361\n",
      "Iteration 2173, loss = 0.08917994\n",
      "Iteration 2174, loss = 0.08916637\n",
      "Iteration 2175, loss = 0.08915288\n",
      "Iteration 2176, loss = 0.08913947\n",
      "Iteration 2177, loss = 0.08912614\n",
      "Iteration 2178, loss = 0.08911287\n",
      "Iteration 2179, loss = 0.08909966\n",
      "Iteration 2180, loss = 0.08908651\n",
      "Iteration 2181, loss = 0.08907341\n",
      "Iteration 2182, loss = 0.08906036\n",
      "Iteration 2183, loss = 0.08904736\n",
      "Iteration 2184, loss = 0.08903440\n",
      "Iteration 2185, loss = 0.08902147\n",
      "Iteration 2186, loss = 0.08900859\n",
      "Iteration 2187, loss = 0.08899574\n",
      "Iteration 2188, loss = 0.08898292\n",
      "Iteration 2189, loss = 0.08897013\n",
      "Iteration 2190, loss = 0.08895737\n",
      "Iteration 2191, loss = 0.08894464\n",
      "Iteration 2192, loss = 0.08893193\n",
      "Iteration 2193, loss = 0.08891925\n",
      "Iteration 2194, loss = 0.08890659\n",
      "Iteration 2195, loss = 0.08889396\n",
      "Iteration 2196, loss = 0.08888135\n",
      "Iteration 2197, loss = 0.08886875\n",
      "Iteration 2198, loss = 0.08885618\n",
      "Iteration 2199, loss = 0.08884363\n",
      "Iteration 2200, loss = 0.08883109\n",
      "Iteration 2201, loss = 0.08881858\n",
      "Iteration 2202, loss = 0.08880608\n",
      "Iteration 2203, loss = 0.08879360\n",
      "Iteration 2204, loss = 0.08878113\n",
      "Iteration 2205, loss = 0.08876868\n",
      "Iteration 2206, loss = 0.08875625\n",
      "Iteration 2207, loss = 0.08874383\n",
      "Iteration 2208, loss = 0.08873143\n",
      "Iteration 2209, loss = 0.08871904\n",
      "Iteration 2210, loss = 0.08870667\n",
      "Iteration 2211, loss = 0.08869431\n",
      "Iteration 2212, loss = 0.08868196\n",
      "Iteration 2213, loss = 0.08866963\n",
      "Iteration 2214, loss = 0.08865732\n",
      "Iteration 2215, loss = 0.08864501\n",
      "Iteration 2216, loss = 0.08863272\n",
      "Iteration 2217, loss = 0.08862045\n",
      "Iteration 2218, loss = 0.08860818\n",
      "Iteration 2219, loss = 0.08859593\n",
      "Iteration 2220, loss = 0.08858370\n",
      "Iteration 2221, loss = 0.08857147\n",
      "Iteration 2222, loss = 0.08855926\n",
      "Iteration 2223, loss = 0.08854706\n",
      "Iteration 2224, loss = 0.08853488\n",
      "Iteration 2225, loss = 0.08852270\n",
      "Iteration 2226, loss = 0.08851054\n",
      "Iteration 2227, loss = 0.08849839\n",
      "Iteration 2228, loss = 0.08848626\n",
      "Iteration 2229, loss = 0.08847413\n",
      "Iteration 2230, loss = 0.08846202\n",
      "Iteration 2231, loss = 0.08844992\n",
      "Iteration 2232, loss = 0.08843783\n",
      "Iteration 2233, loss = 0.08842576\n",
      "Iteration 2234, loss = 0.08841369\n",
      "Iteration 2235, loss = 0.08840164\n",
      "Iteration 2236, loss = 0.08838960\n",
      "Iteration 2237, loss = 0.08837757\n",
      "Iteration 2238, loss = 0.08836556\n",
      "Iteration 2239, loss = 0.08835355\n",
      "Iteration 2240, loss = 0.08834156\n",
      "Iteration 2241, loss = 0.08832958\n",
      "Iteration 2242, loss = 0.08831761\n",
      "Iteration 2243, loss = 0.08830565\n",
      "Iteration 2244, loss = 0.08829371\n",
      "Iteration 2245, loss = 0.08828178\n",
      "Iteration 2246, loss = 0.08826985\n",
      "Iteration 2247, loss = 0.08825794\n",
      "Iteration 2248, loss = 0.08824605\n",
      "Iteration 2249, loss = 0.08823416\n",
      "Iteration 2250, loss = 0.08822228\n",
      "Iteration 2251, loss = 0.08821042\n",
      "Iteration 2252, loss = 0.08819857\n",
      "Iteration 2253, loss = 0.08818673\n",
      "Iteration 2254, loss = 0.08817490\n",
      "Iteration 2255, loss = 0.08816308\n",
      "Iteration 2256, loss = 0.08815128\n",
      "Iteration 2257, loss = 0.08813948\n",
      "Iteration 2258, loss = 0.08812770\n",
      "Iteration 2259, loss = 0.08811593\n",
      "Iteration 2260, loss = 0.08810417\n",
      "Iteration 2261, loss = 0.08809242\n",
      "Iteration 2262, loss = 0.08808069\n",
      "Iteration 2263, loss = 0.08806896\n",
      "Iteration 2264, loss = 0.08805725\n",
      "Iteration 2265, loss = 0.08804555\n",
      "Iteration 2266, loss = 0.08803385\n",
      "Iteration 2267, loss = 0.08802218\n",
      "Iteration 2268, loss = 0.08801051\n",
      "Iteration 2269, loss = 0.08799885\n",
      "Iteration 2270, loss = 0.08798721\n",
      "Iteration 2271, loss = 0.08797557\n",
      "Iteration 2272, loss = 0.08796395\n",
      "Iteration 2273, loss = 0.08795234\n",
      "Iteration 2274, loss = 0.08794074\n",
      "Iteration 2275, loss = 0.08792915\n",
      "Iteration 2276, loss = 0.08791758\n",
      "Iteration 2277, loss = 0.08790601\n",
      "Iteration 2278, loss = 0.08789446\n",
      "Iteration 2279, loss = 0.08788291\n",
      "Iteration 2280, loss = 0.08787138\n",
      "Iteration 2281, loss = 0.08785986\n",
      "Iteration 2282, loss = 0.08784836\n",
      "Iteration 2283, loss = 0.08783686\n",
      "Iteration 2284, loss = 0.08782537\n",
      "Iteration 2285, loss = 0.08781390\n",
      "Iteration 2286, loss = 0.08780244\n",
      "Iteration 2287, loss = 0.08779098\n",
      "Iteration 2288, loss = 0.08777954\n",
      "Iteration 2289, loss = 0.08776811\n",
      "Iteration 2290, loss = 0.08775670\n",
      "Iteration 2291, loss = 0.08774529\n",
      "Iteration 2292, loss = 0.08773389\n",
      "Iteration 2293, loss = 0.08772251\n",
      "Iteration 2294, loss = 0.08771114\n",
      "Iteration 2295, loss = 0.08769978\n",
      "Iteration 2296, loss = 0.08768843\n",
      "Iteration 2297, loss = 0.08767709\n",
      "Iteration 2298, loss = 0.08766576\n",
      "Iteration 2299, loss = 0.08765444\n",
      "Iteration 2300, loss = 0.08764314\n",
      "Iteration 2301, loss = 0.08763185\n",
      "Iteration 2302, loss = 0.08762056\n",
      "Iteration 2303, loss = 0.08760929\n",
      "Iteration 2304, loss = 0.08759803\n",
      "Iteration 2305, loss = 0.08758678\n",
      "Iteration 2306, loss = 0.08757554\n",
      "Iteration 2307, loss = 0.08756432\n",
      "Iteration 2308, loss = 0.08755310\n",
      "Iteration 2309, loss = 0.08754190\n",
      "Iteration 2310, loss = 0.08753071\n",
      "Iteration 2311, loss = 0.08751953\n",
      "Iteration 2312, loss = 0.08750836\n",
      "Iteration 2313, loss = 0.08749720\n",
      "Iteration 2314, loss = 0.08748605\n",
      "Iteration 2315, loss = 0.08747491\n",
      "Iteration 2316, loss = 0.08746379\n",
      "Iteration 2317, loss = 0.08745267\n",
      "Iteration 2318, loss = 0.08744157\n",
      "Iteration 2319, loss = 0.08743048\n",
      "Iteration 2320, loss = 0.08741940\n",
      "Iteration 2321, loss = 0.08740833\n",
      "Iteration 2322, loss = 0.08739727\n",
      "Iteration 2323, loss = 0.08738622\n",
      "Iteration 2324, loss = 0.08737518\n",
      "Iteration 2325, loss = 0.08736416\n",
      "Iteration 2326, loss = 0.08735315\n",
      "Iteration 2327, loss = 0.08734214\n",
      "Iteration 2328, loss = 0.08733115\n",
      "Iteration 2329, loss = 0.08732017\n",
      "Iteration 2330, loss = 0.08730920\n",
      "Iteration 2331, loss = 0.08729824\n",
      "Iteration 2332, loss = 0.08728730\n",
      "Iteration 2333, loss = 0.08727636\n",
      "Iteration 2334, loss = 0.08726544\n",
      "Iteration 2335, loss = 0.08725452\n",
      "Iteration 2336, loss = 0.08724362\n",
      "Iteration 2337, loss = 0.08723273\n",
      "Iteration 2338, loss = 0.08722185\n",
      "Iteration 2339, loss = 0.08721098\n",
      "Iteration 2340, loss = 0.08720012\n",
      "Iteration 2341, loss = 0.08718927\n",
      "Iteration 2342, loss = 0.08717844\n",
      "Iteration 2343, loss = 0.08716761\n",
      "Iteration 2344, loss = 0.08715680\n",
      "Iteration 2345, loss = 0.08714600\n",
      "Iteration 2346, loss = 0.08713520\n",
      "Iteration 2347, loss = 0.08712442\n",
      "Iteration 2348, loss = 0.08711365\n",
      "Iteration 2349, loss = 0.08710290\n",
      "Iteration 2350, loss = 0.08709215\n",
      "Iteration 2351, loss = 0.08708141\n",
      "Iteration 2352, loss = 0.08707069\n",
      "Iteration 2353, loss = 0.08705997\n",
      "Iteration 2354, loss = 0.08704927\n",
      "Iteration 2355, loss = 0.08703858\n",
      "Iteration 2356, loss = 0.08702789\n",
      "Iteration 2357, loss = 0.08701722\n",
      "Iteration 2358, loss = 0.08700657\n",
      "Iteration 2359, loss = 0.08699592\n",
      "Iteration 2360, loss = 0.08698528\n",
      "Iteration 2361, loss = 0.08697465\n",
      "Iteration 2362, loss = 0.08696404\n",
      "Iteration 2363, loss = 0.08695343\n",
      "Iteration 2364, loss = 0.08694284\n",
      "Iteration 2365, loss = 0.08693226\n",
      "Iteration 2366, loss = 0.08692169\n",
      "Iteration 2367, loss = 0.08691113\n",
      "Iteration 2368, loss = 0.08690058\n",
      "Iteration 2369, loss = 0.08689004\n",
      "Iteration 2370, loss = 0.08687951\n",
      "Iteration 2371, loss = 0.08686900\n",
      "Iteration 2372, loss = 0.08685849\n",
      "Iteration 2373, loss = 0.08684800\n",
      "Iteration 2374, loss = 0.08683751\n",
      "Iteration 2375, loss = 0.08682704\n",
      "Iteration 2376, loss = 0.08681658\n",
      "Iteration 2377, loss = 0.08680613\n",
      "Iteration 2378, loss = 0.08679569\n",
      "Iteration 2379, loss = 0.08678526\n",
      "Iteration 2380, loss = 0.08677484\n",
      "Iteration 2381, loss = 0.08676443\n",
      "Iteration 2382, loss = 0.08675404\n",
      "Iteration 2383, loss = 0.08674365\n",
      "Iteration 2384, loss = 0.08673328\n",
      "Iteration 2385, loss = 0.08672291\n",
      "Iteration 2386, loss = 0.08671256\n",
      "Iteration 2387, loss = 0.08670222\n",
      "Iteration 2388, loss = 0.08669189\n",
      "Iteration 2389, loss = 0.08668157\n",
      "Iteration 2390, loss = 0.08667126\n",
      "Iteration 2391, loss = 0.08666096\n",
      "Iteration 2392, loss = 0.08665068\n",
      "Iteration 2393, loss = 0.08664040\n",
      "Iteration 2394, loss = 0.08663013\n",
      "Iteration 2395, loss = 0.08661988\n",
      "Iteration 2396, loss = 0.08660963\n",
      "Iteration 2397, loss = 0.08659940\n",
      "Iteration 2398, loss = 0.08658918\n",
      "Iteration 2399, loss = 0.08657897\n",
      "Iteration 2400, loss = 0.08656877\n",
      "Iteration 2401, loss = 0.08655858\n",
      "Iteration 2402, loss = 0.08654824\n",
      "Iteration 2403, loss = 0.08653777\n",
      "Iteration 2404, loss = 0.08652728\n",
      "Iteration 2405, loss = 0.08651679\n",
      "Iteration 2406, loss = 0.08650627\n",
      "Iteration 2407, loss = 0.08649575\n",
      "Iteration 2408, loss = 0.08648522\n",
      "Iteration 2409, loss = 0.08647469\n",
      "Iteration 2410, loss = 0.08646415\n",
      "Iteration 2411, loss = 0.08645360\n",
      "Iteration 2412, loss = 0.08644305\n",
      "Iteration 2413, loss = 0.08643250\n",
      "Iteration 2414, loss = 0.08642195\n",
      "Iteration 2415, loss = 0.08641139\n",
      "Iteration 2416, loss = 0.08640084\n",
      "Iteration 2417, loss = 0.08639029\n",
      "Iteration 2418, loss = 0.08637974\n",
      "Iteration 2419, loss = 0.08636919\n",
      "Iteration 2420, loss = 0.08635864\n",
      "Iteration 2421, loss = 0.08634810\n",
      "Iteration 2422, loss = 0.08633756\n",
      "Iteration 2423, loss = 0.08632702\n",
      "Iteration 2424, loss = 0.08631649\n",
      "Iteration 2425, loss = 0.08630596\n",
      "Iteration 2426, loss = 0.08629544\n",
      "Iteration 2427, loss = 0.08628492\n",
      "Iteration 2428, loss = 0.08627441\n",
      "Iteration 2429, loss = 0.08626390\n",
      "Iteration 2430, loss = 0.08625339\n",
      "Iteration 2431, loss = 0.08624290\n",
      "Iteration 2432, loss = 0.08623240\n",
      "Iteration 2433, loss = 0.08622192\n",
      "Iteration 2434, loss = 0.08621143\n",
      "Iteration 2435, loss = 0.08620096\n",
      "Iteration 2436, loss = 0.08619049\n",
      "Iteration 2437, loss = 0.08618002\n",
      "Iteration 2438, loss = 0.08616957\n",
      "Iteration 2439, loss = 0.08615911\n",
      "Iteration 2440, loss = 0.08614867\n",
      "Iteration 2441, loss = 0.08613823\n",
      "Iteration 2442, loss = 0.08612779\n",
      "Iteration 2443, loss = 0.08611737\n",
      "Iteration 2444, loss = 0.08610695\n",
      "Iteration 2445, loss = 0.08609653\n",
      "Iteration 2446, loss = 0.08608612\n",
      "Iteration 2447, loss = 0.08607572\n",
      "Iteration 2448, loss = 0.08606532\n",
      "Iteration 2449, loss = 0.08605493\n",
      "Iteration 2450, loss = 0.08604455\n",
      "Iteration 2451, loss = 0.08603417\n",
      "Iteration 2452, loss = 0.08602380\n",
      "Iteration 2453, loss = 0.08601344\n",
      "Iteration 2454, loss = 0.08600308\n",
      "Iteration 2455, loss = 0.08599273\n",
      "Iteration 2456, loss = 0.08598238\n",
      "Iteration 2457, loss = 0.08597204\n",
      "Iteration 2458, loss = 0.08596171\n",
      "Iteration 2459, loss = 0.08595138\n",
      "Iteration 2460, loss = 0.08594106\n",
      "Iteration 2461, loss = 0.08593075\n",
      "Iteration 2462, loss = 0.08592044\n",
      "Iteration 2463, loss = 0.08591014\n",
      "Iteration 2464, loss = 0.08589985\n",
      "Iteration 2465, loss = 0.08588956\n",
      "Iteration 2466, loss = 0.08587928\n",
      "Iteration 2467, loss = 0.08586900\n",
      "Iteration 2468, loss = 0.08585874\n",
      "Iteration 2469, loss = 0.08584847\n",
      "Iteration 2470, loss = 0.08583822\n",
      "Iteration 2471, loss = 0.08582797\n",
      "Iteration 2472, loss = 0.08581772\n",
      "Iteration 2473, loss = 0.08580749\n",
      "Iteration 2474, loss = 0.08579726\n",
      "Iteration 2475, loss = 0.08578703\n",
      "Iteration 2476, loss = 0.08577682\n",
      "Iteration 2477, loss = 0.08576661\n",
      "Iteration 2478, loss = 0.08575640\n",
      "Iteration 2479, loss = 0.08574620\n",
      "Iteration 2480, loss = 0.08573601\n",
      "Iteration 2481, loss = 0.08572583\n",
      "Iteration 2482, loss = 0.08571565\n",
      "Iteration 2483, loss = 0.08570547\n",
      "Iteration 2484, loss = 0.08569531\n",
      "Iteration 2485, loss = 0.08568515\n",
      "Iteration 2486, loss = 0.08567499\n",
      "Iteration 2487, loss = 0.08566485\n",
      "Iteration 2488, loss = 0.08565471\n",
      "Iteration 2489, loss = 0.08564457\n",
      "Iteration 2490, loss = 0.08563444\n",
      "Iteration 2491, loss = 0.08562432\n",
      "Iteration 2492, loss = 0.08561637\n",
      "Iteration 2493, loss = 0.08560918\n",
      "Iteration 2494, loss = 0.08560182\n",
      "Iteration 2495, loss = 0.08559432\n",
      "Iteration 2496, loss = 0.08558669\n",
      "Iteration 2497, loss = 0.08557894\n",
      "Iteration 2498, loss = 0.08557108\n",
      "Iteration 2499, loss = 0.08556312\n",
      "Iteration 2500, loss = 0.08555508\n",
      "Iteration 2501, loss = 0.08554696\n",
      "Iteration 2502, loss = 0.08553877\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "mlp.score=0.75\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "stdScaler = StandardScaler()\n",
    "\n",
    "stdScaler.fit(X)\n",
    "X_scaled = stdScaler.transform(X)\n",
    "M_scaled = stdScaler.transform(M)\n",
    "\n",
    "mlp.fit(X_scaled, y)\n",
    "y_pred_mlp = mlp.predict(M_scaled)\n",
    "\n",
    "print(f\"mlp.score={mlp.score(X_scaled, y):0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Modify the MLP Hyperparameters\n",
    "\n",
    "Finally, try out some of the hyperparameters associated with the MLP.\n",
    "\n",
    "Specifically, test how few neurons the MLP can do with---still producing a sensible output, i.e. high $R^2$. \n",
    "\n",
    "Also try-out some other activation functions, ala sigmoid, and solvers, like `sgd`.\n",
    "\n",
    "Notice, that the Scikit-learn MLP does not have as many adjustable parameters, as a Keras MLP, for example, the Scikit-learn MLP misses neurons initialization parameters (p. 333-334 [HOML]) and the ELU activation function (p. 336 [HOML]).\n",
    "\n",
    "OPTIONAL$_1$: use a Keras MLP regressor instead of the Scikit-learn MLP (You need to install the  Keras if its not installed as default).\n",
    "\n",
    "OPTIONAL$_2$: try out the `early_stopping` hyperparameter on the `MLPRegressor`. \n",
    "\n",
    "OPTIONAL$_3$: try putting all score-calculations into K-fold cross-validation  methods readily available in Scikit-learn using\n",
    "\n",
    "* `sklearn.model_selection.cross_val_predict`\n",
    "* `sklearn.model_selection.cross_val_score` \n",
    "\n",
    "or similar (this is, in theory, the correct method, but can be hard to use due to the  extremely small number of data points, `n=29`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10, ),\n",
    "                   solver='adam',\n",
    "                   activation='relu',\n",
    "                   tol=1E-5,\n",
    "                   max_iter=100000,\n",
    "                   verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2020-10-15| CEF, initial. \n",
    "2020-10-21| CEF, added Standard Scaler Q.\n",
    "2020-11-17| CEF, removed orhpant text in Qa (moded to Qc).\n",
    "2021-02-10| CEF, updated for ITMAL F21.\n",
    "2021-11-08| CEF, updated print info.\n",
    "2021-02.10| CEF, updated for SWMAL F22.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
